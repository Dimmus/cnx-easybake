<html xmlns="http://www.w3.org/1999/xhtml">
  <body><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45.html" data-type="page"><h1>Discrete-Time Signals and Systems</h1><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45.html">Discrete-Time Signals and Systems</a><ul><li><a>Discrete-Time Signals</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A52855297-0cf6-43b1-adf9-9601c26455d1%401.html">Discrete-time Signals and Signal Processing</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A100d968c-b9a5-4969-8d9e-8a591c08c178%401.html">Plotting Discrete-Time Signals</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8491d0e5-4142-43f2-890c-a8807e0f9b02%401.html">Signal Properties</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A19005586-911a-409b-8e69-3ad81a78e1bb%401.html">Key Discrete-time Test Signals</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A9ece27c4-6ab2-4f5b-a9d2-2912af734d2b%401.html">Real and Complex Sinusoidal Signals</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A6671e8da-3bce-4a81-99ac-ebb85ce56fd9%401.html">The Peculiarity of Discrete-Time Sinusoids</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ad09711e8-464f-474b-a1f5-8cae67ce34f1%401.html">Complex Exponentials</a></li></ul></li><li><a>Signals Are Vectors</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ab3754f43-6ba2-4546-93f1-229a4fd324fd%401.html">Signals are Vectors</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aba6c282e-33b7-4664-9f5d-bb038a8cd21c%401.html">Linear Combinations of Vectors</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A4e807dcd-b5e5-41f8-b1bf-ae07a9d84810%401.html">The Strength of a Vector</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aae07056c-3869-4fa5-bba4-c68910470847%401.html">Inner Products and Orthogonality</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa5760e83-3a00-45f7-8900-fc4ff16f7fad%401.html">Norms and Inner Products of Infinite Length Vectors</a></li></ul></li><li><a>Discrete-Time Systems</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aca0590f6-a180-4540-95f9-db484cac49b5%401.html">Discrete-time Systems</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A9c7e1cfb-e9fa-46df-bb19-04c5ab29f182%401.html">Linear Systems</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aeeec78c6-1006-421a-aa08-f46cc23911e0%401.html">Time-Invariant Systems</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa7fb0c19-dcde-45c8-b2b1-321327b267a9%401.html">Linear Time-Invariant Systems</a></li></ul></li><li><a>Convolution</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8e9c729f-20f2-4ced-842e-6f774282e2e6%401.html">The Impulse Response of Discrete-Time Systems</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A20178296-96b0-46c2-b296-7ae0567f3732%401.html">Convolution of Infinite-Length Discrete-Time Signals</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ad6d53200-e1f0-4b4a-9c64-de9abe526940%401.html">Convolution of Finite-Length Discrete-Time Signals</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ab94f5937-47f0-442a-9869-7ea77d0e6a68%401.html">Properties of Discrete-Time Convolution</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3e922de4-2c0f-4f9b-9abd-6865fa1ee98c%401.html">Discrete-Time Infinite-Length and Finite-Length Convolution Equivalence</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A097fd6ed-9527-43cc-86cf-04df99fff518%401.html">Impulse Response and LTI System Causality</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Af301e8c6-42ac-490d-bea5-ed6b75d06c30%401.html">Impulse Response and LTI System Stability</a></li></ul></li><li><a>Orthogonal Bases and the Discrete Fourier Transform</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Af746e0a1-8da1-43a6-b1c6-3f1497c7245c%401.html">Orthogonal Bases</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aafae700e-90dd-4c27-8ffa-b9def44c3abb%401.html">Eigenanalysis of LTI Systems (Finite-Length Signals)</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Abebd669a-e439-4025-9551-074f09762d53%401.html">The Discrete Fourier Transform</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A457d1580-1d5b-40aa-bdb9-01fca8b5d67c%401.html">Discrete Fourier Transform Properties</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A34418b41-f66e-425c-bec1-5d2289051e9f%401.html">The Fast Fourier Transform</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3c0a4ffb-e056-4637-94eb-6e1da70c808b%401.html">Fast Convolution with the FFT</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa36f93aa-83fe-4fba-bed1-e18065409138%401.html">Other Orthogonal Bases</a></li></ul></li><li><a>The Discrete-Time Fourier Transform</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A580b4363-4532-4798-87b1-28491fa2bef7%401.html">Eigenanalysis of LTI Systems (Infinite-Length Signals)</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ac9fb3ca1-baaf-4bac-9130-e5d273c304c2%401.html">The Discrete-Time Fourier Transform</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8829b7cb-62ab-43a2-9910-a9b40452c825%401.html">Discrete Time Fourier Transform Examples</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A7989578f-0dd9-4eed-9311-ff866f9888f7%401.html">The Discrete-Time Fourier Transform of a Sinusoid</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Adc2b62cc-439b-4895-a130-68f945d55269%401.html">Discrete Time Fourier Transform Properties</a></li></ul></li><li><a>The z-Transform</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3c5fd8f2-8393-4409-b8d1-2f748f731cbf%401.html">The z-Transform</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ac6c8ebe6-a3bc-4c56-b2a0-86da3d6e0cb2%401.html">The z-Transform Region of Convergence</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A1390e34e-6790-4c09-bf0c-bb99998e86ad%401.html">The Transfer Function of Discrete-Time LTI Systems</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Abe2b609c-e531-4fd5-8d1b-0d952acf6956%401.html">z-Transform Properties</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A439e30e4-02d6-4e39-98c2-da8295ce4ed8%401.html">The Inverse z-Transform</a></li></ul></li><li><a>Discrete-Time Filters</a><ul><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A941c71be-ae4a-4f23-8cee-462506b4d564%401.html">Discrete-Time Filtering and Filter Design</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8e1e5826-5004-4d75-8231-9e36db5682a6%401.html">IIR Filter Design</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A7be7bd64-267c-491e-bb67-ff9e515f7dfd%401.html">FIR Filter Design</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A02966e04-2314-4b5f-9852-58f310e29b78%401.html">Inverse Filters and Deconvolution</a></li><li><a href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3177f913-457f-4e88-9e72-4709c2379d0e%401.html">Matched Filters</a></li></ul></li></ul></li></ul></div><ul><li><div data-type="page"><h1>Discrete-Time Signals</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A52855297-0cf6-43b1-adf9-9601c26455d1%401.html" data-type="page"><h1>Discrete-time Signals and Signal Processing</h1><div data-type="document-title">Discrete-time Signals and Signal Processing</div>
  <p id="delete_me"><span data-type="title">A World of Signals and Signal Processing</span><!-- Insert module text here -->
Technological innovations have revolutionized the way we view and interact with the world around us. Editing a photo, re-mixing a song, automatically measuring and adjusting chemical concentrations in a tank: each of these tasks requires real-world data to be captured by a computer and then manipulated digitally to extract the salient information. Ever wonder how signals from the physical world are sampled, stored, and processed without losing the information required to make predictions and extract meaning from the data? <span data-type="term">Signal processing</span> is the study of signals and systems that extract information from the world around us.</p><p id="eip-269"><span data-type="title">Signals, Defined</span>Perhaps the place to start the study of signal processing is the dictionary.  The dictionary definition of a signal will serve us quite well:
</p><blockquote id="signal-quote" cite="http://www.merriam-webster.com/dictionary/signal"> 
 A <span data-type="term">signal</span> is "is a detectable physical quantity...by which messages or information can be transmitted."</blockquote>

And this information aspect is very, very critical to us.  In other words, signals carry information. Signals are all around us;
we encounter them throughout our day. Speech signals, for example, transmit language from one person to another via acoustic waves.
If you're interested in looking for, for example, airplanes our other targets and sensing them by electromagnetic waves,
you can use radar signal processing. Electrophysiological signals carry information about processes that are going on inside our bodies, things like EKGs or MRI images. And finally, financial signals transmit information about events in the economy, signals like stock prices over time or other economic markers.

So as you can see, signals are really very common in the world.
And this course is about signals and the signal processing systems that manipulate signals in order to understand or make transformations on the information in those signals.<p id="eip-659"><span data-type="title">Signals are Functions</span></p><dl id="signal-def" class="definition"><dt>Signal</dt>
<dd id="signal-meaning">
A </dd><dt>signal</dt> is a function that maps an independent variable to a dependent variable.</dl>
Mathematically, we're going to think of signals as functions, and a function is just a mapping from an independent variable that we can change to a dependent variable that depends on that independent variable.  The terminology <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>
is how we're going to denote a signal.
It consists of an independent variable, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>, that for each different value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>,
it produces the value <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.<p id="eip-240"><span data-type="title">Discrete-Time Signals</span>Perhaps you are wondering about the use of brackets--instead of parentheses--in our signal function notation <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>. This is the typical way to refer to <span data-type="term">discrete-time</span> signals. A discrete-time signal is a signal where the independent variable <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math> is an integer (as opposed to a continuous-time signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mi>t</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="continuous">x</ci><ci>t</ci></apply></annotation-xml></semantics></math>, whose independent variable <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>t</mi></mrow><annotation-xml encoding="MathML-Content"><mi>t</mi></annotation-xml></semantics></math> is a real number).</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A100d968c-b9a5-4969-8d9e-8a591c08c178%401.html" data-type="page"><h1>Plotting Discrete-Time Signals</h1><div data-type="document-title">Plotting Discrete-Time Signals</div>
  <p id="delete_me">Recall that a <span data-type="term">discrete-time signal</span> is a <span data-type="term">function</span> with an integer-valued independent variable <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>.  The variable <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math> marches through time from negative infinity to positive infinity. For each value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>, we get the value of  from our function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.
Now, that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> is either going to be a real number, meaning it's going to live in the real number set, or it's going to be a complex number and live in the complex number set.

</p><p id="eip-71"><span data-type="title">Plotting Real Signals</span>We're going to see a lot graphs like these in our study of signal processing:
<figure id="sigfunt"><figcaption>Example of a discrete-time signal.</figcaption><span data-type="media" id="sigfunplott" data-alt="Discrete-time signal">
<img src="/resources/f66560135d56e8de79894a0fe56cdb66d76e84e4/sigFun.svg" data-media-type="image/svg+xml" alt="Discrete-time signal">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8bf0f6cbdc146de3042f18dc2edd23fd0bdf9ba6/sigFun.eps" data-type="image"></span>
  </span>
  </figure>
For each value of one of these <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>, we get the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.
For clarity, we're often going to color in these circles at the top, but that's really just a matter of taste.  We're either going to label the signal on the y-axis or in the title of the graph.
When it's clear from context that we're dealing with a discrete index <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>, we can strip away all of the labels and axes and just plot signals like this, just because it's cleaner for some applications:
<figure id="sigfunNo"><figcaption>A discrete-time signal, without the axes labeled.</figcaption><span data-type="media" id="sigfunNoplot" data-alt="Discrete-time signal">
<img src="/resources/6f705fd46431c979eabbee81e882f728269fbfd0/sigFunNo.svg" data-media-type="image/svg+xml" alt="Discrete-time signal">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/39f65c18f19a0c13bfa410adf33862a6a78abd94/sigFunNo.eps" data-type="image"></span>
  </span>
  </figure></p><p id="eip-232"><span data-type="title">Examples of Discrete-Time Signal Plots</span>Here are some examples of signals.  The first is a financial time series. It's the daily closing share price of Google for a five-month period:
<figure id="google"><figcaption>A financial series signal.</figcaption><span data-type="media" id="googleplot" data-alt="Discrete-time signal">
<img src="/resources/afd53e55e9938580aaf8559d13530b9205c2299e/google.svg" data-media-type="image/svg+xml" alt="Discrete-time signal">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/74f842e00010a0f4439a5803625e0b845a8ed1ff/google.eps" data-type="image"></span>
  </span>
  </figure>
You can see here that it's a discrete time signal, where
each of these signal points corresponds to one single share
price at the end of a day.  There are some fluctuations in the price, and if you were a financial trader or if you were an economist, you would be very interested in the information
that this daily share price closing signal conveys.  
Another example is a temperature signal,the temperature at Houston Intercontinental Airport every day at noon for 365 days that comprise the year 2013 (in degrees Celsius):
<figure id="iahtemp"><figcaption>Daily temperatures over the course of a year.</figcaption><span data-type="media" id="iahtemplot" data-alt="Discrete-time signal">
<img src="/resources/51975de07a00d55ad985220d49e35f14d51ba2e4/iahtemp.svg" data-media-type="image/svg+xml" alt="Discrete-time signal">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/58be10c638e260a24f61b3e46f0c9bcd27de94e6/iahtemp.eps" data-type="image"></span>
</span>
</figure>
Again, we can see that there are fluctuations in this signal, and if you were a meteorologist or a climatologist, you'd be very interested in the information that this signal conveys.  Finally, here's an audio signal that is speech from an actor speaking a part in Shakespeare's play, Hamlet:
<figure id="hamlet"><figcaption>The discrete-time plot of a speech signal.</figcaption><span data-type="media" id="hamletplot" data-alt="Discrete-time signal">
<img src="/resources/c69f53e1bc4fb6536c37442479cea67dbbfc1e24/hamlet.svg" data-media-type="image/svg+xml" alt="Discrete-time signal">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7f01d88a2f63e1aa21593f88c3da8fb668db151c/hamlet.eps" data-type="image"></span>
  </span>
  </figure></p><p id="eip-262"><span data-type="title">Plotting Discrete-Time Signals Correctly</span>We need to remember that with a discrete-time signal, the independent variable is integer valued.  This means that when you plot a signal in a program like MATLAB, you must use a discrete-time plotting function
(like the <code>stem</code> function) that respects the fact that the signal is only defined at discrete time points, rather than a function (like <code>plot</code>) which interpolates between points:
<figure id="correct"><figcaption>Discrete-time signals are undefined between the integer index values and should be plotted accordingly.</figcaption><span data-type="media" id="correctplot" data-alt="Image">
<img src="/resources/f66560135d56e8de79894a0fe56cdb66d76e84e4/sigFun.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8bf0f6cbdc146de3042f18dc2edd23fd0bdf9ba6/sigFun.eps" data-type="image"></span></span></figure><figure id="incorrect"><figcaption>This plot interpolates between the discrete-time integer index values, which is inappropriate for a discrete-time plot.</figcaption><span data-type="media" id="incorrectplot" data-alt="Image">
<img src="/resources/14843a08d97cfa267857d3d89ab6122967331caa/sigFunBad.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d26c637019f421cd9543ac524925eeda0dfbcd71/sigFunBad.eps" data-type="image"></span></span></figure></p><p id="eip-192"><span data-type="title">Plotting Complex-valued Signals</span>Up to this point, we've been talking about real-valued signals.  They comprise a single plot of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math> versus <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>. But what about complex-valued signals?</p><p id="eip-941">Recall that a complex number has a real component and an imaginary component. There are two equivalent ways of expressing a given complex number. For some <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>a</mi></mrow><mo>&#8712;</mo><mi mathvariant="double-struck">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><in></in><ci>a</ci><complexes></complexes></apply></annotation-xml></semantics></math>, we can express <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>a</mi></mrow><annotation-xml encoding="MathML-Content"><mi>a</mi></annotation-xml></semantics></math> in two different ways:</p><ul id="eip-637"><li>Cartesian/rectangular form: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>a</mi></mrow><mo>=</mo><mrow><mrow><mo>&#8476;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mi>a</mi></mrow></mfenced></mrow><mo>+</mo><mrow><mrow><mi>j</mi></mrow><mo>&#8290;</mo><mrow><mo>&#8465;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mi>a</mi></mrow></mfenced></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><ci>a</ci><apply><plus></plus><apply><real></real><ci>a</ci></apply><apply><times></times><ci>j</ci><apply><imaginary></imaginary><ci>a</ci></apply></apply></apply></apply></annotation-xml></semantics></math></li>
<li>Polar form: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>a</mi></mrow><mo>=</mo><mrow><mrow><mo>|</mo><mrow><mi>a</mi></mrow><mo>|</mo></mrow><mo>&#8290;</mo><msup><mi>e</mi><mrow><mrow><mrow><mi>j</mi></mrow><mo>&#8290;</mo><mrow><mo>&#8736;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mi>a</mi></mrow></mfenced></mrow></mrow></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><ci>a</ci><apply><times></times><apply><abs></abs><ci>a</ci></apply><apply><exp></exp><apply><times></times><ci>j</ci><apply><arg></arg><ci>a</ci></apply></apply></apply></apply></apply></annotation-xml></semantics></math>,</li></ul><p id="eip-137">where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mi>j</mi><mo>=</mo><msqrt><mn>-1</mn></msqrt></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><mi>j</mi><apply><root></root><cn>-1</cn></apply></apply></annotation-xml></semantics></math> (in engineering contexts the variable <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>j</mi></mrow><annotation-xml encoding="MathML-Content"><mi>j</mi></annotation-xml></semantics></math> is used to represent this value because <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>i</mi></mrow><annotation-xml encoding="MathML-Content"><mi>i</mi></annotation-xml></semantics></math> represents electrical current).  Just as a complex number can be expressed in two different ways, so can a complex-valued signal:</p><ul id="eip-949"><li>Cartesian/rectangular form: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mo>=</mo><mrow><mrow><mo>&#8476;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mfenced></mrow><mo>+</mo><mrow><mrow><mi>j</mi></mrow><mo>&#8290;</mo><mrow><mo>&#8465;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mfenced></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply><apply><plus></plus><apply><real></real><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply><apply><times></times><ci>j</ci><apply><imaginary></imaginary><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply></apply></apply></apply></annotation-xml></semantics></math></li>
<li>Polar form: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mo>=</mo><mrow><mrow><mo>|</mo><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mo>|</mo></mrow><mo>&#8290;</mo><msup><mi>e</mi><mrow><mrow><mrow><mi>j</mi></mrow><mo>&#8290;</mo><mrow><mo>&#8736;</mo><mo>&#8290;</mo><mfenced open="(" close=")" separators=","><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mfenced></mrow></mrow></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply><apply><times></times><apply><abs></abs><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply><apply><exp></exp><apply><times></times><ci>j</ci><apply><arg></arg><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply></apply></apply></apply></apply></annotation-xml></semantics></math></li></ul><p id="eip-181">What this means is that, if we're plotting a complex-valued signal, we actually need two plots.  As we have seen, there are two different ways we can plot the same complex-valued signal:</p><ul id="eip-437"><li>Cartesian/rectangular form:
<figure id="compsigr"><figcaption>A plot of the real part of a complex-valued signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.</figcaption><span data-type="media" id="compsigrplot" data-alt="Image">
<img src="/resources/fadbe7eb8f332ca859934031e240ceab1983494b/compSigR.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/e574c4a171b8478811c50599276c402a6ba82c1c/compSigR.eps" data-type="image"></span></span>
  </figure><figure id="compsigi"><figcaption>A plot of the imaginary part of a complex-valued signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.</figcaption><span data-type="media" id="compsigiplot" data-alt="Image">
<img src="/resources/75e7acbd39a77514723d7dda25a20338882bdbfc/compSigI.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b43f158a6a1ee5277dbf5cff966acd761be3b224/compSigI.eps" data-type="image"></span></span>
  </figure></li>
<li>Polar form:
<figure id="compSigM"><figcaption>A plot of the magnitude of a complex-valued signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.  Note how all of the values are greater than or equal to 0.</figcaption><span data-type="media" id="compSigMplot" data-alt="Image">
<img src="/resources/de3a24802c29aca7ccba0d2a71a4a758adfb9e96/compSigM.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5701589020f2b0dda93e805b9e8ec218904a9cac/compSigM.eps" data-type="image"></span></span>
  </figure><figure id="compSigP"><figcaption>A plot of the phase of a complex-valued signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math>.  Note how the values range between <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mo>-</mo><mi>&#960;</mi></mrow><annotation-xml encoding="MathML-Content"><mo>-</mo><pi></pi></annotation-xml></semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>&#960;</mi></mrow><annotation-xml encoding="MathML-Content"><pi></pi></annotation-xml></semantics></math>.</figcaption><span data-type="media" id="compSigPplot" data-alt="Image">
<img src="/resources/afa4b5e2216c9d6fb32cfc429da61c4efb83f3eb/compSigP.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d9870b461ad670a3a108421ab616e72d7870c59d/compSigP.eps" data-type="image"></span></span>
  </figure></li></ul></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8491d0e5-4142-43f2-890c-a8807e0f9b02%401.html" data-type="page"><h1>Signal Properties</h1><div data-type="document-title">Signal Properties</div>
  <p id="eip-890"><span data-type="title">Signal Classification</span>Signals can be broadly classified as discrete-time or continuous-time, depending on whether the independent variable is integer-valued or real-valued.  Signals may also be either real-valued or complex-valued.  We will now consider some of the other ways we can classify signals.</p><p id="delete_me"><span data-type="title">Signal Length: Finite/Infinite</span>This classification is just as it sounds. An <span data-type="term">infinite-length</span> discrete-time signal takes values for all time indices: all integer values <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math> on the number line from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mo>-</mo><mi>&#8734;</mi></mrow><annotation-xml encoding="MathML-Content"><mo>-</mo><infinity></infinity></annotation-xml></semantics></math> all the way up to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>&#8734;</mi></mrow><annotation-xml encoding="MathML-Content"><infinity></infinity></annotation-xml></semantics></math>.  A <span data-type="term">finite-length</span> signal is defined only for a certain range of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>, from some <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mi>N </mi><mn>1</mn></msub></mrow><annotation-xml encoding="MathML-Content"><msub><mi>N </mi><mn>1</mn></msub></annotation-xml></semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mi>N </mi><mn>2</mn></msub></mrow><annotation-xml encoding="MathML-Content"><msub><mi>N </mi><mn>2</mn></msub></annotation-xml></semantics></math>.  The signal is not defined outside of that range.</p><p id="eip-993"><span data-type="title">Signal Periodicity</span>As the name suggests, <span data-type="term">periodic</span> signals are those that repeat themselves.  Mathematically, this means that there exists some integer value <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>N</mi></mrow><annotation-xml encoding="MathML-Content"><mi>N</mi></annotation-xml></semantics></math> for which <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mrow><mi>n</mi></mrow><mo>+</mo><mrow><mi>N</mi></mrow></mrow></mfenced></mrow><mo>=</mo><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><apply><ci type="fn" class="discrete">x</ci><apply><plus></plus><ci>n</ci><ci>N</ci></apply></apply><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply></annotation-xml></semantics></math>, for all values of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>. So if we define a fundamental period of this particular signal of length, like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><mi>N</mi><mn>8</mn></apply></annotation-xml></semantics></math>, then we will see the same signal values shifted by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>8</mn></mrow><annotation-xml encoding="MathML-Content"><mn>8</mn></annotation-xml></semantics></math> time indices,
by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>16</mn></mrow><annotation-xml encoding="MathML-Content"><mn>16</mn></annotation-xml></semantics></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>-8</mn></mrow><annotation-xml encoding="MathML-Content"><mn>-8</mn></annotation-xml></semantics></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>-16</mn></mrow><annotation-xml encoding="MathML-Content"><mn>-16</mn></annotation-xml></semantics></math>, etc.  Below is an example of a periodic signal:
<figure id="sigFunPer"><figcaption>A periodic discrete-time signal.  Note how it repeats every 8 time units.</figcaption><span data-type="media" id="sigFunPerplot" data-alt="Image">
<img src="/resources/4ee5759ec9d8b72b91968ff59ea6e96b081b4599/sigFunPer.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a28b424501a3c17ec2ab91f8897f9bfcd380dffe/sigFunPer.eps" data-type="image"></span></span>
  </figure>
So periodic signals repeat, and clearly periodic signals
are going to be, therefore, infinite in length.
It's also important to remember that to be periodic in discrete-time, the period <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>N</mi></mrow><annotation-xml encoding="MathML-Content"><mi>N</mi></annotation-xml></semantics></math> must be an integer.  If there is no such integer-valued <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>N</mi></mrow><annotation-xml encoding="MathML-Content"><mi>N</mi></annotation-xml></semantics></math> for which <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mrow><mi>n</mi></mrow><mo>+</mo><mrow><mi>N</mi></mrow></mrow></mfenced></mrow><mo>=</mo><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq><apply><ci type="fn" class="discrete">x</ci><apply><plus></plus><ci>n</ci><ci>N</ci></apply></apply><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></apply></annotation-xml></semantics></math> (for all values of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mi>n</mi></mrow><annotation-xml encoding="MathML-Content"><mi>n</mi></annotation-xml></semantics></math>), then we classify the signal as being <span data-type="term">aperiodic</span>.
</p><p id="eip-96"><span data-type="title">Converting Between Infinite and Finite Length</span>In different applications, the need will arise to convert a signal from infinite-length to finite-length, and vice versa.  There are many ways this operation can be accomplished, but we will consider the most common.</p><p id="eip-707">The most straightforward way to create a finite-length signal from an infinite-length one is through the process of <span data-type="term">windowing</span>.  A windowing operation extracts a contiguous portion of an infinite-length signal, that portion becoming the new finite-length signal.  Sometimes a window will also scale the smaller portion in a particular way.  Below is a mathematical expression of windowing (without any scaling):</p><p id="eip-799"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow>
<mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mtext>&#160; if &#160;</mtext><mrow><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>&#8804;</mo><mi>n</mi></mrow><mo>&#8804;</mo><msub><mi>N</mi><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd><mtext>undefined</mtext><mtext>&#160; if &#160;</mtext><mtext>else</mtext></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq>
     <apply><ci type="fn" class="discrete">y</ci>
     <ci>n</ci></apply>

     <piecewise>
          <piece>
                <apply><ci type="fn" class="discrete">x</ci>
     <ci>n</ci></apply>
                <apply><leq></leq><apply><leq></leq><msub><mi>N</mi><mn>1</mn></msub><mi>n</mi>
                </apply><msub><mi>N</mi><mn>2</mn></msub></apply>
          </piece>
          <piece><mtext>undefined</mtext><mtext>else</mtext></piece>
     </piecewise>
</apply></annotation-xml></semantics></math></p><p id="eip-828">Below is a signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> (assume it is infinite-length, with only a part of it shown), with a portion of it extracted to create <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math>:
<figure id="sigFunW" data-orient="vertical"><figcaption>(a) an infinite-length signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> (only part of it shown) has a portion extracted via windowing to create (b) a finite-length signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math>.</figcaption><figure id="sigFunW1plot"><figcaption>Infinite-length signal (only a portion of it is shown)</figcaption>
<span data-type="media" id="sigFunW1" data-alt="Image">
<img src="/resources/b8ef71e07005fbd464563cdf6893199fb2503b5b/sigFunW1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/18372ca9e76e0a6b09cc9758161eda785c848f51/sigFunW1.eps" data-type="image"></span></span>
</figure>
<figure id="sigFunW2"><figcaption>Finite-length signal</figcaption>
<span data-type="media" id="sigFunW2plot" data-alt="Image">
<img src="/resources/e304584c427a4b24a154799da0cdf9b6bf3f98b5/sigFunW2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/32a5eade3a648193faa707a585cce16dc41f0516/sigFunW2.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-716">There are two ways a signal can be converted from a finite-length to infinite-length.  The first is referred to as <span data-type="term">zero-padding</span>.  It is easy to take a finite-length signal and then make a larger finite-length signal out of it: just extend the time axis.  We have to decide what values to put in the new time locations, and simply putting <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mn>0</mn></mrow><annotation-xml encoding="MathML-Content"><mn>0</mn></annotation-xml></semantics></math> at all the new locations is a common approach. Here is how it looks, mathematically, to create a longer signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math> from a shorter signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> defined only on <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>&#8804;</mo><mi>n</mi></mrow><mo>&#8804;</mo><msub><mi>N</mi><mn>2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><leq></leq><apply><leq></leq><msub><mi>N</mi><mn>1</mn></msub><mi>n</mi>
                </apply><msub><mi>N</mi><mn>2</mn></msub></apply></annotation-xml></semantics></math>:</p><p id="eip-563"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow>
<mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mtext>&#160; if &#160;</mtext><mrow><mrow><msub><mi>N</mi><mn>0</mn></msub><mo>&#8804;</mo><mi>n</mi></mrow><mo>&lt;</mo><msub><mi>N</mi><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow><mtext>&#160; if &#160;</mtext><mrow><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>&#8804;</mo><mi>n</mi></mrow><mo>&#8804;</mo><msub><mi>N</mi><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd><mn>0</mn><mtext>&#160; if &#160;</mtext><mrow><mrow><msub><mi>N</mi><mn>2</mn></msub><mo>&lt;</mo><mi>n</mi></mrow><mo>&#8804;</mo><msub><mi>N</mi><mn>3</mn></msub></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><eq></eq>
     <apply><ci type="fn" class="discrete">y</ci>
     <ci>n</ci></apply>

     <piecewise>

<piece>
                <mn>0</mn>
                <apply><lt></lt><apply><leq></leq><msub><mi>N</mi><mn>0</mn></msub><mi>n</mi>
                </apply><msub><mi>N</mi><mn>1</mn></msub></apply>
          </piece>

          <piece>
                <apply><ci type="fn" class="discrete">x</ci>
     <ci>n</ci></apply>
                <apply><leq></leq><apply><leq></leq><msub><mi>N</mi><mn>1</mn></msub><mi>n</mi>
                </apply><msub><mi>N</mi><mn>2</mn></msub></apply>
          </piece>
          
<piece>
                <mn>0</mn>
                <apply><leq></leq><apply><lt></lt><msub><mi>N</mi><mn>2</mn></msub><mi>n</mi>
                </apply><msub><mi>N</mi><mn>3</mn></msub></apply>
          </piece>


     </piecewise>
</apply></annotation-xml></semantics></math></p><p id="eip-935">Here, obviously <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><msub><mi>N</mi><mn>0</mn></msub><mo>&lt;</mo><mrow><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>&lt;</mo><msub><mi>N</mi><mn>2</mn></msub></mrow><mo>&lt;</mo><msub><mi>N</mi><mn>3</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><lt></lt><msub><mi>N</mi><mn>0</mn></msub>
<apply><lt></lt><apply><lt></lt><msub><mi>N</mi><mn>1</mn></msub><msub><mi>N</mi><mn>2</mn></msub>
                </apply><msub><mi>N</mi><mn>3</mn></msub></apply></apply></annotation-xml></semantics></math>, and if we extend <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mrow><mi>N</mi></mrow><mn>0</mn></msub></mrow><annotation-xml encoding="MathML-Content"><msub><ci>N</ci><mn>0</mn></msub></annotation-xml></semantics></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><msub><mi>N</mi><mn>3</mn></msub></mrow><annotation-xml encoding="MathML-Content"><msub><mi>N</mi><mn>3</mn></msub></annotation-xml></semantics></math> to negative and positive infinity, respectively, then <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math> will end up being infinite-length.</p><p id="eip-248">The other way to make an infinite-length signal from a finite-length one is to <span data-type="term">periodize</span> it, which means replicating a finite-length signal over and over to create an infinite-length periodic version.  Mathematically, that means defining the new infinite-length periodic signal like this:<code>\begin{eqnarray*}
y[n] &amp;=&amp; \sum_{m=-\infty}^{\infty} x[n-mN], \quad n\in\Integers \\
 &amp;=&amp; \cdots + x[n+2N] + x[n+N] + x[n] + x[n-N] + x[n-2N] + \cdots
\end{eqnarray*}</code></p><p id="eip-58">Graphically, we can see that this amounts to repeating the signal over and over, before and after the original portion:
<figure id="sigFunPers" data-orient="vertical"><figcaption>(a) finite-length signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> is periodized to create (b) an infinite-length signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math> (only a portion of it is shown).</figcaption><figure id="sigFunPer1plot"><figcaption>Finite-length signal</figcaption>
<span data-type="media" id="sigFunPer1" data-alt="Image">
<img src="/resources/ad6b547c91d336c43d08e9affde2a14ce3524a42/sigFunPer1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/3c4459a68563b5ea2ef96f122ccef4af77f0a95c/sigFunPer1.eps" data-type="image"></span></span>
</figure>
<figure id="sigFunPer2plot"><figcaption>Original signal periodized to create an infinite-length signal.</figcaption>
<span data-type="media" id="sigFunPer2" data-alt="Image">
<img src="/resources/946c9355d109b1c99b0a74b473c4bbe02779a199/sigFunPer2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b7da17d46025b764d789fb8b3cfdde65d36a6cae/sigFunPer2.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-711"><span data-type="title">Periodization and Modular Arithmetic</span>It turns out that, as we consider periodization and periodic signals, the notion of modular arithmetic will be helpful.  In modular arithetic, integers do not lie on a line stretching from negative infinity to infinity, but rather on a circle of a defined size $N$. In modulo-8, for example, the numbers are 8 "hours" on a "clock."  Our convention will be for the numbers to traverse from 0 to $N-1$, counterclockwise:
<figure id="clock1"><figcaption>CAPTION.</figcaption><span data-type="media" id="clock1plot" data-alt="Image">
<img src="/resources/8902afba4c5c96b97f87f7e72f86890d09e13db7/clock1.png" data-media-type="image/png" alt="Image" width="150">
</span>
  </figure></p><p id="eip-29">Consider a finite-length signal of size $N$. We can align the time-dependent values of the signal on the modulo circle:
<figure id="clockSig1"><figcaption>CAPTION.</figcaption><span data-type="media" id="clockSig1plot" data-alt="Image">
<img src="/resources/7f2240fb6f7120f92a4b9fb95d01cb68fc2e6aec/clockSig1.png" data-media-type="image/png" alt="Image" width="200">
</span></figure>
When we travel around the clock once, from time index 0 to 7, we express the finite-length signal.  But if we keep traveling, in one direction or the other, then that amounts to periodizing the signal.  Using our modulo notation, we can periodize a finite-length signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>x</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">x</ci><ci>n</ci></apply></annotation-xml></semantics></math> to be an infinite-length periodic signal <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><semantics><mrow><mrow><mrow><mi>y</mi></mrow><mo>&#8290;</mo><mfenced open="[" close="]" separators=","><mrow><mi>n</mi></mrow></mfenced></mrow></mrow><annotation-xml encoding="MathML-Content"><apply><ci type="fn" class="discrete">y</ci><ci>n</ci></apply></annotation-xml></semantics></math> like this: $y[n]=x[(n)_N]$.</p><p id="eip-783"><span data-type="title">Finite/Periodic Signals Relationship</span>We have seen that we can take an $N$-length finite-length signal and periodize it to make an infinite-length periodic signal with a period of $N$.  By the same token, we can also work in reverse and extract one period worth of signal from any periodic signal to create a finite-length signal. <strong>What this means is we can consider periodic signals and finite-length signals to be essentially equivalent</strong>: we can consider just one period of a periodic signal (the rest of the signal is redundant, by definition), or periodize a finite-length signal.  They are two ways of looking at the same thing, a phenomenon we will often see in our study of signals and systems, and we will choose the perspective that best suits our needs for particular applications.</p><p id="eip-677"><span data-type="title">Shifting Infinite-length Signals</span>Given some signal $x[n]$, it will often be necessary for us to consider that signal shifted in time.  We denote such a shift mathematically with an expression like $x[n-m]$, where $m$ is some integer.  If $m$ is greater than zero, then $x[n-m]$ will be just like $x[n]$, except it will be shifted to the right by $m$ time units.  If $m$ is less than zero, it will be shifted to the left.  Here is who that might look for a couple values of $m$:
<figure id="sigfunshifts" data-orient="vertical"><figcaption>A (a) signal $x[n]$ shifted according to the expression $x[n-m]$ where $m$ is (b) positive, and (c) negative.</figcaption><figure id="sfs1"><figcaption>Original signal $x[n]$.</figcaption>
<span data-type="media" id="sfs1p" data-alt="Image">
<img src="/resources/27dcc4a61252a73e1aa6ab320d9e108815d9f74e/sigFunS1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/fc229efb119b82616880d422c1224c9fcba3bb9a/sigFunS1.eps" data-type="image"></span></span>
</figure>
<figure id="sfs2"><figcaption>$x[n]$ shifted to the right.</figcaption>
<span data-type="media" id="sfs2p" data-alt="Image">
<img src="/resources/316a8e953af385be97c184500023704e6a21a300/sigFunS2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/428d939924a63e73b5b8bd1ccac4d2867fae32f4/sigFunS2.eps" data-type="image"></span></span>
</figure>
<figure id="sfs3"><figcaption>$x[n]$ shifted to the left.</figcaption>
<span data-type="media" id="sfs3p" data-alt="Image">
<img src="/resources/ccba2aa3db81f429a0aaa449f27121036f550e62/sigFunS3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/95e7c451ff44ee98e050d358486dd60998f2a0de/sigFunS3.eps" data-type="image"></span></span>
</figure>
</figure>
This type of shifting works the same with periodic signals:
<figure id="perfunshifts" data-orient="vertical"><figcaption>A periodic signal shifted one value in time.</figcaption><figure id="sfps0"><figcaption>Original periodic signal $y[n]$.</figcaption>
<span data-type="media" id="sfps0p" data-alt="Image">
<img src="/resources/0b14ae1b38fff8512c3da1e04944772285a07cc8/sigFunPerS0.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/6a8c413ec379acf7d714b738f32eff4fcb41e5a1/sigFunPerS0.eps" data-type="image"></span></span></figure>
<figure id="sfps1"><figcaption>$y[n]$ shifted to the right.</figcaption><span data-type="media" id="sfps1p" data-alt="Image">
<img src="/resources/0762ca97c16a18e44a63729ab5e7fb28d5570c35/sigFunPerS1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a34335f18d491476960433b631dc10f27dd23c9e/sigFunPerS1.eps" data-type="image"></span></span>
</figure>
</figure>
Of course, for periodic signals, certain shifts actually do not have any effect on them.  If a signal repeats with a period of $N$, then shifting that signal by any integer multiple of $N$ will yield the original signal.  Take a look at the signal above, which was shifted to the right by a time unit of 1.  If we keep on shifting it until it is shifted 8 time units the result will be identical to the original signal.</p><p id="eip-192"><span data-type="title">Shifting Finite-Length Signals</span>Can finite-length signals be shifted, as well?  There does not seem to be any reason why not.  Suppose we have some finite-length signal $x[n]$ (of length $N$) and we define another finite-length signal $v[n]$ to be $v[n]=x[n-1]$. So we have $v[1]=x[0]$ and $v[2]=x[1]$ and so on until $v[N-1]=x[N-2]$. But what about $v[0]$, what do we put there?  And how about $x[N-1]$, where is that supposed to go?  We do not want to invent information to put in $v[0]$, nor lose the information of $x[N-1]$. An elegant solution is to periodize $x[n]$, and then consider the relation $v[n]=x[n-1]$.  In this case, we now have a value for $v[0]$: $v[0]=x[-1]$.  Since $x[n]$ is periodic with period $N$, it also happens that $x[-1]=x[N-1]$, so we do not lose that information.</p><p id="eip-545">This kind of operation, for finite-length signals, is called a <span data-type="term">circular shift</span>, and we can express it mathematically with the help of our modular arithmetic operator. Circularly shifting a finite-length signal $x[n]$ by $m$ time units is expressed as $x[(n-m)_N]$. It can also be visualized by turning $x[n]$ about the circle on which it resides:
<figure id="clockshifts" data-orient="horizontal"><figcaption>Circularly shifting a signal by $m$ amounts to turning it counter-clockwise $m$ steps.</figcaption><figure id="cs1"><figcaption>Original finite-length signal $x[n]$.</figcaption>
<span data-type="media" id="cs1p" data-alt="Image">
<img src="/resources/7f2240fb6f7120f92a4b9fb95d01cb68fc2e6aec/clockSig1.png" data-media-type="image/png" alt="Image" width="150">
</span></figure>
<figure id="cs2"><figcaption>$x[(n-3)_8]$</figcaption><span data-type="media" id="cs2p" data-alt="Image">
<img src="/resources/e60f938b2cca42582d3f87baa7b01e8de5a86cb8/clockSig2.png" data-media-type="image/png" alt="Image" width="150">
</span>
</figure>
</figure></p><p id="eip-753"><span data-type="title">Time Reversing Finite-Length Signals</span>For infinite length signals, the transformation of reversing the time axis $x[&#8722;n]$ is obvious: just flip the signal about $n=0$. But things are not quite so obvious for finite-length signals; if a signal is defined for, say, $n$ between 0 and N, then what gets flipped across the $n=0$ from the negative side? Once again, it turns out the modular arithmetic opererator can be called in for help. We reverse the time axis, modulo N: $x[(-n)_N]$. Below is an image of a finite-length ($N=8$) signal, time-reversed:
<figure id="clock4"><figcaption>We can time reverse a finite-length signal $x[n]$ with the mathematical expression $x[(-n)]_8$.</figcaption><span data-type="media" id="clock4plot" data-alt="Image">
<img src="/resources/c5087a5416dd6b6c0f1b5c41e1d60fc9183943c4/clockSig4.png" data-media-type="image/png" alt="Image" width="150">
</span>
  </figure></p><p id="eip-641"><span data-type="title">Signal Causality</span>A signal $x[n]$ is <span data-type="term">causal</span> if $x[n]=0$ for all $n \lt 0$, it is <span data-type="term">anti-causal</span> if $x[n]=0$ for all $n \geq 0$, and it is <span data-type="term">acausal</span> if is neither causal nor anti-causal.

<figure id="sigFunCausal" data-orient="vertical"><figure id="sigFunCausplot"><figcaption>A causal signal.</figcaption>
<span data-type="media" id="sigFunCaus" data-alt="Image">
<img src="/resources/59fe31427469cee7aa7bca156a09e8c09f991698/sigFunCaus.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/246650c6ac299b978140a1c2acd09b7b9fdc9cfd/sigFunCaus.eps" data-type="image"></span></span>
</figure>
<figure id="sigFunACausplot"><figcaption>An anti-causal signal.</figcaption>
<span data-type="media" id="sigFunACaus" data-alt="Image">
<img src="/resources/78b41cc42f9301f4650012d9b062a383806837c6/sigFunACaus.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/df42a09b6864426005a6856030dfc05515b5b1d9/sigFunACaus.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-560"><span data-type="title">Even and Odd Signals</span>A signal $x[n]$is defined as <span data-type="term">even</span> if $x[-n]=x[n]$, and <span data-type="term">odd</span> if $x[-n]=-x[n]$.
<figure id="sigevod" data-orient="vertical"><figure id="sigFunE1plot"><figcaption>An even signal.</figcaption>
<span data-type="media" id="sigFunE1" data-alt="Image">
<img src="/resources/119e01285094f6fd0fc0e565fe77b62269b3cb8e/sigFunE1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d111421e967c2c7fcfc0237997caae81dd6c32b1/sigFunE1.eps" data-type="image"></span></span>
</figure>
<figure id="sigFunO1plot"><figcaption>An odd signal.</figcaption>
<span data-type="media" id="sigFunO1" data-alt="Image">
<img src="/resources/359a743e6ea9561f4d63172f20134a59cff6655e/sigFunO1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8d73fb06ef708b4e155720b4280f9dcee2da6a5d/sigFunO1.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-215"><span data-type="title">Even/Odd Signal Decomposition</span>MM: Is it really necessary to include this? Usage of the concept (wavelets?) is outside the scope of the class.</p><p id="eip-988"><span data-type="title">Digital Signals</span><span data-type="term">Digital</span> signals are a special sub-class of discrete-time signals.  While the independent time variable for discrete-time signals is integer-valued, the dependent variable (i.e., the value the signal takes at any given time) can take on any value. However, for digital signals, both of these variables are discrete-valued.  Rather than take any value on a continuum, discrete signals take only a limited number of values, or <span data-type="term">levels</span>. Typically, the number of levels is expressed as $D = 2^q$, and each possible value of $x[n]$ is represented as a digital code with $q$ bits.
<figure id="sigFunQ"><figcaption>A digital signal with $q = 2$ bits, so $D = 2^2= 4$ levels.</figcaption><span data-type="media" id="sigFunQplot" data-alt="Image">
<img src="/resources/b769105c9fe7aa12b12c25054ccfac2a6bea1f51/sigFunQ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/bbb3f1345f6c10badc3a5211cbcd18d6528e9fcd/sigFunQ.eps" data-type="image"></span></span>
  </figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A19005586-911a-409b-8e69-3ad81a78e1bb%401.html" data-type="page"><h1>Key Discrete-time Test Signals</h1><div data-type="document-title">Key Discrete-time Test Signals</div>
  <p id="delete_me">In our study of discrete-time signals and signal processing, there are five very important signals that we will use to both illustrate signal processing concepts, and also to probe or test signal processing systems: the <span data-type="term">delta function</span>, the <span data-type="term">unit step function</span>, the <span data-type="term">unit pulse function</span>, the <span data-type="term">real exponential function</span>, <span data-type="term">sinusoidal functions</span>, and <span data-type="term">complex exponential functions</span>.  This module will consider the first four; sinusoids and complex exponentials are particularly important, so a separate model will cover them. Each of these signals will be introduced as infinite-length signals, but they all have straightforward finite-length equivalents.</p><p id="eip-208"><span data-type="title">The Discrete-time Delta Function</span>The delta function is probably the simplest nontrivial signal. It is represented mathematically with (no surprise) the Greek letter delta: $\delta[n]$. It takes the value 0 for all time points, except at the time point 0 where it peaks up to the value 1:
$\delta[n]=\begin{cases}1 &amp; n=0 \\ 0 &amp;
\textrm{otherwise}\end{cases}$
<figure id="delta"><figcaption>The discrete-time delta function.</figcaption><span data-type="media" id="deltaplot" data-alt="Image">
<img src="/resources/882406d947ad14a07afcf872262c11c0f8faa820/delta.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/81761a9c322932fefae4da0000e7cf85c037db48/delta.eps" data-type="image"></span>
</span></figure>
In a variety of important settings, we will often see the delta function shifted by a particular time value. The delta function $\delta[n-m]$ is 0, except for a peak of 1 at time $m$:
<figure id="delta9"><figcaption>A time-shifted discrete-time delta function $\delta[n-m]$, where $m=9$.</figcaption><span data-type="media" id="delta9plot" data-alt="Image">
<img src="/resources/18f9a19cd82bcead41d1bde8b61e7523865290a9/delta9.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/442bee6f557f0cd59ec78622668942d12f503c19/delta9.eps" data-type="image"></span>
</span></figure>
One of the reasons the shifted delta function is so useful is that we can use it to select, or sample, a value of another signal at some defined time value. Suppose we have some signal $x[n]$, and we would like to isolate that signal's value at time $m$. What we can do is multiply that signal by a shifted delta signal. We can say $y[n]=x[n]\delta[n-m]$, but since that $y[n]$ will be zero for all $n$ except at $n=m$, it is equivalent to express it as $y[n]=x[m]\delta[n-m]$, where now $x[m]$ is no longer a function, but a constant. The following figure shows how this operation isolates a particular time sample of $x[n]$:
<figure id="deltas" data-orient="vertical"><figcaption>Using a time-shifted delta function to isolate a sample of the signal $x[n]$.</figcaption><figure id="sigFunD1plot">
<span data-type="media" id="sigFunD1" data-alt="Image">
<img src="/resources/7fb016d0a51331a899ca5344c4be236a1332d3d3/sigFunD1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4c83f52ec7e60e91ea34ff8e77f789839f04574c/sigFunD1.eps" data-type="image"></span></span>
</figure>
<figure id="deltaD9">
<span data-type="media" id="deltaD9plot" data-alt="Image">
<img src="/resources/ec746d994d2c975e404536be62d1097327651430/deltaD9.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a89ae0fe7ee1bb3be88b1650e83cb440feb122ae/deltaD9.eps" data-type="image"></span></span>
</figure>
<figure id="sigDelta">
<span data-type="media" id="sigDeltaplot" data-alt="Image">
<img src="/resources/cc683b9d10067e2fc34da03207943de85feec9f7/sigDelta.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b4dfa61abf91ea856bf754aae1c1b80af17b7455/sigDelta.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-768"><span data-type="title">The Unit Step Function</span>The unit step function can be thought of like turning on a switch. Usually identified as $u[n]$, it is $0$ for all $n \lt 0$, and then at $n=0$ it "switches on" and is $1$ for all $n \geq 0$: $u[n]=\begin{cases}1 &amp; n \lt 0\\ 1 &amp; n\geq 0\end{cases}$:
<figure id="step"><figcaption>The unit step function.</figcaption><span data-type="media" id="stepplot" data-alt="Image">
<img src="/resources/c8c6169f9a0db1ff9b347d08682dddb79c26a8bd/step.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/88da5027dff801eb96b1dd3e8a8104f653e0e1aa/step.eps" data-type="image"></span>
</span></figure>
As with the delta function, it will also be useful for us to shift the step function:
<figure id="stepS"><figcaption>A shifted step function $[n-m]$ with $m=5$.</figcaption><span data-type="media" id="stepSplot" data-alt="Image">
<img src="/resources/4ca8febe6297bf8100f1468445344a4fa51996ba/stepS.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d4729dfd7e4ef29f8cac7d1d45d1d28bc5ca0259/stepS.eps" data-type="image"></span>
</span></figure>
And, as you might have guessed, we can use a shifted step function in a similar way to the delta function by multiplying it with another signal.  Whereas the delta function selected a single value of a certain signal (zeroing out the rest), the step function isolates a portion of a signal after a given time.  Below, a step function is used to zero out all the values of $x[n]$ for $n\lt 5$, keeping the rest:
<figure id="sigfunss" data-orient="vertical"><figcaption>A shifted step function can be used to zero out all values of a signal before a certain time index.</figcaption><figure id="sigFunS5plot">
<span data-type="media" id="sigFunS5" data-alt="Image">
<img src="/resources/7fb016d0a51331a899ca5344c4be236a1332d3d3/sigFunS5.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/fd6e0b855682b0616aecc67db47477972144d311/sigFunS5.eps" data-type="image"></span></span>
</figure>
<figure id="stepS5">
<span data-type="media" id="stepS5plot" data-alt="Image">
<img src="/resources/10f35f549506c8836e3d835a6f3fadd12a2e7c85/stepS5.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8be8d94cb51433e9416a49313aa863f3084f47f7/stepS5.eps" data-type="image"></span></span>
</figure>
<figure id="sigStepS5">
<span data-type="media" id="sigStepS5plot" data-alt="Image">
<img src="/resources/8d5fc6e0b4146cdad59d233b6d9dcf69c5762baf/sigStepS5.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b5e268e1e174bd6a6e49708f745fdea643a435ab/sigStepS5.eps" data-type="image"></span></span>
</figure>
</figure>
Supposing a signal $x[n]$ were not causal, setting $m$ to zero and performing the operation $x[n]u[n]$ would zero out all values of $x[n]$ before $n=0$, thereby making the result causal.</p><p id="eip-26"><span data-type="title">The Unit Pulse Function</span>The unit pulse $p[n]$ is very similar to the unit step function in how it "switches on" from 0 to 1, but then it also "switches off" at a later time. We will say it "switches on" at time $N_1$, and "off" at time $N_2$: $p[n] = \begin{cases}
	0 &amp; n\lt N_1 \\
	1 &amp; N_1 \leq n \leq N_2 \\
	0 &amp; n\gt N_2\\
\end{cases}$
<figure id="pulse"><figcaption>The unit pulse function $p[n]$, here with $N_1 = &#8722;5$ and $N_2
= 3$.</figcaption><span data-type="media" id="pulseplot" data-alt="Image">
<img src="/resources/6f6be89395441bbd9195c2aa9168847b0778d97a/pulse.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/fccaf647008d613a647d29e594e9a1643f425925/pulse.eps" data-type="image"></span>
</span></figure>
Of course, rather than use the above piece-wise notation, it is also possible to express the pulse as the difference of two step functions: $p[n] = u[n-N_1] - u[n-(N_2+1)]$.</p><p id="eip-600"><span data-type="title">The Real Exponential Function</span>Finally, we have the real exponential function, which takes a real number $a$ (that we are going to assume is positive) and raises it to the power of $n,$ where $n$ is the time index: $r[n] = a^n$, $a\in R$, $a\geq 0$. So at $n=0$, $r[n]=a^0$, at $n=1$ it equals $a$, is $a^2$ at $n=2$, and so on. As the name suggests, the signal will exponentially increase or decrease, depending on the value of $a$.
<figure id="rexps" data-orient="vertical"><figure id="rexp2plot"><figcaption>For $a\gt 1$, the real exponential function increases with time.  Here $a=1.1$.</figcaption>
<span data-type="media" id="rexp2" data-alt="Image">
<img src="/resources/1d90dbf5525e7478e9811fcb0191af04bc93f110/rexp2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/70101d35dd2ab43e578eeaaba94f3e2eb848a772/rexp2.eps" data-type="image"></span></span>
</figure>
<figure id="rexp1"><figcaption>For $0 \lt a\lt 1$, the real exponential function decreases with time (or we could say it increases exponentially as the time index decreases).  Here $a=.9$.</figcaption>
<span data-type="media" id="rexp1plot" data-alt="Image">
<img src="/resources/29ddc70b0ea9fe53c917c9cfffc75594640de087/rexp1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/0bc70f6cc170a972b98bd9d8c339aa63924a9532/rexp1.eps" data-type="image"></span></span>
</figure></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A9ece27c4-6ab2-4f5b-a9d2-2912af734d2b%401.html" data-type="page"><h1>Real and Complex Sinusoidal Signals</h1><div data-type="document-title">Real and Complex Sinusoidal Signals</div>
  <p id="delete_me"><!-- Insert module text here -->
Discrete-time real and complex valued sinusoidal signals are an incredibly important signal class in the study of discrete-time signals and systems. Of course, sinusoidal waves show up in all sorts of science and engineering applications, but they are particularly relevant for signal processing because they are the foundation of Fourier analysis.</p><p id="eip-330"><span data-type="title">Real Valued Sinusoids</span>There are two real-valued discrete-time sinusoidal wave signals: the <span data-type="term">sine</span> wave signal and the <span data-type="term">cosine</span> wave signal. They are represented mathematically as $\sin(\omega n +\phi)$ and $\cos(\omega n +\phi)$. Let's take a look a those in more detail. First, as we have seen with other discrete-time signals, $n$ is the independent variable time index, and it runs from negative infinity to infinity. The variable $\omega$ is known as the <span data-type="term">frequency</span> of the sinusoidal signal, and we will see how changing the value of $\omega$ impacts the rate of the signal's oscillation. The variable $\phi$ is the <span data-type="term">phase</span> of the signal, and changing it will shift the signal left along the time axis. Finally, the terms $\sin$ or $\cos$ return the corresponding trigonometric value to $\omega n +\phi$ for each value of the time index $n$. Here are a few examples of real sinusoidal waves:
<figure id="sines" data-orient="vertical"><figcaption>(a) A plot of $\cos(0n)$. At every point in time, this signal takes the value $\cos(0)=1$. (b) A plot of $\sin(0n)$. At every point in time, this signal takes the value $\sin(0)=0$. (c) A plot of $\sin(\frac{\pi}{4}n+\frac{2\pi}{6})$. (d) A plot of $\cos(\pi n)$. Note how when $\omega=\pi$, as in this example, the signal is oscillating as rapidly as possible, between -1 and 1 at every single time instance. This phenomenon is the opposite of when $\omega=0$, for which the signal does not oscillate at all. So in some sense we can see that $0$ is the lowest possible frequency $\omega$, and $\pi$ is the highest.</figcaption><figure id="sinuEx1plot">
<span data-type="media" id="sinuEx1" data-alt="Image">
<img src="/resources/10697ab44dfa9f76a19996ab74a801012359f751/sinuEx1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9104ab4a5290e668da7622810264b8931ef5160d/sinuEx1.eps" data-type="image"></span></span>
</figure>
<figure id="sinuEx3">
<span data-type="media" id="sinuEx3plot" data-alt="Image">
<img src="/resources/8c054011df6e07eb4629159502909f6b49858a46/sinuEx3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/79175870e573e27fc6f6bd7013bb8f5561c0c940/sinuEx3.eps" data-type="image"></span></span>
</figure>
<figure id="sinuEx2">
<span data-type="media" id="sinuEx2plot" data-alt="Image">
<img src="/resources/51e1b173b902ec929f2443efed73f75ec2da257c/sinuEx2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/78c0a24a6c570794bbba15f51d49a2db3a97cba7/sinuEx2.eps" data-type="image"></span></span>
</figure>
<figure id="sinuEx4">
<span data-type="media" id="sinuEx4plot" data-alt="Image">
<img src="/resources/a88827d777f83ca94e96ef341831ec8238f70e5b/sinuEx4.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/489d5c0cecc4ae66c9db3313be3e4498babf64f3/sinuEx4.eps" data-type="image"></span></span>
</figure>

</figure>
We saw in the figure above how the frequency $\omega$ influences the rate of the wave's oscillation. The other variable in the signal, the phase $\phi$, can shift the wave backwards and forwards along the time axis, without affecting the frequency. Below are plots of a cosine wave which all have the same frequency, but with a variety of phase shifts (i.e., different values of $\phi$):
<figure id="phases" data-orient="vertical"><figcaption>Four cosine waves with the same frequency, but different phases. Note how a phase shift of $\frac{\pi}{2}$ shifts the cosine to be a sine wave, and a phase shift of $2\pi$ shifts it all the way over to where it was without any phase shift (because the cosine is periodic for this frequency).</figcaption><figure id="cosP1-plot"><figcaption>$\cos\left(\frac{\pi}{6}n-0\right)$.</figcaption>
<span data-type="media" id="cosP1" data-alt="Image">
<img src="/resources/cec4bcd446d82d5184f1ff6df11621c55b0aac8e/cosP1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/49181d5783a4d36c09bbb3b3b703480144906401/cosP1.eps" data-type="image"></span></span>
</figure>
<figure id="cosP2"><figcaption>$\cos\left(\frac{\pi}{6}n-\frac{\pi}{4}\right)$.</figcaption>
<span data-type="media" id="cosP2-plot" data-alt="Image">
<img src="/resources/6722cc0c58160409d3119a45b1505e13f5521c8f/cosP2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/62cb8850595f005c3013ae7088fc4f8c82c28e2a/cosP2.eps" data-type="image"></span></span>
</figure>
<figure id="cosP3"><figcaption>$\cos\left(\frac{\pi}{6}n-\frac{\pi}{2}\right) = \sin\left(\frac{\pi}{6}n\right)$.</figcaption>
<span data-type="media" id="cosP3-plot" data-alt="Image">
<img src="/resources/7b07564b7865d94a79725cb0e391068d9647f5cf/cosP3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/27e79b466fffe39bbdda9e2e2878fe77653e19b7/cosP3.eps" data-type="image"></span></span>
</figure>
<figure id="cosP1again"><figcaption>$\cos\left(\frac{\pi}{6}n-2\pi\right) = \cos\left(\frac{\pi}{6}n\right)$.</figcaption>
<span data-type="media" id="cosP1-plotagain" data-alt="Image">
<img src="/resources/cec4bcd446d82d5184f1ff6df11621c55b0aac8e/cosP1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/49181d5783a4d36c09bbb3b3b703480144906401/cosP1.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-455"><span data-type="title">Complex Valued Sinusoids</span>So we have reviewed the real sine waves $\sin$ and $\cos$, and perhaps seeing them in proximity brought to mind a very special relationship called <span data-type="term">Euler's Formula</span>: $e^{j\theta}=\cos(\theta)+j\sin(\theta)$ (you may remember this from math class with an $i$ instead, but recall engineers use that letter for current, and we call the imaginary number $j$). That formula works for any particular value of $\theta$, so of course it applies when we consider $\omega n+\phi$, as above, which gives us a complex valued sinusoid: $e^{j(\omega n +\phi)}=\cos(\omega n +\phi)+j\sin(\omega n +\phi)$. Let's look at some plots of complex sinusoids. Unlike two-dimensional real sinusoids (which have an one-dimensional independent time variable $n$ and a take a one-dimensional value at each time value), complex sinusoids are three dimensional: they have the time dimension, a real dimension, and an imaginary dimension. So they can be visualized as a three dimensional helix in space:
<figure id="FigureA57"><figcaption>A three dimensional visualization of a complex sinusoid. Note that this image is continuous-valued, whereas a discrete valued version would actually appear as points along the line.</figcaption><span data-type="media" id="FigureA57plot" data-alt="Image">
<img src="/resources/d10ad10fbc31f879369f1a36199416a105fd75ff/FigureA57.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9efbff048adfb8ba8b7bdf0d4d9deff3628a492c/FigureA57.eps" data-type="image"></span>
</span></figure>
If you were to look at this helix from directly above, you would see only the real portion of the helix, and it would appear to be a cosine wave. If you looked at it from the side, you would see the imaginary aspect of it, as a sine wave. The frequency variable $\omega$ controls how quickly the helix rotates across time $n$, and also the direction: positive values cause it to rotate in the counterclockwise manner shown, and negative values would result in it rotating clockwise.</p><p id="eip-462">While it is illuminating to visualize complex simusiods in three dimensions, in practice it is actually most common to view them in two, separately plotting either the real and the imaginary parts with respect to time, or the magnitude and phase across time:
<figure id="csmpplots" data-orient="vertical"><figcaption>A complex sinusoid plotted according to its magnitude and phase. Note that the magnitude of a single complex sinusoid is trivial, as $|e^{j(\omega n +\phi)}|=1$.</figcaption><figure id="cexpMplot"><figcaption>The magnitude of a complex sinusoid.</figcaption>
<span data-type="media" id="cexpM" data-alt="Image">
<img src="/resources/cad3a3e8397ce88d431cd291bfc2d685e312fc7f/cexpM.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ccd63623fdb96e26f180eff4648bc2f574c9b320/cexpM.eps" data-type="image"></span></span>
</figure>
<figure id="cexpP"><figcaption>The phase, or angle, of a complex sinusoid.</figcaption>
<span data-type="media" id="cexpPplot" data-alt="Image">
<img src="/resources/4ec0f438f4648383cb8757a40e162f0ae9674a3d/cexpP.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7faba807a7bfb733c2e5263c7dbc216a0c512809/cexpP.eps" data-type="image"></span></span>
</figure>
</figure>


<figure id="csriplots" data-orient="vertical"><figcaption>A complex sinusoid plotted according to its real and imaginary parts. These are a cosine, and sine, respectively, which follows from Euler's Formula.</figcaption>
<figure id="cexpR"><figcaption>The real part of a complex sinusoid.</figcaption>
<span data-type="media" id="cexpRplot" data-alt="Image">
<img src="/resources/40888ccb2d03cfc5e181bfa6687c0b3a4d467a69/cexpR.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/3c4297747b817913e3ba376c4ee1fa39e12fcaa9/cexpR.eps" data-type="image"></span></span>
</figure>
<figure id="cexpI"><figcaption>The imaginary part of a complex sinusoid.</figcaption>
<span data-type="media" id="cexpI-plot" data-alt="Image">
<img src="/resources/94b1e8f558a7432839c0070376c8d1d1d7d9ef89/cexpI.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ee5f575d4ce19aa5e2839c46b757ff3854d226b2/cexpI.eps" data-type="image"></span></span>
</figure>
</figure>
We'll wrap up our introduction of sinusoids by briefly considering the concept of negative-valued frequencies. It is easiest to see the difference a negative frequency makes, compared to a positive frequency of the same magnitude, by expressing it all mathematically:
$e^{j (-\omega) n} ~=~ e^{-j\omega n} ~=~ \cos(-\omega n) + j \sin(-\omega n) ~=~ \cos(\omega n) - j \sin(\omega n)$
So negating the frequency of a complex sinusoid has no effect on the real part of the signal (the cosine), but it flips the sign of the imaginary part (the sine). This operation (preserving the real part, but changing the sign of the imaginary part) is also known as taking the complex conjugate of the signal. So negating the frequency of a complex sinusoid is the same thing as taking the complex conjugate of it:$e^{j (-\omega) n} ~=~ e^{-j \omega n}~=~ \left( e^{j \omega n} \right)^*$.</p><p id="eip-208"><span data-type="title">Why use imaginary numbers?</span>Now perhaps you are wondering the point of using imaginary numbers. After all, aren't all real world signals, well, real-valued? They are indeed, but we can consider them as the real-part of a complex-valued signal. And why go to that trouble? There are many good reasons, but here is one to start with: exponential functions are much easier to work with than trigonometric functions. You can easily simplify $e^{a} e^{b}$ into a single term, but you very likely would be turning to a table to simplify $\sin(a)\cos(b)$, wouldn't you?</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A6671e8da-3bce-4a81-99ac-ebb85ce56fd9%401.html" data-type="page"><h1>The Peculiarity of Discrete-Time Sinusoids</h1><div data-type="document-title">The Peculiarity of Discrete-Time Sinusoids</div>
  <p id="delete_me"><!-- Insert module text here -->
Compared to their continuous-time counterparts (those that take a continuous-valued independent time variable $t$), discrete-time sinusoidal signals have two unique characteristics. It is possible for them to <span data-type="term">alias</span>, and they are not always <span data-type="term">periodic</span>.</p><p id="eip-728"><span data-type="title">Aliasing of Discrete-time Sinusoids</span>One might think that if two different discrete-time sinuoids have different frequencies, that they would be different signals. Such is the case with continuous-time sinusoids, but not always for the discrete-time version. Consider two discrete-time sinuoids $x_1[n]$ and $x_2[n]$ with different frequencies, $\omega$ and $\omega+2\pi$:
$x_1[n]=e^{j(\omega n+\phi)}$
$x_2[n]=e^{j((\omega+2\pi)n +\phi}$
We can then simplify the expression of $x_2[n]$, using the fact that $e^{j2\pi}=1$ to arrive at this surprising conclusion:
$x_2[n]=e^{j((\omega+2\pi)n +\phi}=e^{j(\omega n+2\pi n +\phi)}=e^{j(\omega n +\phi)}e^{j2\pi n}=e^{j(\omega n +\phi)}(1)^n=e^{j(\omega n +\phi)}=x_1[n]$
So $x_1[n]$ and $x_2[n]$ had different frequencies, yet they are identical! You can see this plotted out with $\omega=\frac{\pi}{6}$ below:
<figure id="cosP1s" data-orient="vertical"><figcaption>Here $x_1[n]$ and $x_2[n]$ have different frequencies, yet are identical.</figcaption><figure id="cosP1-plot"><figcaption>$x_1[n]=\cos\left(\frac{\pi}{6}n\right)$.</figcaption>
<span data-type="media" id="cosP1" data-alt="Image">
<img src="/resources/cec4bcd446d82d5184f1ff6df11621c55b0aac8e/cosP1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/49181d5783a4d36c09bbb3b3b703480144906401/cosP1.eps" data-type="image"></span></span>
</figure>
<figure id="cosP12"><figcaption>$x_2[n]=\cos\left(\frac{13\pi}{6}n\right) = \cos\left((\frac{\pi}{6}+2\pi)n\right)$.</figcaption>
<span data-type="media" id="cosP12-plot" data-alt="Image">
<img src="/resources/cec4bcd446d82d5184f1ff6df11621c55b0aac8e/cosP1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/49181d5783a4d36c09bbb3b3b703480144906401/cosP1.eps" data-type="image"></span></span>
</figure>
</figure>
This phenomenon is called <span data-type="term">aliasing</span>. It happens when frequencies are offset by any integer multiple of $2\pi$ (you can use $\omega+2\pi m$ in the example above and see for yourself). </p><p id="eip-668">So only frequencies along a continuous interval of length $2\pi$ on the real number are distinct from each other. For this reason, when we deal with discrete-time frequencies we consider only those along the interval $0\leq\omega\lt2\pi$ or $-\pi\lt\omega\leq\pi$, as any other frequency aliases back to an identical signal with a frequency in that range. Within these ranges, frequencies close to $0$ (or $2\pi, depending on the range used) are low frequencies--their sinusoids do not oscillate very quickly-- and frequencies close to $\pi$ (or $-\pi, depending on the range) are high frequencies:
<figure id="coslohi" data-orient="vertical"><figcaption>(a) Low frequencies are those $\omega$ close to 0 or $2\pi$ rad. (b) High frequencies are those $\omega$ close to $\pi$ or $-\pi$ rad.</figcaption><figure id="cosLo-plot"><figcaption>$\cos\left(\frac{\pi}{10}n\right)$.</figcaption>
<span data-type="media" id="cosLo" data-alt="Image">
<img src="/resources/9355572cdec3047ce8f3bb7b52140717741c9da2/cosLo.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/08fdb17b1d02c64069d824243a8bad169fb79a1b/cosLo.eps" data-type="image"></span></span>
</figure>
<figure id="cosHi"><figcaption>$\cos\left(\frac{9 \pi}{10}n\right)$.</figcaption>
<span data-type="media" id="cosHi-plot" data-alt="Image">
<img src="/resources/4d107c5df68a59657569d3fc1f86a745a5dcd4a1/cosHi.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2cd5425b660f7f1a57cc46fbc56f1b5569866310/cosHi.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-843"><span data-type="title">Periodicity of Discrete-time Sinusoids</span>Recall that a signal $x[n]$ is defined to be periodic if there exists some integer $N$ for which $x[n+N]=x[n]~,~\forall n$. Suppose we have a complex sinusoid with a frequency $\omega=2\pi\frac{k}{N}$, where $k$ and $N$ are integers. This just means that $\omega$ is a fraction of $2\pi$. It turns out that this signal is periodic, with period $N$:
$\begin{align*}
x[n]&amp;=e^{j(2\pi\frac{k}{N} n+ \phi)}\\
x[n+N]&amp;=e^{j(2\pi\frac{k}{N} (n+N) + \phi)} \\
&amp;=e^{j(2\pi\frac{k}{N} n + 2\pi\frac{k}{N} N + \phi)}\\
&amp;=e^{j(2\pi\frac{k}{N} n + \phi)}  e^{j(2\pi\frac{k}{N} N)}\\
&amp;=e^{j(2\pi\frac{k}{N} n + \phi)}e^{j(2\pi\frac{k}{N}N)}\\
&amp;=e^{j(2\pi\frac{k}{N} n + \phi)}(e^{j(2\pi k)})\\
&amp;=x[n]
\end{align*}$
Here is a plot of a sinusoid with frequency $2\pi\frac{3}{16}$. You will note that it has a period of $N=16$:
<figure id="cosPer"><figcaption>$x_1[n] = \cos( \frac{2\pi 3}{16} n)$ is periodic, with $N=16$.</figcaption><span data-type="media" id="cosPer-plot" data-alt="Image">
<img src="/resources/93f45a4cd7ef02fc81d7e12191bdc60c44e5cfc6/cosPer.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9ac607d2d93e0513e5a7922c2acd2ff0769269df/cosPer.eps" data-type="image"></span>
</span></figure>
Now, these fractions of $2\pi$ are special values of $\omega$ we will call <span data-type="term">harmonic frequencies</span>, for sinusoids with such frequencies are periodic.</p><p id="eip-812">In contrast, consider sinusoids whose frequencies are <em data-effect="italics">not</em> fractions of $\pi$:
$\begin{align*}
x[n]&amp;=e^{j(\omega n+ \phi)}\\
x[n+N]&amp;=e^{j(\omega (n+N) + \phi)} \\
&amp;=e^{j(\omega n + \omega N + \phi)}\\
&amp;=e^{j(\omega n + \phi)}  e^{j(\omega N)}\\
&amp;=e^{j(\omega n + \phi)}e^{j(\omega N)}\\
&amp;\neq x[n],\textrm{unless } \omega N=2\pi k \rightarrow \omega=2\pi\frac{k}{N}
\end{align*}$
So we see that discrete-time sinusoids are periodic if, and only if, their frequencies are fractions of $2\pi$. Consider the example of a non-periodic sinusoid below. It definitely oscillates, and at first it appears to be periodic, but look carefully and you will see that it is not, unlike the one from the figure above.
<figure id="cosNotPer"><figcaption>$x_2[n] = \cos(1.16 \, n)$. The frequency of 1.16 is not a fraction of $2\pi$, so this sinusoidal signal is not periodic.</figcaption><span data-type="media" id="cosNotPer-plot" data-alt="Image">
<img src="/resources/901c0abb0e95db1694e9ab704cf9ae55248716ca/cosNotPer.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5290779adbd7f1bf7c8d68c3886344915a7139ba/cosNotPer.eps" data-type="image"></span>
</span></figure>
Take note of sinusoids whose frequencies are of the form $\omega=2\pi\frac{k}{N}$, for they will play a starring role in the Fourier analysis of periodic and finite-length discrete-time signals.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ad09711e8-464f-474b-a1f5-8cae67ce34f1%401.html" data-type="page"><h1>Complex Exponentials</h1><div data-type="document-title">Complex Exponentials</div>
  <p id="delete_me"><span data-type="title">From Complex Sinusoids to Complex Exponentials</span><!-- Insert module text here -->
Recall the form of a discrete-time complex sinusoid: $x[n]=e^{j(\omega n + \phi)$. As we have already seen, that signal itself is complex-valued, i.e., it has both a real and an imaginary part. But look closely at just the exponent, and you will see that the exponent itself is purely imaginary.</p><p id="eip-868">Suppose we let the exponent be complex-valued, say of the form $a+jb$. Then we have $e^{(a+jb)n}=e^{an}e^{jbn}=(e^a)^n e^{jbn}$. So the result is a complex sinusoid multipled by a real exponential signal (whose base is $e^a$).</p><p id="eip-886"><span data-type="title">Complex Exponentials, Defined</span>We do not typically represent complex exponentials in the way derived above, but rather express them in the form $x[n]=z^n$, where $z$ is a complex number. Being a complex number, it lies on the complex plane with a magnitude of $|z|$ and an angle of $\angle z$ we define as $\omega$. So then, if we would like to express $x[n]=z^n$ as a combination of a real exponential and a complex sinusoid, as above, we have:
$x[n]=z^n=|z|^n e^{j\omega n}$. Below are some plots of complex exponentials for different values of $z$.
<figure id="cexp10" data-orient="vertical"><figcaption>The real and imaginary parts of a complex exponential $x^n$ for which $|z|\lt 1$.</figcaption><figure id="cexp10R-plot">
<span data-type="media" id="cexp10R" data-alt="Image">
<img src="/resources/aac295202554ca78984e713883e65ce418ad2ba4/cexp10R.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/c87dbc3d0bd370c1b7ff39611f81a4e65e116624/cexp10R.eps" data-type="image"></span></span>
</figure>
<figure id="cexp10I">
<span data-type="media" id="cexp10I-plot" data-alt="Image">
<img src="/resources/e96187d272465d6c66752aaa900f3c4b3474b9ae/cexp10I.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ca6a07167b9c547169981709658fd1111f1c955d/cexp10I.eps" data-type="image"></span></span>
</figure>
</figure>
<figure id="cexp11" data-orient="vertical"><figcaption>The real and imaginary parts of a complex exponential $x^n$ for which $|z|\gt 1$.</figcaption><figure id="cexp11R-plot">
<span data-type="media" id="cexp11R" data-alt="Image">
<img src="/resources/312095eab4a14c4238557657b897920f07b5a7e3/cexp11R.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9119b5137a6d8e99ab96a5df66ff97733f7f012b/cexp11R.eps" data-type="image"></span></span>
</figure>
<figure id="cexp11I">
<span data-type="media" id="cexp11I-plot" data-alt="Image">
<img src="/resources/650c21b3986004955bd2d24c40d04cf99499fa72/cexp11I.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/db509dd5a5598891a8ee32c18cffd119c190f04e/cexp11I.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-503">So when the magnitude $|z|$ is greater than 1, we have a signal that oscillates and exponentially grows with time, and if the magnitude is less than 1, it decays over time. And, you guessed it, if the magnitude is exactly equal to 1, it does not grow or decay, but only oscillates. In fact, if the magnitude is 1, the complex exponential is, by definition, simply a complex sinusoid: $|1|^n e^{j\omega n}=e^{j\omega n}$. Therefore you can see that complex sinusoids are a subset of the more general complex exponential signals.</p></div></li></ul></li><li><div data-type="page"><h1>Signals Are Vectors</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ab3754f43-6ba2-4546-93f1-229a4fd324fd%401.html" data-type="page"><h1>Signals are Vectors</h1><div data-type="document-title">Signals are Vectors</div>
  <p id="delete_me"><!-- Insert module text here -->
One mathematical way of understanding signals is to see them as functions. A signal $x[n]$ carries some kind of information, having a value $x[n]$ at every given point of its independent time variable $n$. Another, and complementary, way of understanding signals is to consider them as vectors within vector spaces. By doing this we will be able to apply various tools of linear algebra to help us better understand signals and the systems that modify them.</p><p id="eip-687"><span data-type="title">Vector Spaces</span>A <span data-type="term">vector space</span> $V$ is a collection of vectors such that if $x, y \in V$ and $\alpha$ is a scalar then 
$ \alpha x \in V \text{and} x+y \in V $
In words, this means that if two vectors are elements of a vector space, any combination or scaled version of them is also in the space. When we consider the scaling factors, we mean $\alpha$ that are either real or complex numbers.</p><p id="eip-954">There are many different kinds of vector spaces, but the two in which we are especially interested are $R^N$ and $C^N$. $R^N$ is the set of all vectors of dimension $N$, in which every entry of the vector is a real number, and $C^N$ is exactly the same, except the entries are complex valued.</p><p id="eip-562"><span data-type="title">Starting Small &#8211; A Two Dimensional Vector Space</span>You are already familiar with a prominent example of a vector space, the two-dimensional real coordinate space $R^2$. Every ordered combination of two real numbers is a vector in this space, and can be visualized as a point or arrow in the two-dimensional Cartesian plane. Suppose $x$ and $y$ are each vectors in $R^2$. 
$x=\begin{bmatrix}
x[0] \\ x[1]
\end{bmatrix}
$y=\begin{bmatrix}
y[0] \\ y[1]
\end{bmatrix}
where $x[0], x[1], y[0], y[1]\in R$. The indices we use to refer to elements of the vector start their numbering at $0$. This is the common convention in signal processing and many mathematical languages, like $C$, but note that vector indices in MATLAB start with 1.
Scaled versions of these vectors are still within the space:
$\alpha x=\alpha \begin{bmatrix}
x[0] \\ x[1]
\end{bmatrix}=\begin{bmatrix}
\alpha x[0] \\ \alpha x[1]
\end{bmatrix}\in R^2$
So is the sum of two vectors:
$x+y=\begin{bmatrix}
x[0] \\ x[1]
\end{bmatrix}+\begin{bmatrix}
y[0] \\ y[1]
\end{bmatrix}=\begin{bmatrix}
x[0]+y[0] \\ x[1]+y[1]
\end{bmatrix}\in R^2$</p><p id="eip-881"><span data-type="title">The Vector Space $R^N$</span>Let's now move from two dimensional vectors to those with $N$ dimensions, each taking a real valued number:
$x=\begin{bmatrix}x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}$, where $x[n]\in\Real$
So we have a mathematical entity $x$, with $N$ ordered real values associated with it. Stated this way, we see that $x$ is a signal, simply expressed in a vector form $x$ as opposed to the signal notation form $x[n]$, but both forms refer to the exact same thing.</p><p id="eip-65">Just as with in 2-dimensions, we can perform the same operations on the $N$-dimensional signal/vector $x$:
<figure id="sigfunscales" data-orient="vertical"><figcaption>When a vector $x$ in $R^N$ is scaled by $\alpha\in R$, the result is still in $R^N$.</figcaption><figure id="sigFun3-plot"><figcaption>An example of an $N$-dimensional signal/vector $x$.</figcaption>
<span data-type="media" id="sigFun3" data-alt="Image">
<img src="/resources/a6fe02b6f03e34381ff740f25b0b90bf323c4ba8/sigFun3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b502f6a4251de0d8449f574e4479ff304a743bb0/sigFun3.eps" data-type="image"></span></span>
</figure>
<figure id="sigFunScaled1"><figcaption>The signal $x$, scaled by $\alpha=3$.</figcaption>
<span data-type="media" id="sigFunScaled1-plot" data-alt="Image">
<img src="/resources/7fa1c4cc2cec6130fcd7747129584881a9752cb9/sigFunScaled1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5b35a6978829accff6a62ba86fdd22e82cae5347/sigFunScaled1.eps" data-type="image"></span></span>
</figure>
</figure>
When each of the real-valued elements of $x$ is scaled by a real value $\alpha$, the results are still real-valued, of course, so the resulting scaled vector is still within $R^N$. As $R^N$ is a vector space, that should come as no surprise, and neither should the fact that the sum of two vectors in $R^N$ is itself another vector in the vector space $R^N$:
<figure id="rnsums" data-orient="vertical"><figcaption>The sum of two vectors in $R^N$ is another vector in $R^N$.</figcaption><figure id="sum1a-plot">
<span data-type="media" id="sum1a" data-alt="Image">
<img src="/resources/a6b5a6361e36bad2bfcf0c50ed694a9ea6d406ea/sum1a.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/e1fbcd1d70e9e20fef1b36b7ca3180cf29ea9ceb/sum1a.eps" data-type="image"></span></span>
</figure>
<figure id="sum2a">
<span data-type="media" id="sum2a-plot" data-alt="Image">
<img src="/resources/f4577b13832e5ba80100f05e9130627f191f9f24/sum2a.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d97855495780d078f6f7270f4010f1d47417e62f/sum2a.eps" data-type="image"></span></span>
</figure>
<figure id="sum3a">
<span data-type="media" id="sum3a-plot" data-alt="Image">
<img src="/resources/21e124954a738ee58617ad84060400fbefa5a7de/sum3a.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7a1514d2209ed74248c4f0d92efb3c5c8a6a7ffd/sum3a.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-297"><span data-type="title">The Vector Space $C^N$</span>Ordered sequences of $N$ complex numbers form the vector space $C^N$, just like their real counterparts do in $R^N$:
$x=\begin{bmatrix}x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}$, where $x[n]\in C$
So each element of a vector in $C^N$ is a complex number, which can be represented either in terms of its real and imaginary parts, or in terms of its magnitude and phase. The vector as a whole can also be represented in those ways, either in rectangular form
Rectangular form
$x
= \begin{bmatrix} \Re{x[0]} + j \Im{x[0]} \\ \Re{x[1]} + j \Im{x[1]} \\ \vdots \\ \Re{x[N-1]} + j \Im{x[N-1]} \end{bmatrix}
= {\rm Re} \left\{ \begin{bmatrix} x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}\right\}
+ j\: {\rm Im} \left\{ \begin{bmatrix} x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}\right\}
$
or in polar form
$
x
= \begin{bmatrix} |x[0]|\,e^{j \angle x[0]} \\ |x[1]|\,e^{j \angle x[1]} \\ \vdots \\ |x[N-1]|\,e^{j \angle x[N-1]} \end{bmatrix}
$.
</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aba6c282e-33b7-4664-9f5d-bb038a8cd21c%401.html" data-type="page"><h1>Linear Combinations of Vectors</h1><div data-type="document-title">Linear Combinations of Vectors</div>
  <p id="delete_me"><!-- Insert module text here -->
In the study of signals and signal processing, there is a particular mathematical operation that will show up quite a few times, that of a <span data-type="term">linear combination</span>. Given a collection of vectors in a vector space, say $M$ vectors $x_0, x_1, \dots x_{M-1} \in C^N$, and $M$ scalars $\alpha_0, \alpha_1, \dots, \alpha_{M-1} \in C$, then the linear combination of these vectors is:
$y=\alpha_0 x_0 + \alpha_1 x_1 + \dots + \alpha_{M-1} x_{M-1} = \sum_{m=0}^{M-1} \alpha_m x_m$. The result is itself also a vector in the vector space.</p><p id="eip-199"><span data-type="title">Linear Combination Example: A Mixing Board</span>A linear combination is a scaled sum of different vectors in a vector space. A real world example of a linear combination is the mixing board used in music recording studios. The board takes in a variety of different inputs and combines them--amplifying some, reducing the level of others--to create a single output of music. Mathematically, we could say $x_0=$ drums, $x_1=$ bass, $x_2=$ guitar, \dots, $x_{22}=$ saxophone, $x_{23}=$ singer, and then the mixing board creates the linear combination
$y = \alpha_0 x_0 + \alpha_1 x_1 + \dots + \alpha_{M-1} x_{M-1} = \sum_{m=0}^{M-1} \alpha_m x_m$.
Changing the different $\alpha_m$'s would result in a different kind of sound that either emphasized or de-emphasizes certain instruments. For example, the producer in the studio that day may be particularly interested in the cowbell.</p><p id="eip-303"><span data-type="title">Linear Combinations as Matrix Multiplication</span>It is possible to express a linear combination without explicitly using sums, but rather as the product of a matrix and a vector. To do this, all of the vectors to be scaled and summed in the linear combination must first be arranged into a matrix:
$\begin{align*}
X &amp;= \begin{bmatrix} x_0 | x_1 | \cdots | x_{M-1} \end{bmatrix}\\
&amp;=\begin{bmatrix}
x_0[0] &amp;x_1[0] &amp;\cdots &amp;x_{M-1}[0] \\
x_0[1] &amp;x_1[1] &amp;\cdots &amp;x_{M-1}[1] \\
\vdots &amp;\vdots &amp;&amp;\vdots \\
x_0[N-1] &amp;x_1[N-1] &amp;\cdots &amp;x_{M-1}[N-1] \end{bmatrix}
\end{align*}$
Then the scaling factors for each vector are stacked into a single vector:
$a = \begin{bmatrix}\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{M-1} \end{bmatrix}$
Once those are in place, the linear combination is ultimately expressed as a simple matrix-vector multiplication:
$y=\alpha_0 x_0 + \alpha_1 x_1 + \dots + \alpha_{M-1} x_{M-1} = \sum_{m=0}^{M-1} \alpha_m x_m
= \begin{bmatrix}x_0 | x_1 | \cdots | x_{M-1} \end{bmatrix}\begin{bmatrix}\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{M-1} \end{bmatrix}= X a$</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A4e807dcd-b5e5-41f8-b1bf-ae07a9d84810%401.html" data-type="page"><h1>The Strength of a Vector</h1><div data-type="document-title">The Strength of a Vector</div>
  <p id="delete_me"><!-- Insert module text here -->
  In the study of signal processing, the "strength" of signals (which, remember, can be expressed as vectors) is often of interest. It is not immediately clear what standard could be used for judging the strength of a vector. A similar problem comes up when determining the strength of a person: does it have to do with the amount of weight that can be lifted, and/or the number of times it can be lifted, and if so what kind of weight or lift? So it is with vectors; consider the two below:
<figure id="sigFun10s" data-orient="vertical"><figcaption>Which signal is stronger?</figcaption><figure id="sigFun10-plot">
<span data-type="media" id="sigFun10" data-alt="Image">
<img src="/resources/a6fe02b6f03e34381ff740f25b0b90bf323c4ba8/sigFun10.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/068c39cecd17423b435f75a0b24deea91e426c19/sigFun10.eps" data-type="image"></span></span>
</figure>
<figure id="sigFun11">
<span data-type="media" id="sigFun11-plot" data-alt="Image">
<img src="/resources/397cf43888098df98525bff99371b545ffe7faf8/sigFun11.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8e85ad16ab45489ea358b9d3b87b97042a5ac7fb/sigFun11.eps" data-type="image"></span></span>
</figure>
</figure>
Which is stronger than the other? Perhaps you might have intuition just by looking at them, but undoubedtly it will be better if we can introduce some kind of rigorous definition of strength.</p><p id="eip-83"><span data-type="title">Norms: The Strength of Vectors</span>There indeed are such ways to measure the strength of vectors, and they are called <span data-type="term">norms</span>. Just as there are different ways to measure human strength, there are different ways to measure the strength of the vector. The first one we will consider is the 2-norm.</p><p id="eip-877">The <span data-type="term">Euclidean length</span>, or <span data-type="term">$2$-norm</span>, of a vector $x\in C^N$ is defined as:
$\|x\|_2 = \sqrt{ \sum_{n=0}^{N-1} |x[n]|^2 }$
This norm, like all norms, is a function that takes a vector as its argument and then produces a real number that is always greater than or equal to 0. The 2-norm is a very common norm, so often it will be seen without even having a subscript, i.e. as $\|x\|$.</p><p id="eip-730">Let's calculate a 2-norm. Suppose:
$x=\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$
Then $\|x\|_2$, the 2-norm of $x$, is:
$\| x \|_2 = \sqrt{ \sum_{n=0}^{N-1} |x[n]|^2} = \sqrt{1^2 + 2^2 + 3^2} = \sqrt{14}$</p><p id="eip-533">It happens that the 2-norm is actually just one kind of a more general type of norm, called <span data-type="term">$p$-norms</span>. For any real number $p\geq 1$, the $p$-norm is:
$\|x\|_p = \left( \sum_{n=0}^{N-1} |x[n]|^p \right)^{1/p}$
So the 2-norm is a $p$-norm with $p=2$. Another norm of interest to us is when $p=1$, the <span data-type="term">1-norm</span>:
$\|x\|_1 = \sum_{n=0}^{N-1} |x[n]|$</p><p id="eip-646">A final norm that we will consider is called the <span data-type="term">$\infty$-norm</span>. It is essentially what happens as the $p$ in the $p$-norm increases to $\infty$, but has this easy to calculate expression:
$\|x\|_\infty=\max_n |x[n]|$
So if you have a vector $x$ and take the absolute value of all the entries, the $\infty$ norm of $x$ is simply the largest of those values.</p><p id="eip-588"><span data-type="title">The Meaning of Different Norms</span>There is a reason for having different norms; they each are helpful in their own way for measuring particular things about a signal. Some of these measurements have clear physical quantities associated with them. For example, consider the electrical signal travelling through a loudspeaker, and suppose $x$ corresponds to the voltage of this signal. The $2$-norm of this signal is significant, because it corresponds to the electrical <span data-type="term">energy</span> going through the coil (this is a bit of a simplification). The $\infty$-norm of the signal is important because it represents the maximum voltage value going through the circuit. We might want to pay attention to each of those values, though for different reasons. If the $2$-norm is too large, then the excessive amount of energy might melt the circuit. If the $\infty$-norm is too large, the extreme voltage at a given time might cause the speaker cone to move too far, breaking it. </p><p id="eip-226">Or consider another example of the utility of different norms. Suppose you are designing a computer-driven car. You clearly will want to make sure it does not deviate from the programmed path.  Call the deviation at a given time the error signal $d$. Would you seek to minimize $\|d\|_2$ or $\|d\|_infty$? If you minimize $\|d\|_2$, then the car will, overall, keep to the path very closely, but quick and large deviations would be permitted: if over the course of a long drive the car followed the path perfectly, except for a fraction of a second it accidentally swerved into the other lane, the $\|d\|_2$ would be small, but that could potentially still be a disaster! Instead, you would want to minimize the $\|d\|_infty$ value, so the car never moves more than a maximum distance from the path.</p><p id="eip-959"><span data-type="title">Normalizing a Vector</span>Suppose you run a recording studio and you are putting different recorded tracks onto a single album. You run in to the following problem: the artist was standing closer to the microphone on some tracks, and farther away on others. The loudness varies wildly across the different tracks, so it would be frustrating for customers to listen to the album, having to change the volume with every track. What would you do?</p><p id="eip-336">If you have studied signal processing, then you will know that you can consider each track as a signal, and that each track will have a different norm.  In order to make the loudness the same, you would like to modify each track so that it has the same "strength", or norm (technically, unless all the tracks are the same length, you would like them to have the same norm per time length). Thankfully, it is very straightforward to modify a signal so as to change its norm. There are two steps.</p><p id="eip-45">First, you will modify the vector so that it is a unit norm, i.e. a norm of 1.  This is called <span data-type="term">normalizing</span> the vector. To do that, you simply scale the vector by its norm. Suppose your vector is $x$; the normalized version of $x$ is then $\frac{x}{\|x\|_2}$. For example, earlier we saw that the vector:
$x=\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$
had a norm of $\sqrt{14}$. The normalized version of $x$ then is $x$ divided by that value:
$x=\begin{bmatrix} \frac{1}{\sqrt{14}} \\ \frac{2}{\sqrt{14}} \\ \frac{3}{\sqrt{14}} \end{bmatrix}$
That new vector has a 2-norm of 1, as desired. If you wanted the norm of the vector to be some other value, say the number $M$, you would multiply the normalized vector by $M$. </p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aae07056c-3869-4fa5-bba4-c68910470847%401.html" data-type="page"><h1>Inner Products and Orthogonality</h1><div data-type="document-title">Inner Products and Orthogonality</div>
  <p id="delete_me"><!-- Insert module text here -->
In discrete-time signal processing, understanding signals as vectors within a vector space allows us to use tools of analysis and linear algebra to examine signal properties. One of the properties we may want to consider is the similarity of (or difference between) two vectors. A mathematical tool that provides insight into this is the <span data-type="term">inner product</span>.</p><p id="eip-433"><span data-type="title">Transposing Vectors</span>One of the ways to express the inner product of two vectors is through matrix multiplication, so first we must introduce the concept of transposing vectors. In order to multiply two matrices (a vector is simply a matrix in which one of the dimensions is 1), the column dimension of the first must match the row dimension of the second. To make those match for two vectors of the same length, we must <span data-type="term">transpose</span> one of them. To take the transpose of a matrix, simply turn the rows into columns: the first row will become the first column in the transposed matrix; the second row, the second column, and so on. Here is how that looks for a vector. An $N$-row, single column vector transposed becomes a 1 row, $N$-column vector:
$\begin{bmatrix}x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}^T = \begin{bmatrix}x[0] &amp; x[1] &amp; \cdots &amp; x[N-1] \end{bmatrix}$
</p><p id="eip-672">Now, when it comes to complex valued vectors, we can take a transpose in the same way, but for the purposes of finding an inner product we actually need to take the conjugate, or Hermitian, transpose, which involves taking the transpose and then the complex conjugate:
$\begin{bmatrix}x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}^H =\begin{bmatrix}x[0]^* &amp; x[1]^* &amp; \cdots &amp; x[N-1]^* \end{bmatrix}$
Of course, for real valued vectors, the regular transpose and Hermitian transpose are identical.</p><p id="eip-854"><span data-type="title">The Inner Product</span>The inner product of two complex (or real) valued vectors is defined as:
$\langle x, y \rangle = y^H x = \sum_{n=0}^{N-1} x[n] \, y[n]^*$
So the inner product operation takes two vectors as inputs and produces a single number. It turns out that the number it produces is related to the angle $\theta$ between the two vectors:
$\cos \theta_{x,y} = \frac{\Re{\langle x, y \rangle}}{\|x\|_2 \, \|y\|_2}$
This formula works for complex and real vectors, although taking the real part of the inner product is redundant for real valued ones.</p><p id="eip-630">For two (or three) dimensional vectors, this angle is exactly what you would expect it to be. Let 
$x=\begin{bmatrix}1 \\ 2 \end{bmatrix}$, ~ $y=\begin{bmatrix}3 \\ 2 \end{bmatrix}$
We have
$\|x\|_2^2 = 1^2 + 2^2 = 5$, $\|y\|_2^2 = 3^2 + 2^2 = 13$ ,and $\langle x, y \rangle=(1)(3)+(2)(2)=7$.
The angle between them is $\arccos\left(\frac{7}{\sqrt{(5)(13)}}\right) \approx 0.519~{\rm rad} \approx 29.7^\circ$. If you plot the vectors out in the Cartesian plane, you will indeed see an angle between them of about 30 degrees.</p><p id="eip-751">For higher dimensional signals the result of the inner product--how it relates to the angle between signals--may not seem as intuitive, but the information it provides is still just as useful, and of course it is computed in the same was as with shorter vectors. Consider the signals below:<figure id="sigFun10s" data-orient="vertical"><figure id="sigFun10-plot">
<span data-type="media" id="sigFun10" data-alt="Image">
<img src="/resources/a6fe02b6f03e34381ff740f25b0b90bf323c4ba8/sigFun10.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ab4550042fbd6bdad4e0ce63fb8f9469d58dcd33/sigFun10.eps" data-type="image"></span></span>
</figure>
<figure id="sigFun11">
<span data-type="media" id="sigFun11-plot" data-alt="Image">
<img src="/resources/397cf43888098df98525bff99371b545ffe7faf8/sigFun11.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/832f49fd54ab5ef0b0dac8a14bff02dcabb7a327/sigFun11.eps" data-type="image"></span></span>
</figure>
</figure>
The inner product of these two signals, computed according to the formula above, is $\langle x, y \rangle ~=~ y^T x ~=~  5.995$, which corresponds to an angle of $\theta_{x,y} ~=~ 64.9^\circ$.</p><p id="eip-509"><span data-type="title">Inner product: Limiting Cases</span>Recall that the inner product is defined as a sum ($\sum_{n=0}^{N-1} x[n] \, y[n]^*$), which can also be expressed as a vector product ($y^H x$). Let's look at a couple of interesting values that sum could take.</p><p id="eip-270">The dot product of two signals could be rather large. If the signals are identical, it is simply the norm of the signal, squared: 
$\langle x, x \rangle = \sum_{n=0}^{N-1} x[n] \, x[n]^* = \sum_{n=0}^{N-1} |x[n]|^2 = \|x\|_2^2$.</p><p id="eip-363">On the other hand, it is also possible for the dot product sum to be 0. Consider the two signals below:
<figure id="orthog1as" data-orient="vertical"><figure id="orthog1a-plot">
<span data-type="media" id="orthog1a" data-alt="Image">
<img src="/resources/4fd7069af39075a137b126d27fe0654b39127b18/orthog1a.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/cff62b335922055521dd6e0c7cc53e1d6ff47b7c/orthog1a.eps" data-type="image"></span></span>
</figure>
<figure id="orthog1b">
<span data-type="media" id="orthog1b-plot" data-alt="Image">
<img src="/resources/849e3d4686f9c1c5d335349e60ee17d8f82f23da/orthog1b.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9054744304c7a735d74435b092ff792ef7ba1a65/orthog1b.eps" data-type="image"></span></span>
</figure>
</figure>
The inner product of those two signals is obviously zero because each pointwise product is also zero. But it is possible, of course, for products in the sum to be nonzero and still have the total add up to zero:
<figure id="orthog2as" data-orient="vertical"><figure id="orthog2a-plot">
<span data-type="media" id="NAME" data-alt="Image">
<img src="/resources/4f1121a623ed467a1f3da5dedb3bcfe8b0f10953/orthog2a.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/01f4a97894ee074b596452ac34a77e1e7593dca4/orthog2a.eps" data-type="image"></span></span>
</figure>
<figure id="orthog2b">
<span data-type="media" id="orthog2b-plot" data-alt="Image">
<img src="/resources/1cc6ba9d31e8a52582bfef30b968817a1db6cb1c/orthog2b.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/732c6785aa9d4eb4d5686a76947fb718ae711d67/orthog2b.eps" data-type="image"></span></span>
</figure>
</figure>
Whenever the inner product of two signals is zero, it is defined that those signals are <span data-type="term">orthogonal</span>. </p><p id="eip-51"><span data-type="title">Orthogonality of Harmonic Sinusoids</span>Recall the special class of discrete-time finite length signals called harmonic sinusoids: $s_k[n] = e^{j \frac{2 \pi k}{N} n}, ~~~ n,k,N\in\Integers, ~~ 0\leq n \leq N-1, ~~ 0\leq k \leq N-1$
It is a very interesting property that any two of these sinusoids having different frequencies (i.e., $k\neq l$) are orthogonal:
$\begin{align*}
\langle s_k | s_l \rangle &amp;= \sum_{n=0}^{N-1} d_k[n] d_l^*[n]\\
&amp;= \sum_{n=0}^{N-1} e^{j \frac{2 \pi k}{N} n}  (e^{j \frac{2 \pi l}{N} n})^*\\
&amp;= \sum_{n=0}^{N-1} e^{j \frac{2 \pi k}{N} n} \: e^{-j \frac{2 \pi l}{N} n} \\
&amp;= \sum_{n=0}^{N-1} e^{j \frac{2 \pi}{N} (k-l) n} ~~~ \textrm{let }r=k-l \in I, r\neq 0 \\
&amp;= \sum_{n=0}^{N-1} e^{j \frac{2 \pi}{N} r n}\\
&amp;=\sum_{n=0}^{N-1} a^n ~~~ \textrm{with }~a=e^{j \frac{2 \pi}{N} r},\textrm{and recall }\sum_{n=0}^{N-1} a^n = \frac{1-a^N}{1-a} \\
  &amp;= \frac{ 1- e^{j \frac{2 \pi r N }{N}} }{1-e^{j \frac{2 \pi r}{N}} } ~=~ 0 ~~\checkmark
\end{align*}$
If two of these sinusoids have the same frequency, then their inner product is simply $N$:
$\begin{align*}
\|s_k \|_2^2 &amp;= \sum_{n=0}^{N-1} |d_k[n]|^2 \\
&amp;= \sum_{n=0}^{N-1} |e^{j \frac{2 \pi k}{N} n}|^2\\
&amp;=\sum_{n=0}^{N-1} 1 ~=~ N\checkmark
\end{align*}$</p><p id="eip-504">So the dot product of two harmonic sinusoids is zero if their frequencies are different, and $N$ if they are the same. In order to make latter number is $1$, instead of $N$, they are sometimes normalized:
$\tilde{d}_k[n] = \frac{1}{\sqrt{N}}\, e^{j \frac{2 \pi k}{N} n},
~~~ n,k,N\in Z, ~~ 0\leq n \leq N-1, ~~ 0\leq k \leq N-1$
</p><p id="eip-76"><span data-type="title">Matrix Multiplication and Inner Products</span>Let's take look at the formula for the matrix multiplication $y=Xa$. For notation, we will represent the value on the $n$th row and $m$th column of $X$ as $x_m[n]$. The matrix multiplication looks like this:
\begin{bmatrix}\vdots  &amp; \vdots &amp;&amp; \vdots  \\
x_0[n] &amp; x_1[n] &amp; \cdots &amp; x_{M-1}[n] \\
\vdots  &amp; \vdots &amp;&amp; \vdots \end{mbatrix}
And the value for the multiplication is:
$y[n] = \sum_{m=0}^{M-1} \alpha_m \, x_m[n]$
Hopefully that formula looks familiar! What it is showing is that the matrix multiplication $y=Xa$ is simply the inner product of $a$ with each row of $X$, when $X$ and $a$ are real. If they are complex valued, then the complex conjugate of one of them would have to be performed before the matrix multiplication for each value of $y$ to be the inner product of the matrix row with $a$.</p><p id="eip-633"><span data-type="title">The Cauchy Schwarz Inequality</span>Above we saw that the inner product of two vectors can be as small as 0, in which case the vectors are orthogonal, or it can be large, such as when the two vectors are identical (and the inner product is simply the norm of  the vector, squared). It turns out that there is a very significant inequality that explains these two cases. It is called the <span data-type="term">Cauchy Schwarz Inequality</span>, which states that for two vectors $x$ and $y$,
$|\langle x,y \rangle |\leq |\x\| \|y\|$
Now the magnitude of the inner product is always greater than or equal to 0 (being 0 if the vectors are orthogonal), so we can expand the inequality thus:
$0\leq |\langle x,y \rangle |\leq |\x\| \|y\|$
If we divide the equation by $|\x\| \|y\|$, then we have
$0\leq \frac{|\langle x,y \rangle |}{|\x\| \|y\|}\leq 1$
This explains why we can define
$\cos \theta_{x,y} = \frac{\Re{\langle x, y \rangle}}{\|x\|_2 \, \|y\|_2}$, 
for the cosine function also has a range of 0 to 1.

</p><p id="eip-609">Now there are many different proofs of the inequality, and it is something of a mathematical pastime to appreciate their various constructions. But for signal processing purposes, we are more interested in the utility of the inequality. What it basically says is that--when the lengths of two vectors are taken into consideration--their inner product ranges in value from 0 to 1. Because of this, we can see that the inner product introduces some kind of comparison between two different vectors. It is at its smallest when they are, in a sense, very different from each other, or <span data-type="term">orthogonal</span>. It is at its peak when the vectors are simply scalar multiples of each other, or in other words, are very alike.</p><p id="eip-419">It turns out there are many application in which we would like to determine how similar one signal is to another. How does a digital communication system decide whether the signal corresponding to a "0" was transmitted or the signal corresponding to a "1"? How does a radar or sonar system find targets in the signal it receives after transmitting a pulse? How does many computer vision systems find faces in images? For each of these questions, the similarity/dissimilarity bounds established by the Cauchy Schwarz inequality help us to determine the answer.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa5760e83-3a00-45f7-8900-fc4ff16f7fad%401.html" data-type="page"><h1>Norms and Inner Products of Infinite Length Vectors</h1><div data-type="document-title">Norms and Inner Products of Infinite Length Vectors</div>
  <p id="delete_me"><!-- Insert module text here -->
Like as with finite length signals (which can be understood as vectors in $R^N$ or $C^N$), we can take the norms and inner products of infinite length signals, as well. For the most part, the concepts will apply just the same.</p><p id="eip-167"><span data-type="title">The Norms of Infinite Length Vectors</span>As with finite-length vectors, the $p$-norm for infinite-length vectors is a sum:
$\|x\|_p = \left( \sum_{n=-\infty}^{\infty} |x[n]|^p \right)^{1/p}$
Unlike with the $p$-norms of finite-length signals, the summation for infinite length vectors is necessarily fininite. As a consequence, the $p$-norms of infinite length signals may not be bounded. Consider a finite-length signal $x[n]=1$ for $0\leq n\lt N-1$. The $2$ norm for this signal would be $\sqrt{N}$. However, for an infinite length signal of constant value 1, all the $p$ norms will be unbounded!</p><p id="eip-916">For some infinite length vectors, the $p$ norm may only exist for certain values of $p$. Consider the signal:
$x[n] = \begin{cases}
	0 &amp; n\leq0 \\
	\frac{1}{n} &amp; n\geq 1 \\
\end{cases}$
<figure id="oneovern"><span data-type="media" id="oneovern-plot" data-alt="Image">
<img src="/resources/ee15c69da181961e24ad425ae8a7104702603e27/oneovern.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1cc87dae4454c3424ecb717ac09f84c6eb8e3985/oneovern.eps" data-type="image"></span>
</span></figure>
The 2-norm of this signal exists:
$\|x\|_2^2 = \sum_{n=-\infty}^{\infty} |x[n]|^2 = \sum_{n=1}^{\infty} \left|\frac{1}{n}\right|^2
= \sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \approx 1.64$
But the 1-norm is unbounded:
$\|x\|_1 = \sum_{n=-\infty}^{\infty} |x[n]| = \sum_{n=1}^{\infty} \frac{1}{n} = \infty$</p><p id="eip-152">For a finite-length vector, the $\infty$ norm is simply the maximum value of the magnitudes of all its elements. For infinite-length vectors, it is instead the <span data-type="term">supremum</span>, or the <span data-type="term">least upper bound</span> of the set of all the elements of the vector. The distinction between that value, and the maximum value, of the set is outside the bounds of this course. For all the cases we will consider, the $\infty$ norm will simply be the largest of the magnitudes of the elements of the vector. For example, for the signal $x[n]$ below, $\|x\|_\infty=1$:
<figure id="oneovern2"><span data-type="media" id="oneovern2-plot" data-alt="Image">
<img src="/resources/ee15c69da181961e24ad425ae8a7104702603e27/oneovern.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1cc87dae4454c3424ecb717ac09f84c6eb8e3985/oneovern.eps" data-type="image"></span>
</span></figure></p><p id="eip-407"><span data-type="title">Inner Product of Infinite Length Signals</span>The inner product of infinite length signals is defined the same way as for finite-length signals, except that the sum is infinite:
$\langle x, y \rangle = \sum_{n=-\infty}^{\infty} x[n] \, y[n]^*$
The angle is also the same:
$\cos \theta_{x,y} = \frac{\Re{\langle x, y \rangle}}{\|x\|_2 \, \|y\|_2}$</p><p id="eip-462"><span data-type="title">Linear Combinations of Infinite Length Vectors</span>When it comes to infinite-length vectors, we will also be interested in linear combinations of them. However, in contrast with finite-length vectors, we will be interested in the linear combination of an infinite number of the vectors:
$y = \sum_{m=-\infty}^{\infty} \alpha_m x_m$
As with finite-length vectors, infinite-length vectors can be stacked into a matrix (an infinitely large one) and be multiplied by an infinite length vector of scalers to perform linear combination:
$X = \begin{bmatrix} \cdots | x_{-1} | x_0 | x_1 | \cdots \end{bmatrix}$
$a = \begin{bmatrix} \vdots \\ \alpha_{-1} \\ \alpha_0 \\ \alpha_1 \\ \vdots \end{bmatrix}$

$\begin{align*}y&amp;= \sum_{m=-\infty}^{\infty} \alpha_m x_m
&amp;= \begin{bmatrix} 
           &amp; \vdots &amp; \vdots &amp; \vdots &amp;  \\
\cdots &amp; x_{-1}[-1] &amp; x_{0}[-1] &amp; x_{1}[-1] &amp; \cdots \\
\cdots &amp; x_{-1}[0] &amp; x_{0}[0] &amp; x_{1}[0] &amp; \cdots \\
\cdots &amp; x_{-1}[1] &amp; x_{0}[1] &amp; x_{1}[1] &amp; \cdots \\
           &amp; \vdots &amp; \vdots &amp; \vdots &amp;  \\
\end{bmatrix}  \begin{bmatrix} \vdots \\ \alpha_{-1} \\ \alpha_0 \\ \alpha_1 \\ \vdots \end{bmatrix}
&amp;= X a$</p></div></li></ul></li><li><div data-type="page"><h1>Discrete-Time Systems</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aca0590f6-a180-4540-95f9-db484cac49b5%401.html" data-type="page"><h1>Discrete-time Systems</h1><div data-type="document-title">Discrete-time Systems</div>
  <p id="delete_me"><!-- Insert module text here -->
Discrete-time signals are mathematical entities; in particular, they are functions with an independent time variable and a dependent variable that typically represents some kind of real-world quantity of interest. But as interesting as a signal may be on its own, engineers usually want to <em data-effect="italics">do</em> something to it. This kind of action is what discrete-time systems are all about. A <span data-type="term">discrete-time system</span> is a mathematical transformation that maps a discrete-time input signal (usually designated $x$) into a discrete-time output signal (usually designated $y$). In other words, it takes an input signal and modifies it to produce an output signal:
<figure id="NAME"><figcaption>System $\mathcal{H}$ takes takes a discrete time signal $x$ as an input and produces an output $y$.</figcaption><span data-type="media" id="NAME-plot" data-alt="Image">
<img src="/resources/549f1e17c9daf84bb2611651997a80a3606d5e17/systemio.png" data-media-type="image/png" alt="Image" width="400">
<span data-media-type="image/png" data-print="true" data-src="/resources/549f1e17c9daf84bb2611651997a80a3606d5e17/systemio.png" data-type="image" width="100"></span>
</span></figure>
There is no end to the possibilities of what a system could do. Systems might be trivially dull (e.g., produce an output of 0 regardless of the input) or incredibly complex (e.g., isolate a single voice speaking in a crowd). Here are a few examples of systems:
</p><div data-type="list" data-list-type="bulleted" id="system-examples">
<div data-type="item">A speech recognition system converts acoustic waves of speech into text </div>
<div data-type="item">A radar system transforms the received radar pulse to estimate the position and velocity of targets </div>
<div data-type="item">A functional magnetic resonance imaging (fMRI) system transforms measurements of electron spin into voxel-by-voxel estimates of brain activity </div>
<div data-type="item">A 30 day moving average smooths out the day-to-day variability in a stock price </div>
</div><p id="eip-263"><span data-type="title">Signal Length and Systems</span>Recall that discrete-time signals can be broadly divided into two classes based upon their length: they are either infinite length or finite length (and recall also that periodic signals, though infinite in length, can be viewed as finite-length signals when we take a single period into account). Likewise, discrete-time systems are also finite or infinite length, depending on the kind of input signals they take. Finite-length systems take in a finite-length input and produce a finite-length output (of the same length), with infinite-length systems doing the same for infinite-length signals.</p><p id="eip-197"><span data-type="title">Examples of Discrete-time Systems</span>So a system takes an input signal $x$ and produces an output signal $y$. How does this look, mathematically? Below are several examples of systems and their mathematical expression:
</p><div data-type="list" data-list-type="bulleted" id="system-examples-math"><div data-type="item">
Identity: $y[n] = x[n]$
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$
 \]
</div>
<div data-type="item">
Decimate: $y[n] = x[2n]$
</div>
<div data-type="item">
Square time: $y[n] = x[n^2]$
</div>
<div data-type="item">
Moving average (combines shift, sum, scale): $y[n] = \frac{1}{2}(x[n]+x[n-1])$
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$</div>
</div><p id="eip-981">So systems take input signals and produce output signals. We have seen some examples of systems, and have also introduced a broad categorization of systems as either operating on finite or infinite length signals.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A9c7e1cfb-e9fa-46df-bb19-04c5ab29f182%401.html" data-type="page"><h1>Linear Systems</h1><div data-type="document-title">Linear Systems</div>
  <p id="delete_me"><!-- Insert module text here -->
Discrete-time systems are mathematical transformations that take input signals and map them to output signals. For a given input $x$, a discrete-time system will produce a new signal $y$:
<figure id="systemio"><span data-type="media" id="systemio-plot" data-alt="Image">
<img src="/resources/549f1e17c9daf84bb2611651997a80a3606d5e17/systemio.png" data-media-type="image/png" alt="Image" width="250">
</span></figure>
It turns out that it is very important to know or determine if a system has certain characteristics, and among these is the classification of linearity. A system is <span data-type="term">linear</span> if it has two special properties: <em data-effect="italics">scaling</em> (sometimes called homogeneity) and <em data-effect="italics">additivity</em>. The first of these is satisfied if, for any arbitrary input $x$, scaling the input by any complex valued constant value $\alpha$ will result in the output being scaled by the same value. Mathematically, this scaling rule is represented as $H\{\alpha x\}=\alpha H\{x\}$, and as a system diagram it looks like this:
<figure id="scaling"><span data-type="media" id="scaling-plot" data-alt="Image">
<img src="/resources/9883fef8982005ccbd3728cf0a62e96b79918300/scaling.png" data-media-type="image/png" alt="Image" width="250">
</span></figure>
A system has the additivity property if, for any two arbitrary inputs, the output of the sum of them is the same as the sum of their individual outputs: $H\{x_1+x_2\}=H\{x_1\}+H\{x_2\}$ .
<figure id="additivity"><span data-type="media" id="additivity-plot" data-alt="Image">
<img src="/resources/00323a3dfaa64d2129162d3bd1fd472e374e6094/additivity.png" data-media-type="image/png" alt="Image" width="450">
</span></figure>
If a system lacks either of these properties, then it is said to be <span data-type="term">nonlinear</span>.</p><p id="eip-676"><span data-type="title">Determining Linearity</span>Consider again the definition of linearity; to be linear, a system must preserve the scaling and additivity properties for any arbitrary input. Therefore, determining linearity amounts to completing a mathematical proof that assumes an arbitrary input (or two inputs, for additivity) and the conditions of property in question, and then shows the necessary result. Suppose $H\{x[n]\}=3x[n]$. Here is how the additivity proof would look:</p><p id="eip-25">Let $x_1$ and $x_2$ be arbitrary inputs to system $H$.
$\begin{align*}
H\{x_1[n]\}&amp;=3x_1[n]\\
H\{x_2[n]\}&amp;=3x_2[n]\\
H\{x_1[n]+x_2[n]\}&amp;=3(x_1[n]+x_2[n])\\
&amp;=3x_1[n]+3x_2[n]\\
&amp;=H\{x_1[n]\}+H\{x_2[n]\}
\end{align*}$</p><p id="eip-426">Now, to show a system is nonlinear requires a different kind of proof. Rather than having to prove both of the properties hold for any arbitrary input(s), only a single example needs to be provided for which either of the properties fail.  For example, consider the system $H\{x[n]\}=x[n]+1$. We can show it is nonlinear thus:
$\begin{align*}
\textrm{Let } x[n]&amp;=0\\
H\{x[n]\}&amp;=x[n]+1\\
&amp;=0+1\\
&amp;=1\\
\textrm{But } H\{2x[n]\}&amp;=2x[n]+1\\
&amp;=2\cdot 0+1\\
&amp;=1\\
&amp;\neq 2 H\{x[n]\}\rightarrow \textrm{Nonlinear}
\end{align*}$</p>

<div data-type="exercise" class="exercise" id="grilltest"><div data-type="problem" class="problem" id="fs-id1170043802008">
<p id="grilltestp1">
Good students of signals and systems must become adept at determining the linearity (or nonlinearity) of systems. Practice on the system examples below; which of them are linear?
</p><div data-type="list" data-list-type="bulleted" id="system-examples-math"><div data-type="item">
Identity: $y[n] = x[n]$
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$
 \]
</div>
<div data-type="item">
Decimate: $y[n] = x[2n]$
</div>
<div data-type="item">
Square time: $y[n] = x[n^2]$
</div>
<div data-type="item">
Moving average (combines shift, sum, scale): $y[n] = \frac{1}{2}(x[n]+x[n-1])$
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$</div>
</div>

</div>
<div data-type="solution" class="solution" id="fs-id1170031618352">
<p id="sol1p1">
</p><div data-type="list" data-list-type="bulleted" id="system-examples-mathans"><div data-type="item">
Identity: $y[n] = x[n]$ <strong>Linear</strong>
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$ <strong>Linear</strong>
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$ <strong>Nonlinear</strong>
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$ <strong>Nonlinear</strong>
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$ <strong>Linear</strong>
</div>
<div data-type="item">
Decimate: $y[n] = x[2n]$ <strong>Linear</strong>
</div>
<div data-type="item">
Square time: $y[n] = x[n^2]$ <strong>Linear</strong>
</div>
<div data-type="item">
Moving average (combines shift, sum, scale): $y[n] = \frac{1}{2}(x[n]+x[n-1])$ <strong>Linear</strong>
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$ <strong>Linear</strong></div>
</div>

</div>
</div><p id="eip-651"><span data-type="title">Linear Systems and Matrix Multiplication</span>One of the interesting characteristics of linear systems is that the systems' input/output relationship can be expressed as a matrix multiplication (note that this is distinct from the notion of using matrix multiplication to express one signal as a linear combination of others [LINK]). In fact, this relationship is actually an identity: any linear system can be expressed as a matrix multiplication, and matrix multiplications are linear systems. Below is how to represent any linear system mathematically, either in matrix multiplication notation,
$y ~=~ {\bf H}\, x$
or, in summation notation:
$y[n] ~=~ \sum_m \: [{\bf H}]_{n,m} \, x[m] ~=~ \sum_m \: h_{n,m} \, x[m]$ (where $h_{n,m} ~=~ [{\bf H}]_{n,m}$ represents the row-$n$, column-$m$ entry of the matrix $\bf H$).
</p><p id="eip-615">This matrix multiplication can be understood in two ways. First, the multiplication means that each value in the vector $y$ is the inner product of the corresponding row of $H$ with the vector $x$. Or, equivalently, the vector $y$ can be seen as a weighted sum of the columns of $H$, with the values in the vector $x$ being the weights of the corresponding columns. Below is a picture of matrix multiplication, with different colors representing different values. Try to comprehend the multiplication with both perspectives.
<figure id="yhx2"><span data-type="media" id="yhx2-plot" data-alt="Image">
<img src="/resources/d531b11555a3466a9dd6489979c2e872da802048/yhx2.png" data-media-type="image/svg+xml" alt="Image" width="400">
</span></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aeeec78c6-1006-421a-aa08-f46cc23911e0%401.html" data-type="page"><h1>Time-Invariant Systems</h1><div data-type="document-title">Time-Invariant Systems</div>
  <p id="delete_me"><!-- Insert module text here -->
Recall that a discrete-time system is a mathematical entity that takes an input signal (usually denoted $x$) and produces an output signal (usually denoted $y$). In the study of signal processing, systems that have certain characteristics are of particular interest. Among these characteristics is <span data-type="term">time-invariance</span>.</p><p id="eip-254">There is a very basic intuition behind the notion of time-invariance. In many cases, we would like a system to behave a certain way, no matter when an input may be given. To use a practical analogy, people expect their toasters to operate the same way on Tuesdays as they do on Mondays.</p><p id="eip-152">A system is time-invariant if a time delay for an input results in the same time delay on the output. Expressing this idea of system time-invariance mathematically is straightforward. Consider a system $H$; let $x[n]$ be some arbitrary input, and call the system's output for that input $y[n]$. $H$ is time-invariant if, for some arbitrary integer value $q$, $H[x[n-q]]=y[n-q]$:
<figure id="tisystemio"><span data-type="media" id="tisystemio-plot" data-alt="Image">
<img src="/resources/ea6feecece45c6b3ae5cf70d3f3b2d2fb7b631f4/tisystemio.png" data-media-type="image/png" alt="Image" width="300">
</span></figure>
</p><p id="eip-825"><span data-type="title">Example: Moving Average System</span>Consider, for example, the moving average system $y[n]= \frac{1}{2}(x[n]+x[n-1])$. Is this system time invariant? To find out, let's delay the input by some value $q$, and see what the output is. We'll call the delayed input signal $x'[n]$ (=$x[n-q]$) and the new output $y'$:
$y'[n]=\frac{1}{2}(x'[n]+x'[n-1])=\frac{1}{2}(x[n-q]+x[(n-1)-q])$
Now the important question, is $y'[n]=y[n-q]$? That will be easy to determine, simply replace every $n$ with $n-q$ in that original $y[n]$ equation and see if it is the same as $y'[n]$:
$y[n-q]=\frac{1}{2}(x[n-q]+x[n-q-1])=\frac{1}{2}(x[n-q]+x[(n-1)-q])=y'[n]\checkmark$</p><p id="eip-938"><span data-type="title">Example: Decimation</span>Now let's consider a decimation system, $y[n]=x[2n]$. Is this system time-invariant? First, create a new signal that is a delayed version of $x[n]$: $x'[n]=x[n-q]$. Next, find the output corresponding to this new signal: $y'[n]=x'[2n]$. Now express this output in terms of the original input: $y'[n]=x'[2n]=x[2n-q]$. Finally, check to see if this is the same as $y[n-q]$. As $y[n-q]=x[2(n-q)]$ is not equivalent to $y'[n]$ (for $2n-1\neq 2(n-q)$ for all $n$ and $q$), the system is not time-invariant.</p><div data-type="exercise" class="exercise" id="eip-971"><div data-type="problem" class="problem" id="eip-249">
  <p id="eip-400">
Now practice on the system examples below; which of them are time-invariant?  </p><div data-type="list" data-list-type="bulleted" id="system-examples-math"><div data-type="item">
Identity: $y[n] = x[n]$
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$
 \]
</div>

<div data-type="item">
Square time: $y[n] = x[n^2]$
</div>

<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$</div>
</div>

</div>

<div data-type="solution" class="solution" id="eip-281">
  <p id="eip-200">
   </p><div data-type="list" data-list-type="bulleted" id="system-examples-mathans"><div data-type="item">
Identity: $y[n] = x[n]$ <strong>time-invariant</strong>
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$ <strong>time-invariant</strong>
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$ <strong>time-invariant</strong>
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$ <strong>time-invariant</strong>
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$ <strong>time-invariant</strong>
</div>

<div data-type="item">
Square time: $y[n] = x[n^2]$ <strong>not time-invariant</strong>
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$ <strong>time-invariant</strong></div>
</div>

</div>
</div><p id="eip-744"><span data-type="title">Time-Invariance for Finite-Length Systems</span>All of the discussion above has considered systems which take infinite-length signals as inputs. A system which takes finite-length signals as inputs and produces them as outputs can also exhibit the characteristic of time-invariance, defined slightly differently. Such systems are time-invariant if any <em data-effect="italics">circular</em> shift on the input results in a corresponding <em data-effect="italics">circular</em> shift on the output:
<figure id="DTtisystemio"><span data-type="media" id="DTtisystemio-plot" data-alt="Image">
<img src="/resources/660f57474bf56e31a76a4efca2226e09b06820a5/DTtisystemio.png" data-media-type="image/png" alt="Image" width="300">
</span></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa7fb0c19-dcde-45c8-b2b1-321327b267a9%401.html" data-type="page"><h1>Linear Time-Invariant Systems</h1><div data-type="document-title">Linear Time-Invariant Systems</div>
  <p id="delete_me"><!-- Insert module text here -->
Among the many characteristics and classifications of discrete-time systems, two of particular importance are linearity and time-invariance [LINKS]. If a system happens to exhibit <em data-effect="italics">both</em> of these qualities, then it is referred to as being an <span data-type="term">LTI</span> system (linear time-invariant). These systems are very significant in the study of signal processing, for reasons that will be clear when the concept of the system impulse response is considered.</p><div data-type="exercise" class="exercise" id="eip-521"><div data-type="problem" class="problem" id="eip-961">
  <p id="eip-764">
Consider the systems below. Which are LTI?
</p><div data-type="list" data-list-type="bulleted" id="system-examples-math"><div data-type="item">
Identity: $y[n] = x[n]$
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$
 \]
</div>
<div data-type="item">
Decimate: $y[n] = x[2n]$
</div>
<div data-type="item">
Square time: $y[n] = x[n^2]$
</div>
<div data-type="item">
Moving average (combines shift, sum, scale): $y[n] = \frac{1}{2}(x[n]+x[n-1])$
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$</div>
</div>

</div>

<div data-type="solution" class="solution" id="eip-119">
  <p id="eip-220">
</p><div data-type="list" data-list-type="bulleted" id="system-examples-mathans"><div data-type="item">
Identity: $y[n] = x[n]$ <strong>LTI</strong>
</div>
<div data-type="item">
Scaling: $y[n] = 2\, x[n]$ <strong>LTI</strong>
</div>
<div data-type="item">
Offset: $y[n] = x[n]+2$ <strong>not LTI (time-invariant, but not linear)</strong>
</div>
<div data-type="item">
Square signal: $y[n] = (x[n])^2$ <strong>not LTI (time-invariant, but not linear)</strong>
</div>
<div data-type="item">
Shift: $y[n] = x[n+m] \quad m\in Z$ <strong>LTI</strong>
</div>
<div data-type="item">
Decimate: $y[n] = x[2n]$ <strong>not LTI (linear, but not time-invariant)</strong>
</div>
<div data-type="item">
Square time: $y[n] = x[n^2]$ <strong>not LTI (time-invariant, but not linear)</strong>
</div>
<div data-type="item">
Moving average (combines shift, sum, scale): $y[n] = \frac{1}{2}(x[n]+x[n-1])$ <strong>LTI</strong>
</div>
<div data-type="item">
Recursive average: $y[n] =  x[n] + \alpha\,y[n-1]$ <strong>LTI</strong></div>
</div>
  
</div>
</div><p id="eip-798"><span data-type="title">Matrix Structure of LTI Systems (infinite-length)</span>We recall that linear systems can be expressed through a matrix multiplication. Suppose that a system is linear; any output $y$ can be expressed as the multiplication of an infinite-dimensional matrix $H$ with the input $x$:
$y ~=~ {\bf H} \, x
y[n] ~=~ \sum_m \: [{\bf H}]_{n,m} \, x[m]
$</p><p id="eip-967">Now if a linear system is also time-invariant, it turns out the matrix $H$ will have an interesting structure. To see this, we will first express the matrix multiplication in a summation notation, where $h_{n,m} ~=~ [{\bf H}]_{n,m}$ represents the row-$n$, column-$m$ entry of the matrix $\bf H$:
$
y[n] ~=~ {\cal H}\{ x[n]\} ~=~ \sum_{m=-\infty}^{\infty}  h_{n,m} \, x[m], \quad -\infty \lt n \lt \infty
$
Supposing the system $H$ is time-invariant, we have:
${\cal H}\{ x[n-q]\} ~=~ \sum_{m=-\infty}^{\infty}  h_{n,m} \, x[m-q] ~=~ y[n-q]$
If we apply a simple change of variables ($n' = n-q$ and $m' = m-q$), we then have:
${\cal H}\{ x[n']\} ~=~ \sum_{m'=-\infty}^{\infty}  h_{n'+q,m'+q} \, x[m'] ~=~ y[n']$
If we compare this final equation with the original one, we see that for an LTI
$h_{n,m} ~=~ h_{n+q,m+q} \quad \forall\: q\in\Integers$</p><p id="eip-633">So for LTI systems, the matrix $H$ that defines the system's input/output relationship has a special structure: $h_{n,m} ~=~ h_{n+q,m+q} \quad \forall\: q\in\Integers$ (where $h_{n,m}$ is the value at the nth row and mth column of the matrix $H$). Such matrices are called <span data-type="term">Toeplitz Matrices</span>. They have identical values along their diagonals:
<figure id="toeplitzmat"><span data-type="media" id="toeplitzmat-plot" data-alt="Image">
<img src="/resources/ae38d31174d18ba6cc91a4c30624ea68b49a43a2/toeplitzmat.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
Because of that property, all of the information in the matrix is contained in a single row, or column. We'll call the 0th row $h[n]$: $h[n] = h_{n,0}$:
<figure id="toeplitzmatimp"><span data-type="media" id="toeplitzmatimp-plot" data-alt="Image">
<img src="/resources/a812208b5601df3264c13c1e75e36ae3650c128d/toeplitzmatimp.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
Below is an example of a pictorial representation of a Toeplitz matrix. Note how the diagonals are all the same color:
<figure id="toeplitz1"><figcaption>CAPTION.</figcaption><span data-type="media" id="toeplitz1-plot" data-alt="Image">
<img src="/resources/19e4b7117e4fe52188746620a28638f4409b0b6a/toeplitz1.png" data-media-type="image/png" alt="Image" width="300">
<span data-media-type="application/postscript" data-print="true" data-src="toeplitz1.eps" data-type="image"></span>
</span></figure></p><p id="eip-3"><span data-type="title">Matrix Structure of LTI Systems (finite-length)</span>For systems operating on finite-length signals, we can apply similar analysis. Linear systems can be expressed as a matrix multiplication $y=Hx$, here shown using summation notation:
$y[n] ~=~ {\cal H}\{ x[n]\} ~=~ \sum_{m=0}^{N-1}  h_{n,m} \, x[m], \quad 0\leq n \leq N-1$

If we enforce time-invariance on such a system, we then have:
${\cal H}\{ x[(n-q)_N]\} ~=~ \sum_{m=0}^{N-1}  h_{n,m} \, x[(m-q)_N] ~=~ y[(n-q)_N]$

A change of variables ($n' = n-q$) then yields:
${\cal H}\{ x[(n')_N]\} ~=~ \sum_{m'=-q}^{M-1-q}  h_{(n'+q)_N,(m'+q)_N} \, x[(m')_N] ~=~ y[(n')_N]$

We see, then, that for LTI finite-length systems, $h_{n,m} ~=~ h_{(n+q)_N,(m+q)_N} \quad \forall\: q\in\Integers$. This is similar to the Toeplitz structure for infinite-length systems, but note the circular shifts, how the rows "wrap" around the edges of the matrix:
<figure id="circulantmat"><span data-type="media" id="circulantmat-plot" data-alt="Image">
<img src="/resources/17b8f47a26bd56c9b0d5daaaca26562eae05a837/circulantmat.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>

As with the Toeplitz matrices, all of the information in a circulant matrix is contained in a single row or column, the other rows/columns simply being shifted versions of the original. So we can call the N-length signal $h[n]$ the 0th column of the matrix, and express the whole matrix in terms of it:
<figure id="circulantmatimp"><span data-type="media" id="circulantmatimp-plot" data-alt="Image">
<img src="/resources/8e1c1251ac07b5a4e90bfa2293440d1e44dd83dc/circulantmatimp.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>

Below is a pictorial representation of a circulant matrix. Like Toeplitz matrices, the values along diagonals are equivalent, but in addition to having this property, the values "wrap around" the matrix boundaries:
<figure id="circulant1"><span data-type="media" id="circulant1-plot" data-alt="Image">
<img src="/resources/d91e9e37e728598312b6afbd5fce5c71c43239b9/circulant1.png" data-media-type="image/png" alt="Image" width="300">
</span></figure></p></div></li></ul></li><li><div data-type="page"><h1>Convolution</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8e9c729f-20f2-4ced-842e-6f774282e2e6%401.html" data-type="page"><h1>The Impulse Response of Discrete-Time Systems</h1><div data-type="document-title">The Impulse Response of Discrete-Time Systems</div>
  <p id="delete_me"><span data-type="title">The Impulse Response of Infinite-Length LTI Systems</span>Systems are mathematical transformations that take input signals and map them to output signals:
<figure id="xHy"><figcaption>The system $H$ takes an input $x$ and produces an output $y$.</figcaption><span data-type="media" id="xHy-plot" data-alt="Image">
<img src="/resources/4c446c3aca5ef4b8a542431fdb5ff5445d5e5341/xHy.png" data-media-type="image/png" alt="Image" width="200">
</span></figure>
Recall that linear systems can be expressed as matrix multiplications, the output $y$ being the product of the infinitely long matrix $H$ and the input vector $x$: $x=Hx$. Recall also that if a linear system is also time-invariant, the matrix $H$ has a special "Toeplitz" structure: each column/row is simply a shifted version of any other column/row:
<figure id="hH"><span data-type="media" id="hH-plot" data-alt="Image">
<img src="/resources/423943f998cb08d998f19257b7fbc0473250f4fa/hH.png" data-media-type="image/png" alt="Image">
</span></figure>
The 0th column of the $H$ matrix for an LTI system has a special interpretation. If we multiply the $H$ matrix by a delta function, then the result will simply be that 0th column:
<figure id="Hddh"><figcaption>Note how when the $H$ matrix is multiplied by it, the delta function "selects" the 0th column.</figcaption><span data-type="media" id="Hddh-plot" data-alt="Image">
<img src="/resources/6898991ddfbcc90d172326e839528b3000c6c0fa/Hddh.png" data-media-type="image/png" alt="Image">
</span></figure>
We call the result of this multiplication, i.e. the multiplication of the $H$ matrix by delta (impulse) function, the <span data-type="term">impulse response</span> of the system. It is usually denoted $h[n]$, or if referring to it simple as a vector, as $h$. The impulse response $h[n]$ is simply the output of a system when the input is $\delta[n]$. Using the matrix notation from above, $h$ is the 0th column of the system matrix $H$.</p><p id="eip-468">Because the impulse response is simply a column (specifically, the 0th column) of the system matrix $H$, then if $H$ is an LTI system, the impulse response completely characterizes the system. It tells us absolutely everything we need to know about the system! Why is this? Because we can construct the entire system matrix $H$ by simply shifting the impulse response accordingly. The value of the matrix at row $n$ and column $m$ is simply $h[n-m]$.</p><p id="eip-341">As a result, we can also express the system input-output matrix multiplication in terms of the impulse response. If we have that $[{\bf H}]_{n,m}$ is the value of the $H$ matrix at row $n$ and column $m$, then the matrix multiplication $y[n]=Hx=\sum_\limits_{m=-\infty}^{\infty}[{\bf H}]_{n,m}x[m]$. From above we have the matrix values expressed in terms of the impulse response; subbing this in we see the output is a sum involving the impulse response and the input: $y[n]=Hx=\sum_\limits_{m=-\infty}^{\infty}h[n-m]x[m]$.</p><div data-type="example" class="example" id="eip-835"><div data-type="title" class="title">Impulse Response of the Scaling System</div><p id="eip-335">Let's find the impulse response of a simple scaling system:
$y[n]=H\{x[n]\})=2x[n]$
To find the impulse response $h[n]$, simply calculate the output when the input is a delta function:
$h[n]=H\{\delta[n]\})=2\delta[n]$
<figure id="deltah"><span data-type="media" id="deltah-plot" data-alt="Image">
<img src="/resources/4d3325353373861faa5e1ec7100ddf2e92b6da25/deltah.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="deltah.eps" data-type="image"></span>
</span></figure>
If we would like to know what the system matrix $H$ is, we use the formula $[{\bf H}]_{n,m} = h[n-m] = 2\delta[n-m]$. Below is a pictorial representation of that matrix:
<figure id="toeplitzDeltah"><span data-type="media" id="toeplitzDeltah-plot" data-alt="Image">
<img src="/resources/dc6a00c8be278a2aa7d42351493d1c813246d0bf/toeplitzDeltah.png" data-media-type="image/png" alt="Image">
</span>
</figure></p></div><div data-type="example" class="example" id="eip-670"><div data-type="title" class="title">Impulse Response of the Shift System</div><p id="eip-877">Now let's try finding the impulse response of a simple time-shifting system:
$y[n]=H\{x[n]\}=x[n-2]$
Once again, find the impulse response by giving a delta function as the input:
$h[n]=H\{\delta[n]\}=\delta[n-2]$
<figure id="deltaS"><span data-type="media" id="deltaS-plot" data-alt="Image">
<img src="/resources/199411f561bbb534f38a3dde21a48669a8085945/deltaS.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/cf027926534e7be4ad00b58aa53cac841e32a412/deltaS.eps" data-type="image"></span>
</span></figure>
The system matrix $H$ can be found, as before, as $[{\bf H}]_{n,m} = h[n-m] = \delta[n-m-2]$
<figure id="toeplitzDeltaS"><span data-type="media" id="toeplitzDeltaS-plot" data-alt="Image">
<img src="/resources/afba1808a86eebf22d2f79ab04e0855d525d0103/toeplitzDeltaS.png" data-media-type="image/png" alt="Image">
</span></figure></p></div><div data-type="example" class="example" id="eip-427"><div data-type="title" class="title">Impulse Response of a Moving Average System</div><p id="eip-399">Consider now a simple moving average system:
$y[n] ~=~ {\cal H}\{ x[n] \} ~=~ \frac{1}{2}\left( x[n]+x[n-1]\right)$
For this system, we find that the impulse response is:
$h[n] ~=~ {\cal H}\{ \delta[n] \} ~=~ \frac{1}{2}\left( \delta[n]+\delta[n-1]\right)$
<figure id="hMA"><span data-type="media" id="hMA-plot" data-alt="Image">
<img src="/resources/839b200ab461f47f67d9f4b4eec0307dad85590d/hMA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ba99e536eff65f87fbd1a67a585d0df551f313a4/hMA.eps" data-type="image"></span>
</span></figure>
The system matrix is found to be $[{\bf H}]_{n,m} = h[n-m] = \frac{1}{2}\left( \delta[n-m]+\delta[n-m-1]\right)$
<figure id="toeplitzMA"><span data-type="media" id="toeplitzMA-plot" data-alt="Image">
<img src="/resources/6e7827ad928a284f1a59f995a5e5fc0db0610019/toeplitzMA.png" data-media-type="image/png" alt="Image">
</span></figure></p></div><div data-type="example" class="example" id="eip-222"><div data-type="title" class="title">Impulse Response of the Recursive Average System</div><p id="eip-373">A recursive moving average system has the following input-output relationship: $y[n] ~=~ {\cal H}\{ x[n] \} ~=~ x[n] + \alpha\, y[n-1]$. To find the impulse response of this system, we first let $x[n]=\delta[n]$ and find the output:
$h[n]={\cal H}\{ x[n] \}+\alpha\, h[n-1]$
A bit more work is required if we would like a non-recursively defined impulse response. Assuming that the system $H$ is initially at rest (it has no output in the absence of an input), then we can find the output by computing several values of $h[n]$:
$h[-1]=0$
$h[0]=\delta[0]+\alpha h[-1]=1+\alpha \cdot 0=1$
$h[1]=\delta[1]+\alpha h[0]=0+\alpha\cdot 1=\alpha$
$h[2]=\delta[2]+\alpha h[1]=0+\alpha\cdot \alpha=\alpha^2$
Having discerned a pattern we have that $h[n]=\alpha^n u[n]$ (this may be more rigorously proven by induction, if desired):
<figure id="hRA"><span data-type="media" id="hRA-plot" data-alt="Image">
<img src="/resources/fc391de8f568802cc377e0c541d1404e8f208a73/hRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b8d6137419b653eb911648d432ed36f93c1177d6/hRA.eps" data-type="image"></span>
</span></figure>
<figure id="toeplitzRA"><figcaption>A pictorial representation of the recursive average system.</figcaption><span data-type="media" id="toeplitzRA-plot" data-alt="Image">
<img src="/resources/a054a9f7f3b715ddeda2103e17999aa6ea896d55/toeplitzRA.png" data-media-type="image/png" alt="Image">
</span></figure></p></div><p id="eip-563"><span data-type="title">The Impulse Response of Finite-Length LTI Systems</span>For a finite-length LTI system, just as with infinite length systems, each rows/columns of its system matrices is a shifted version of another row/column. Thus, the system matrices for finite length systems are also of a Toeplitz structure. However, for finite-length systems not only are the rows/columns shifted, they are circularly shifted; the values in the rows/columns "wrap around" when shifted across the boundary of the matrix.</p><p id="eip-779">As a result, the entire system matrix can be expressed in terms of the 0th column of the matrix, which (as with infinite length systems) is given the mathematical expression $h[n]$ and is named the impulse response. For a length $N$ LTI system, the formula to determine the value of the matrix at a particular row $n$ and column $m$ is $[{\bf H}]_{n,m} ~=~ h[(n-m)_N]$. Note how this is nearly identical to the infinite length formula, except for the modulo-$N$ operator.
<figure id="hHfl"><figcaption>A pictorial representation of the impulse response and system matrix of a finite-length LTI system.</figcaption><span data-type="media" id="hHfl-plot" data-alt="Image">
<img src="/resources/be057a9e4bbf4e143b26b7cbbd58114f6d34a34c/hHfl.png" data-media-type="image/png" alt="Image">
</span></figure>
As with infinite-length systems, being able to express the entire system matrix in terms of the impulse response means that the system's input-output matrix multiplication relationship can be expressed as a sum involving the input and impulse response: $y[n]=Hx=\sum_{m=0}^{N-1} \: h[(n-m)_N] \, x[m]$</p><p id="eip-888"><span data-type="title">Impulse Response Length of LTI Systems</span>For LTI systems, it is often of interest whether its impulse response is of infinite length (i.e., infinite nonzero support) or finite length. Such as distinction is typically made only for infinite-length systems (as the impulse response of finite-length systems is obviously always finite-length!). If a system has an impulse response of infinite length, it is called an <span data-type="term">infinite impulse response (IIR)</span> system; its impulse response is of finite length, it is called a <span data-type="term">finite impulse response (FIR)</span> system.
<figure id="iirfir" data-orient="vertical"><figcaption>Examples of IIR and FIR systems.</figcaption><figure id="iir-plot"><figcaption>A recursive average system ($y[n]=x[n]+\alpha y[n-1]$) is an example of an IIR system.</figcaption>
<span data-type="media" id="iir" data-alt="Image">
<img src="/resources/fc391de8f568802cc377e0c541d1404e8f208a73/hRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b8d6137419b653eb911648d432ed36f93c1177d6/hRA.eps" data-type="image"></span></span>
</figure>
<figure id="fir-plot"><figcaption>A moving average system ($y[n]=\frac{1}{2}(x[n]+x[n-1])$) is an example of an FIR system.</figcaption>
<span data-type="media" id="fir" data-alt="Image">
<img src="/resources/839b200ab461f47f67d9f4b4eec0307dad85590d/hMA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ba99e536eff65f87fbd1a67a585d0df551f313a4/hMA.eps" data-type="image"></span></span>
</figure>
</figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A20178296-96b0-46c2-b296-7ae0567f3732%401.html" data-type="page"><h1>Convolution of Infinite-Length Discrete-Time Signals</h1><div data-type="document-title">Convolution of Infinite-Length Discrete-Time Signals</div>
  <p id="delete_me">It is a fundamental operation in signal processing, given an LTI system $H$ and input $x[n]$, to find the output $y[n]$. From our study of LTI systems, we have seen that there are several ways to find the output of the system.
--One is to find the output $y[n]$ at each time $n$ using the input-output relationship (which defines the out $y[n]$ in terms of the input and output at other various times).
--Another option is to take the system matrix $H$ and the input vector $x[n]$ and perform a matrix multiplication $y=Hx$
--A final option is to find the output at a given time in terms of a summation involving only the input and the system's impulse response:$y[n] ~=~ \sum_{m=-\infty}^\infty h[n-m] \, x[m]$

That final option has a special name: it is the <span data-type="term">convolution</span> of $x[n]$ and the system impulse response $h[n]$. It also has a special operator symbol, $\ast$, so that the convolution of $x[n]$ and $h[n]$ is expressed as $x[n]\ast h[n]$. </p><p id="eip-203"><span data-type="title">Computing Convolution</span>Given an LTI system's impulse response $h[n]$ and an input $x[n]$, the process of computing the output $y[n]$ via the convolution sum formula ($\sum_{m=-\infty}^\infty h[n-m] \, x[m]$) is straightforward, according to the following steps:
--Step 1: decide which of $x$ or $h$ you will flip and shift; you have a choice since $x \ast h = h \ast x$. We will suppose you choose to flip $h$.
--Step 2: plot $x[m]$
--Step 3: plot $h[-m]$, the time reversed version of the impulse response
--Step 4:~ To compute $y$ at the time point $n$, plot the time-reversed impulse response after it has been shifted to the right (delayed) by $n$ time units:  $h[-(m-n)]=h[n-m]$
--Step 5: $y[n]=$ the inner product between the signals $x[m]$ and $h[n-m]$
(Note: for complex signals, do not complex conjugate the second signal in the inner product)
-- Step 6: Repeat for all $n$ of interest (potentially all $n\in Z$)
--Step 7: Plot $y[n]$ and perform a reality check to make sure your answer seems reasonable</p><div data-type="example" class="example" id="eip-919"><p id="eip-146">Here we will illustrate convolution by convolving two signals together, $x[n]$ and $h[n]$, plotted below. We will call the result of the convolution $y[n]$, so $y[n]=x[n]\ast h[n]$.
<figure id="origxhplots" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xn-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xn" data-alt="Image">
<img src="/resources/5d7cc588a547346ec2e4d4cff6facf695c48118f/xn.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7d35297bc050775a056ba7466cdd7139c85a2fda/xn.eps" data-type="image"></span></span>
</figure>
<figure id="hn-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="hn" data-alt="Image">
<img src="/resources/1464d0cc322028bb0ab5a5044bf78a4fc1cb1629/hn.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/86a3780c74e954a66d80b25841db9af9aa6212be/hn.eps" data-type="image"></span></span>
</figure>
</figure>
Recall that the first step of the convolution process is to decide which of the signals we will "flip and shift." The final result will be the same in either case, the only difference is in working the problem out. But here it truly does not matter, because the signals are identical. But we will say that we are flipping and shifting $h[n]$.
</p><p id="eip-415">The second and third steps are to plot $x[m]$ and $h[-m]$:
<figure id="step23" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="hnm-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="hnm" data-alt="Image">
<img src="/resources/1d4fba64a6d4401d32752b5b1e4189be87b2ba86/hnm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/67a001fdd653e8177ccbb034ac64e4eb3244e25b/hnm.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-608">Now step 4 and 5 are to shift $h[n-m]$ by varying $n$ and then computing the inner product between $h[n-m]$ and $x[m]$. For each shift, that inner product will be the value of the output $y[n]$ for that value of $n$. In the figures below, we will see how $h[n-m]$ and $x[m]$ look for each $n$ ($x[m]$ of course will not change), and will progressively build up $y[n]$ as $n$ increases. We'll start with $n=-1$:
<figure id="step451" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm2" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="hn1m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="hn1m" data-alt="Image">
<img src="/resources/12176f448aa777f666efb9f744c7ee15a7a2a6bb/hn1m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/071fcb0d64e2e195ca55ead979ee6878cd6d46c2/hn1m.eps" data-type="image"></span></span>
</figure>
<figure id="yn1n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="yn1n" data-alt="Image">
<img src="/resources/7b1db93382f346d6df346f68bb8969d82baf10f9/yn1n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/02326924f954b474d86c1b8fbacfd07f7230cf40/yn1n.eps" data-type="image"></span></span>
</figure>
</figure>
Notice how for this value of $n$, and for all $n$ less than that as well (for which $h[n-m]$ will be even farther to the left), $x[m]$ and $h[n-m]$ have no overlapping nonzero values. Therefore the dot product in these cases is zero.</p><p id="eip-155">So for $n\lt 0$, the dot products of $x[m]$ and $h[n-m]$ are zero, and hence $y[n]$ is zero for these $n$. Now let's consider $n=0$:
<figure id="step452" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm3-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm3" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h0m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h0m" data-alt="Image">
<img src="/resources/1d4fba64a6d4401d32752b5b1e4189be87b2ba86/hnm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/67a001fdd653e8177ccbb034ac64e4eb3244e25b/hnm.eps" data-type="image"></span></span>
</figure>
<figure id="y0n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y0n" data-alt="Image">
<img src="/resources/dd1fb3944f9ea9e1973b5b94d7e3a3cbdb0e2055/y0n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/0f739bbc1e1509e0703d1d3aac5e93b6d1aa7b50/y0n.eps" data-type="image"></span></span>
</figure>
</figure>
So now, for $n=0$, $x[m]$ and $h[n-m]$ overlap just a bit (at $m=0$), and the value of the dot product of the two signals is $1$. So $y[0]=1$.</p><p id="eip-81">The next shift to the right makes $n=1$:
<figure id="step453" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm4-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm4" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h1m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h1m" data-alt="Image">
<img src="/resources/5af1d65b44c89f9efeed4d31068fb6dce5cbc484/h1m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4292557e59848f4a1ef6e3a01e38c755f8f3d174/h1m.eps" data-type="image"></span></span>
</figure>
<figure id="y1n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y1n" data-alt="Image">
<img src="/resources/591e01aa7549a38e4b5e6097e3dbec1eab1b1733/y1n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/6e1cc3d60e2c4a80f13c382e18228e1fc3028c38/y1n.eps" data-type="image"></span></span>
</figure>
</figure>
There are two nonzero locations where $x[m]$ and $h[n-m]$ line up, $m=0$ and $m=1$. The dot product of the signals for this $n=1$ is $1\cdot 1+1\cdot 1=2$, so $y[2]=2$.</p><p id="eip-465">Continuing to shift $h[n-m]$ to the right, we have for $n=2$:
<figure id="step454" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm5-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm5" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h2m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h2m" data-alt="Image">
<img src="/resources/2d15610c6ba1f60bb51c9b2001afc14df88ff958/h2m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/c64e0c7a8872837a414482bca534bdc58b570d72/h2m.eps" data-type="image"></span></span>
</figure>
<figure id="y2n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y2n" data-alt="Image">
<img src="/resources/dc98d52113ede14cf82259d52b5232e885d587f0/y2n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/c5c8e254c15b572bad7a42ad6acdf1bc51edfa80/y2n.eps" data-type="image"></span></span>
</figure>
</figure>
For $n=2$, the two signals are now maximally aligned, with a dot product of $1\cdot 1+1\cdot 1+1\cdot 1=3$. So $y[2]=3.$</p><p id="eip-452">At $n=3$, $h[n-m]$ has moved past the point of maximum alignment; as with $n=1$, the dot product between it and $x[m]$ is $2$:
<figure id="step455" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm6-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm6" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h3m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h3m" data-alt="Image">
<img src="/resources/c86d61dc43b6627a665f508dd4e9c4f3bd8b2636/h3m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/76d5d3d816e7ddcf57021977d4bb71cf8b6906d1/h3m.eps" data-type="image"></span></span>
</figure>
<figure id="y3n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y3n" data-alt="Image">
<img src="/resources/f1fd49fb5fb3cfe1b26f17381ee9b2acd5b8be87/y3n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1678774d9432e48d0563c2fe1d73483f1c662082/y3n.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-624">We're approaching the end of our work on this convolution problem. At $n=4$, $h[n-m]$ has just about moved on past the point of overlapping nonzero values with $x[m]$:
<figure id="step456" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm7-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm7" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h4m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h4m" data-alt="Image">
<img src="/resources/142b9597f733b8ea4aed9d296cad5459c2346f36/h4m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1ef9530d193436dc55ac350ad78b1cd2d57111ce/h4m.eps" data-type="image"></span></span>
</figure>
<figure id="y4n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y4n" data-alt="Image">
<img src="/resources/8dee69f2e9bdd1aaec32cc1079da2d4562cf930d/y4n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/91d6757559df5273adc868b8e0ab79b1f064f636/y4n.eps" data-type="image"></span></span>
</figure>
</figure>
The got product between the two signals is only 1.</p><p id="eip-48">Finally, for $n=5$, there is no nonzero overlap between $x[m]$ and $h[n-m]$, so the dot product is zero. For all $n\gt 5$ (for which $h[n-m]$ moves even farther to the right) this is the case as well:
<figure id="step457" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xm8-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xm8" data-alt="Image">
<img src="/resources/47a9134e04d78c549ea6c7f30ba5640999fe107f/xm.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/679f9a58cb73731d5eb026211729686a3e08acc7/xm.eps" data-type="image"></span></span>
</figure>
<figure id="h5m-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="h5m" data-alt="Image">
<img src="/resources/1206bdd8025701f3a71c0dd3979c1fe29e824c3d/h5m.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/90d79cad12ce331654fa5fb7ba6ccb4edddbc4ea/h5m.eps" data-type="image"></span></span>
</figure>
<figure id="y5n-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="y5n" data-alt="Image">
<img src="/resources/5da1cd0febe3eb33dd55b81e64fe31c75df1ea71/y5n.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4813904f56af9c6c0731c9af749b221d66b2b21d/y5n.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-13">We have found the dot product between $x[m]$ and $h[n-m]$, and therefore $y[n]$, for all integer values of $n$, so our convolution computation is finished. The final step of the convolution is to examine the plot of $y[n]$ and do a quick "reality check" to make sure the answer is reasonable. At this introductory point, you might not know what to expect a convolution looks like, so it may not be clear if it is reasonable or not. But in time, especially as you understand the "flip and shift" nature of convolution, you will gain a feel for how convolutions should generally look.</p></div></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ad6d53200-e1f0-4b4a-9c64-de9abe526940%401.html" data-type="page"><h1>Convolution of Finite-Length Discrete-Time Signals</h1><div data-type="document-title">Convolution of Finite-Length Discrete-Time Signals</div>
  <p id="delete_me">Given a discrete-time LTI system with an impulse response $h[n]$, it is a common operation to find the system's output $y[n]$ for some input $x[n]$. There are several ways this can be done, including finding the  system matrix $H$ and then using it to find the output via matrix multiplication: $y=Hx$. Recalling the circulant structure of that matrix, the output can be expressed in formula form: $y[n] ~=~ x[n] \circledast h[n] ~=~ \sum_{m=0}^{N-1} \: h[(n-m)_N] \, x[m]$. This operation is known as <span data-type="term">circular convolution</span> (or <span data-type="term">finite-length convolution</span>).</p><p id="eip-453"><span data-type="title">Computing Circular Convolution</span>The circular convolution $x[n]\circledast h[n]$ can be computed using the following seven step procedure:
--First, decide which of $x$ or $h$ you will flip and shift. Since circular convolution is commutative, either choice will result in the same output.
--Second, plot the values of $x[m]$ on a circular "clock," beginning at "midnight" (for $m=0$), and continuing clockwise for a total number of "hours" that is equal to the length of the signal.
--Third, plot $h[(-m)_N]$ on another clock. This simply amounts to plotting the values, starting at midnight, in a counter-clockwise direction.
For a given value of $n$, $y[n]$ will be the dot product of the two "clocks." Your original plots of $x$ and $h$ correspond to $n=0$. For $n=1$, first rotate the values of $h$ in a clockwise manner, i.e., delay them one "hour" time unit. Rotating $h$ in this way is what creates $h[(n-m)_N]$. So:
--Fourth, rotate the $h$ clock accordingly and,
--Fifth, perform the inner product.
--Sixth, repeat this for $n$ ranging from $0$ to $N-1$.
--Seventh, plot your final answer and do a "reality check" to see if it seems reasonable.</p><div data-type="example" class="example" id="eip-608"><p id="eip-315">Let's see the circular convolution process in action. Consider the two finite-length ($N=8$) signals below:
<figure id="xhplots" data-orient="vertical"><figcaption>Discrete-time finite-length signals $x[m]$ and $h[m]$.</figcaption><figure id="x10-plot">
<span data-type="media" id="x10" data-alt="Image">
<img src="/resources/9098c4956994aa27681910a0d61af97d0cf58851/x10.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/cca235694c3966e6e2c85cd088bddafb46a7b40d/x10.eps" data-type="image"></span></span>
</figure>
<figure id="h10-plot">
<span data-type="media" id="h10" data-alt="Image">
<img src="/resources/74a22cef32d0f45af59599c2fb3fefc24f5968cc/h10.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a26d480d7c9dfb6344b7ca99ba74d4d786baae04/h10.eps" data-type="image"></span></span>
</figure></figure>
Since $N=8$, our "clock" will have have 8 "hours." We will place the "zero hour" at the top of the clock, as if it were midnight, but the starting place is arbitrary, so long as you are consistent (you can also choose to have your time run counter-clockwise, if you prefer).
<figure id="cwclock"><figcaption>For the purposes of computing circular convolution by hand, the values of the signals may be plotted on a "clock" with a number of hours equal to the length of the signals. The time values progress clockwise.</figcaption><span data-type="media" id="cwclock-plot" data-alt="Image">
<img src="/resources/6f5f91a7a1653f71d6fa82e8a4e2ea6a653f6e03/cwclock.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span></figure></p><p id="eip-707">The second step is to plot $x[m]$ and $h[(-m)_8]$. For $x$, this means starting at the zero our and then writing the values clockwise. For $h$, this means starting at zero and then writing the values counter-clockwise:
<figure id="ccex1" data-orient="vertical"><figcaption>Plots of $x[m]$ and $h[(n-m)_8]$.</figcaption><figure id="cwxm1-plot"><figcaption>The values of $x[m]$ are plotted clockwise, starting at "midnight."</figcaption>
<span data-type="media" id="cwxm1" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwhnm-plot"><figcaption>To plot $h[(-m)_8]$, the values of $h[m]$ are simply plotted counter-clockwise, i.e., backwards in time, starting at "midnight."</figcaption>
<span data-type="media" id="cwhnm" data-alt="Image">
<img src="/resources/273284be88977aceb694f414b63c661161947c48/cwhnm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure></p><p id="eip-733">Having plotted $x[m]$ and $h[(-m)_8]$, we can now begin to find our output values corresponding to each $n$. We'll start with $n=0$, which for $h[(n-m)_8]$ is of course what we have originally plotted:
<figure id="ccex2" data-orient="vertical"><figcaption>Plots of $x[m]$ and $h[(0-m)_8]$, the inner products of which is $y[0]$.</figcaption><figure id="cwxm2-plot"><figcaption>The plot of $x[m]$ will remain constant for each of the inner products.</figcaption>
<span data-type="media" id="cwxm2" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh0m-plot"><figcaption>The to plot $h[(n-m)_8]$, the $h[(-m)_8]$ signal is rotated $n$ "hour" time units clockwise. Here it is plotted for $n=0$.</figcaption>
<span data-type="media" id="cwh0m" data-alt="Image">
<img src="/resources/59df065bde226408342134b87ea76f38f2ac51c8/cwh0m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
We'll move around clockwise, starting at zero hour, to find the inner product. The value of the inner product for $n=0$ is $y[0]=1\cdot 0 +0\cdot 0+ -1\cdot0 +0 \cdot 0+1\cdot 4 +0\cdot 3 + -1 \cdot 2 + 0 \cdot 1=4-2=2$.</p><p id="eip-544">Now, for each successive value of $n$, starting with $n=1$ and going up to $n=7$, we will rotate the $h$ plot clockwise an "hour," and then find the inner product with the (unchanged) $x$.
<figure id="ccex3" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=1$.</figcaption><figure id="cwxm3-plot">
<span data-type="media" id="cwxm3" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh1m-plot">
<span data-type="media" id="cwh1m" data-alt="Image">
<img src="/resources/27fe24f2499989ce2d469d66433a370eb40caf2c/cwh1m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=1$ is $y[1]=1\cdot 1 +0\cdot 0+ -1\cdot0 +0 \cdot 0+1\cdot 0 +0\cdot 4 + -1 \cdot 3 + 0 \cdot 2=1-3=-2$.</p><p id="eip-417">Below are the plots for $n=2$. $x$ remains unchanged, and $h$ is shifted one hour clockwise from where it was at $n=1$:
<figure id="ccex41" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=2$.</figcaption><figure id="cwxm41-plot">
<span data-type="media" id="cwxm41" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh41m-plot">
<span data-type="media" id="cwh41m" data-alt="Image">
<img src="/resources/c0aa4502c73d5fcf72003056e6af2a0dbcc59c41/cwh2m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure></p><p id="eip-830">Now for $n=3$:
<figure id="ccex42" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=3$.</figcaption><figure id="cwxm42-plot">
<span data-type="media" id="cwxm42" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh42m-plot">
<span data-type="media" id="cwh42m" data-alt="Image">
<img src="/resources/f8bb1cb76d2dd66ec7bb564d9b7754fe7c874c6f/cwh3m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=3$ is $y[3]=1\cdot 3 +0\cdot 2+ -1\cdot 1 +0 \cdot 0+1\cdot 0 +0\cdot 0 + -1 \cdot 0 + 0 \cdot 4=3-1=2$.</p><p id="eip-742">Next, $n=4:
<figure id="ccex4" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=4$.</figcaption><figure id="cwxm4-plot">
<span data-type="media" id="cwxm4" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh4m-plot">
<span data-type="media" id="cwh4m" data-alt="Image">
<img src="/resources/1c5dfc6d59c19535a23e8f1031b0ac4397761c1f/cwh4m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=4$ is $y[4]=1\cdot 4 +0\cdot 3+ -1\cdot 2 +0 \cdot 1+1\cdot 0 +0\cdot 0 + -1 \cdot 0 + 0 \cdot 0=4-2=2$.</p><p id="eip-464">$n=5$:
<figure id="ccex5" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=5$.</figcaption><figure id="cwxm5-plot">
<span data-type="media" id="cwxm5" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh5m-plot">
<span data-type="media" id="cwh5m" data-alt="Image">
<img src="/resources/c64dba6c9761aebdddbb3a7f00d9952170fa1b01/cwh5m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=5$ is $y[5]=1\cdot 0 +0\cdot 4+ -1\cdot 3 +0 \cdot 2+1\cdot 1 +0\cdot 0 + -1 \cdot 0 + 0 \cdot 0=-3+1=-2$.</p><p id="eip-74">$n=6$:
<figure id="ccex6" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=6$.</figcaption><figure id="cwxm6-plot">
<span data-type="media" id="cwxm6" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh6m-plot">
<span data-type="media" id="cwh6m" data-alt="Image">
<img src="/resources/e17b95fa0386b0fb57cc237b98947a6340e87254/cwh6m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=6$ is $y[6]=1\cdot 0 +0\cdot 0+ -1\cdot 4 +0 \cdot 3+1\cdot 2 +0\cdot 1 + -1 \cdot 0 + 0 \cdot 0=-4+2=-2$.</p><p id="eip-134">And finally, $n=7$:
<figure id="ccex7" data-orient="vertical"><figcaption>$x[m]$ and $h[(n-m)_8]$ for $n=7$.</figcaption><figure id="cwxm7-plot">
<span data-type="media" id="cwxm7" data-alt="Image">
<img src="/resources/f5c9f1dff8b439607cab9cac086c7384a4037526/cwxm.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
<figure id="cwh7m-plot">
<span data-type="media" id="cwh7m" data-alt="Image">
<img src="/resources/d933a0b14846278dabcdffaeb2f210e9b2128f48/cwh7m.png" data-media-type="image/svg+xml" alt="Image" width="200">
</span>
</figure>
</figure>
The value of the inner product for $n=7$ is $y[7]=1\cdot 0 +0\cdot 0+ -1\cdot 0 +0 \cdot 4+1\cdot 3 +0\cdot 2 + -1 \cdot 1 + 0 \cdot 0=3-1=2$. Now that we have found the output for each $n$ from $0$ to $7$, we may plot $y[n]$
<figure id="y10"><figcaption>The final plot of $y[n]$.</figcaption><span data-type="media" id="y10-plot" data-alt="Image">
<img src="/resources/d3cc4f30fef5c002378fa9d021afdbeafbc1ff70/y10.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4a5a70739175e9f8a0f891b72867b18c85ca0a25/y10.eps" data-type="image"></span>
</span></figure></p></div></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ab94f5937-47f0-442a-9869-7ea77d0e6a68%401.html" data-type="page"><h1>Properties of Discrete-Time Convolution</h1><div data-type="document-title">Properties of Discrete-Time Convolution</div>
  <p id="delete_me">Recall the definition of discrete-time convolution. For infinite-length signals we have:
$y[n] ~=~ x[n]*h[n] ~=~ \sum_{m=-\infty}^{\infty} \: h[n-m] \, x[m] , \quad -\infty\lt n\lt\infty$
The formula for finite-length (with length $N$) signals is:
$y[n] ~=~ x[n] \circledast h[n] ~=~ \sum_{m=0}^{N-1} \: h[(n-m)_N] \, x[m] , \quad 0\leq n\leq N-1$
</p><p id="eip-548">As with other mathematical operations, convolution has several important properties which we will now explore. We will express these properties in terms of infinite-length convolution; unless otherwise noted, they apply also to finite-time convolution. </p><p id="eip-891"><span data-type="title">Convolution is Commutative</span>Like addition and multiplication, the operation of convolution is commutative. For two signals $x[n]$ and $h[n]$, $x[n]\ast h[n]=h[n]\ast x[n]$. This means that when convolution is calculated by hand, either of the two signals can be the one chosen to "flip and shift." It also means that the output of a system with impulse response $h[n]$ and input $x[n]$ is the same as a system with impulse response $x[n]$ and input $h[n]$. In block diagram form, these two diagrams are equivalent:
<figure id="commusys"><figcaption>CAPTION.</figcaption><span data-type="media" id="commusys-plot" data-alt="Image">
<img src="/resources/dee2e51d1650dfc2b8dda4314dfc30a7062501af/commusys.png" data-media-type="image/png" alt="Image">
</span></figure>
Though the output remains the same, appreciating the commutativity of convolution may give added insight to the convolution process. </p><p id="eip-389">Proving the commutativity of convolution is straightforward, simply apply a change of variable so the convolution sum equation:
$\begin{align*}
y[n] &amp;= x[n] * h[n]\\
&amp;=\sum_{m=-\infty}^{\infty} \: h[n-m] \, x[m]\\
&amp;\textrm{Let }v=n-m\rightarrow m = n-v\\
&amp;=\sum_{n-v=-\infty}^{n-v=\infty} \: h[v] \, x[n-v]\\
&amp;=\sum_{v=\infty+n}^{v=-\infty+n} \: h[v] \, x[n-v]\\
&amp;=\sum_{v=-\infty}^{v=\infty} \: h[v] \, x[n-v]\\
&amp;=\sum_{v=-\infty}^{v=\infty} \: x[n-v]\, h[v]  \\
&amp;=h[n]\ast x[n]
\end{align*}$</p><p id="eip-447"><span data-type="title">Convolution is Associative: Cascade of LTI Systems</span>Suppose a signal is processed by one LTI system, and the output is fed into another LTI system:
<figure id="cascsys1"><figcaption>CAPTION.</figcaption><span data-type="media" id="cascsys1-plot" data-alt="Image">
<img src="/resources/ad7640cb71d2b513deaa537e52a65b6c5c381fb1/cascsys1.png" data-media-type="image/png" alt="Image">
</span></figure>
Can these two "cascaded" systems be simplified so that they are represented by a single system? It turns out that they can, because convolution is not only commutative, it is also associative:
$(x\ast h_1)\ast h_2=x(\ast h_1\ast h_2)$
Therefore if a signal $x[n]$ is input into a system with impulse response $h_1[n]$, and the output of that is input a system with impulse response $h_2[n]$, the final output $y[n]$ is equivalent to that if $x[n]$ had been input into a signal system with an impulse response of $h_1\ast h_2$:
<figure id="cascsys"><figcaption>CAPTION.</figcaption><span data-type="media" id="cascsys-plot" data-alt="Image">
<img src="/resources/3d2dbd11bbb398db74df1794968e1f617f45b694/cascsys.png" data-media-type="image/png" alt="Image">
</span></figure>
Proving the associativity of convolution is a matter of careful rearrangement of the sums:
$\begin{align*}
(x * h_1)*h_2[n]&amp;=\sum_{m=-\infty}^\infty h_2[n-m]\left(\sum_{l=-\infty}^\infty h_1[m-l]x[l]\right)\\
&amp;=\sum_{l=-\infty}^\infty\sum_{m=-\infty}^\infty h_2[n-m] h_1[m-l]x[l]\\
&amp;=\sum_{l=-\infty}^\infty\left(\sum_{m=-\infty}^\infty h_2[n-m] h_1[m-l]\right)x[l]\\
&amp;=\sum_{l=-\infty}^\infty\left(h_1\ast h_2\right)[n-l]x[l]\\
&amp;= x * (h_1*h_2)[n]\\
\end{align*}$</p><p id="eip-149"><span data-type="title">Convolution is Distributive: Parallel LTI Systems</span>Suppose now that one input $x[n]$ is fed into two different LTI systems, and then the two outputs are then summed to form a new output:
<figure id="parsys1"><figcaption>CAPTION.</figcaption><span data-type="media" id="parsys1-plot" data-alt="Image">
<img src="/resources/6c59d6dd8a350455fca6ccff2b9681d73ed78cdf/parsys1.png" data-media-type="image/png" alt="Image">
</span></figure>
Just as with the cascaded systems, this parallel arrangement of LTI systems can also be expressed as a single system, because of the distributive property of convolution, that $x[n]*h_1[n]+x[n]*h_2[n]=x[n] * (h_1[n]+h_2[n])$:
<figure id="parsys"><figcaption>CAPTION.</figcaption><span data-type="media" id="parsys-plot" data-alt="Image">
<img src="/resources/b5752004b14ab11e2be8aba3436f127e37dc6464/parsys.png" data-media-type="image/png" alt="Image">
</span></figure>
The proof of this property is a consequence of the distributive property of addition:
$\begin{align*}
x[n]*h_1[n]+x[n]*h_2[n]&amp;=\sum_{m=-\infty}^\infty h_1[n-m]x[m]+\sum_{m=-\infty}^\infty h_2[n-m]x[m]\\
&amp;=\sum_{m=-\infty}^\infty\left(h_1[n-m]x[m]+h_2[n-m]x[m]\right)\\
&amp;=\sum_{m=-\infty}^\infty\left(h_1[n-m]+h_2[n-m]\right)x[m]\\
&amp;= x[n] * (h_1[n]+h_2[n])\\
\end{align*}$
</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3e922de4-2c0f-4f9b-9abd-6865fa1ee98c%401.html" data-type="page"><h1>Discrete-Time Infinite-Length and Finite-Length Convolution Equivalence</h1><div data-type="document-title">Discrete-Time Infinite-Length and Finite-Length Convolution Equivalence</div>
  <p id="delete_me"><span data-type="title">The Non-zero Support of the Convolution of Infinite-Length Signals</span>The convolution of two infinite-length signals yields another infinite length signal. However, just because a signal is infinite in length does not mean it has an infinite number of nonzero values. The delta function $\delta[n]$ is nonzero only at $n=0$, even though it is infinite in length (i.e., it is defined for all integers $n$).</p><p id="eip-999">For reasons that will be apparent shortly, it is worth considering the <span data-type="term">duration interval</span> of infinite length signals and their convolutions. Suppose an infinite-length signal is nonzero at all points less than $n=0$ and greater than $n=5$. Its duration interval goes from $n=0$ to $n=5$, and we say that the length of this duration is 6. It turns out that if signal $a[n]$ has a duration length of $D_a$, and signal $b[n]$ has a duration length of $D_b$, than the duration length of the convolution $a[n]\ast b[n]$ is $D_a+D_b-1$.<figure id="xps" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xp-plot"><figcaption>A signal $x[n]$ with duration length of $4$.</figcaption>
<span data-type="media" id="xp" data-alt="Image">
<img src="/resources/7a395b45011694edebd508f6d036d1f36dbfb8a8/xp.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b53ae44a3b753741c95c95500dc930ee64c70de6/xp.eps" data-type="image"></span></span>
</figure>
<figure id="xx-plot"><figcaption>When $x[n]$ is convolved with itself, the duration length of the result is $4+4-1=7$.</figcaption>
<span data-type="media" id="xx" data-alt="Image">
<img src="/resources/fdeef597dd34f17f20ea72cc727951f99483dab3/xx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4ec744ba424132e44c821252bd71153b787890c4/xx.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-551"><span data-type="title">"Equivalence" of Linear and Circular Convolution</span>The reason why we would like to consider the duration length of two infinite-length signals' (linear) convolution is that there are reasons we would like to produce the same result through circular convolution, i.e., the convolution of finite-length signals. Of course, the outputs of a linear convolution and circular convolution are not equivalent, strictly speaking; they produce two distinct categories of signals: the first one infinite-length, the second one finite-length. It is possible, however, to take the duration sections of infinite-length signals and modify them as finite-length signals so that their (circular) convolution is the same as the duration section of the infinite (linear) convolution of the infinite-length signals.</p><p id="eip-985"><span data-type="title">Zero-padding and Resulting Circular Convolutions</span>This is all perhaps best explained with illustrations. Consider the following infinite-length signal $x[n]$ (obviously only a portion of it is shown):
<figure id="xn"><span data-type="media" id="xn-plot" data-alt="Image">
<img src="/resources/7a395b45011694edebd508f6d036d1f36dbfb8a8/xp.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b53ae44a3b753741c95c95500dc930ee64c70de6/xp.eps" data-type="image"></span>
</span></figure>
This convolution of this signal with itself produces another infinite-length signal (again, only a portion of it is shown):
<figure id="xxout"><span data-type="media" id="xxout-plot" data-alt="Image">
<img src="/resources/fdeef597dd34f17f20ea72cc727951f99483dab3/xx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4ec744ba424132e44c821252bd71153b787890c4/xx.eps" data-type="image"></span>
</span></figure>
We will attempt to produce finite-length signal that is equal to the duration section of that convolution above. To do this, we will first try to circularly convolve two finite-length signals that are simply the duration section of $x[n]$. We will call  this signal $\tilde{x}[n]$:
<figure id="cc0" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xt0-plot"><figcaption>The finite-length signal $\tilde{x}[n]$ here is simply the duration section of the infinite-length signal $x[n]$ from above.</figcaption>
<span data-type="media" id="xt0" data-alt="Image">
<img src="/resources/971524f71eda6f2ca6670a7d546b12b1d5df6074/xt0.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d599c96417444495c6ebc09bfa5047a1ac7a7819/xt0.eps" data-type="image"></span></span>
</figure>
<figure id="xx0-plot"><figcaption>The circular convolution in this case is clearly not the same as the duration section of the convolution of $x[n]$ with itself.</figcaption>
<span data-type="media" id="xx0" data-alt="Image">
<img src="/resources/aa7733c24ef2ac526601537f0953d3bee201598a/xx0.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5296b043dd09a9fb851cec97dd4d95161d8b63bb/xx0.eps" data-type="image"></span></span>
</figure>
</figure>
It is apparent that simply circularly convolving the duration sections of $x[n]$ will not do. We consider that would happen if we add a 0 to the end of $\tilde{x}[n]$ and try again.
<figure id="cc1" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xt1-plot"><figcaption>...</figcaption>
<span data-type="media" id="xt1" data-alt="Image">
<img src="/resources/8b6d645db384a8256e335350d980cbcd31e145eb/xt1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f073b4e23bb1eed244a5fdeaf4b9a1d48da95155/xt1.eps" data-type="image"></span></span>
</figure>
<figure id="xx1-plot"><figcaption>The circular convolution in this case is clearly not the same as the nonzero support of the convolution of $x[n]$ with itself.</figcaption>
<span data-type="media" id="xx1" data-alt="Image">
<img src="/resources/edeba49e6295f1a6bb3e352b4cc3ff05f383e1ca/xx1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8ca92239a2267e7b3b64c8578d1decf3f4bc888d/xx1.eps" data-type="image"></span></span>
</figure>
</figure>
Well, this does seem to be a step in the right direction of having $\tilde{x}[n]\circledast\tilde{x}[n]$ equal the duration section of $x[n]\ast x[n]$. Let's add another $0$ to $\tilde{x}[n]$:
<figure id="cc2" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xt2-plot"><figcaption>...</figcaption>
<span data-type="media" id="xt2" data-alt="Image">
<img src="/resources/7c52a7e47ffc677cfe9c22f09c34179f39a4f62d/xt2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/6e5da7eb6e42e198541f5aecd3fe625939d2ebe9/xt2.eps" data-type="image"></span></span>
</figure>
<figure id="xx2-plot"><figcaption>...</figcaption>
<span data-type="media" id="xx2" data-alt="Image">
<img src="/resources/f4b7f06a5b0cd244b500908a3b2dcfd500a809d7/xx2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1bb37c9b4deeae9e1019a985fad30960610226d7/xx2.eps" data-type="image"></span></span>
</figure>
</figure>
We are getting close now, and perhaps you have already figured out how many zeros we need to ultimately add to achieve the desired circular convolution. Let's add another:
<figure id="cc3" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xt3-plot"><figcaption>..</figcaption>
<span data-type="media" id="xt3" data-alt="Image">
<img src="/resources/cd98b6f45d2be3ab38a3aad4a351c04045a0c171/xt3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f381628aaed93b9e025f208fbd9eab201ce8ed79/xt3.eps" data-type="image"></span></span>
</figure>
<figure id="xx3-plot"><figcaption>...</figcaption>
<span data-type="media" id="xx3" data-alt="Image">
<img src="/resources/e064965401c0f242a47aa2c8a660765f21cab587/xx3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1c0f85b484ab00bc06c6920136e7789a025164bb/xx3.eps" data-type="image"></span></span>
</figure>
</figure>
So it seems that three zeros added to make $\tilde{x}[n]$ did the trick, for the circular convolution $\tilde{x}[n]\cilrcledast\tilde{x}[n]$ is indeed equal to the duration of $x[n]\ast x[n]$. Let's see, though, what happens if we add yet another zero:
<figure id="cc4" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xt4-plot"><figcaption>The finite-length signal $\tilde{x}[n]$ here is simply the nonzero support section of the infinite-length signal $x[n]$ from above.</figcaption>
<span data-type="media" id="xt4" data-alt="Image">
<img src="/resources/9317e4daf0acb2127bac343ae976641bf131a65a/xt4.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4983d2e8b36ddf769237f132868e636c0f6a35e4/xt4.eps" data-type="image"></span></span>
</figure>
<figure id="xx4-plot"><figcaption>...</figcaption>
<span data-type="media" id="xx4" data-alt="Image">
<img src="/resources/dc366678fb6f3a4c5bcfc46171fb26c144c84731/xx4.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4aa3aed365381f91dd2671c63c138d1018c9acb2/xx4.eps" data-type="image"></span></span>
</figure>
</figure>
At this point, it seems that adding extra zeroes to $\tilde{x}[n]$ will also add zeroes to the circular convolution; there's no harm with that, but it isn't necessary for our goal of the circular convolution equaling the duration section of the linear convolution.</p><p id="eip-511"><span data-type="title">Requirements of Circular/Linear Convolution Equivalence</span>As we saw above, when we have two signals that produce some result via linear convolution, if we would like to achieve that result (or at least, the duration section of that result), we will need to cut out the relevant sections of the two signals and then zero-pad them before circularly convolving them.</p><p id="eip-872">Let's define the amount of zero-padding with a bit more rigor. Consider infinite-length discrete-time signals $a[n]$ and $b[n]$. Let $a_{nz}[n]$ and $b_{nz}[n]$ be finite-length signals which are the duration sections of $a[n]$ and $b[n]$, respectively; that is, they are the smallest contiguous sections of $a[n]$ and $b[n]$ outside of which ALL values in time BEFORE and AFTER those sections are zero (it is, of course, allowable that some values WITHIN the contiguous sections may be zero). Call the duration length of these duration sections $D_a$ and $D_b$, respectively. Now zero-pad the signals $a_{nz}[n]$ and $b_{nz}[n]$ (i.e., appended zeroes to the end of the signals) to bring the new length of these signals to be $D_a+D_b-1$, and call the new zero-padded signals $\tilde{a}_n$ and $\tilde{b}_n$. Then we have that $\tilde{a}_n\circledast\tilde{b}_n$ is equivalent to the duration section of $a[n]\ast b[n]$.</p><p id="eip-9">Put more simply, suppose that the duration length of $a[n]\ast b[n]$ is $D$ (which we have seen equals $D_a+D_b-1$). We must zero-pad the duration sections of $a[b]$ and $b[n]$ to each be a length of $D$ to have their circular convolution be equivalent to the duration section of the linear convolution.</p><p id="eip-591"><span data-type="title">Why this All Matters</span>There is a very good reason to go into this somewhat tedious task dealing with duration sections and their lengths and the relationships of linear and circular convolution. The reason has two parts. First: real-world input/output LTI systems implement LINEAR convolution. Second: there are tremendous computation complexity advantages to performing CIRCULAR convolution (which we will see later). So, if we can express the output of linear convolution through circular convolution, it means we can perform real-world system computations quickly.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A097fd6ed-9527-43cc-86cf-04df99fff518%401.html" data-type="page"><h1>Impulse Response and LTI System Causality</h1><div data-type="document-title">Impulse Response and LTI System Causality</div>
  <p id="delete_me">In addition to linearity and time-invariance, there are other significant classifications of discrete-time systems. One of these is causality. A system is <span data-type="term">causal</span> if its output, for any  $n$, depends only on inputs at or before time $n$. Causality is an important practical qualification on a system: it is not possible to implement a real-time system whose output depends on future values! While the qualification of causality is also relevant for non-LTI systems, it has a special relationship with LTI systems. Recall that for LTI systems, the impulse response can be used to find the system's output given some input (through convolution of the input and the impulse response). Likewise, for LTI systems, the impulse response can also tell us whether or not the system is causal. </p><p id="eip-167"><span data-type="title">The Impulse Response and Causality</span>There is a straightforward relationship between an LTI system's impulse response and whether or not the system is causal: An LTI system is causal if and only if its impulse response is 0 for all $n\lt 0$ (i.e., the impulse response is a causal signal). This follows naturally from the convolution sum. The system's output $y[n]=\sum_{m=-\infty}^\infty h[n-m]x[m]$ Note that if and only if $h[m]=0 \forall m\lt 0$, no $x[m]$ for $m\gt n$ contribute to the sum, which is to say that no future values of the input factor in to the output at time $n$.

Below is an example of the impulse response for a causal system. Note that it is $0$ for $n\lt 0$. This also corresponds to the system matrix being lower triangular:
<figure id="hras" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="hRA-plot"><figcaption>An example of an impulse response for a causal system. The impulse response is zero for $n\lt 0$.</figcaption>
<span data-type="media" id="hRA" data-alt="Image">
<img src="/resources/a73349219aec2ff129e295f2701d865ba1a4cd35/hRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/320c2db51e9569017741c113cc3433038918028c/hRA.eps" data-type="image"></span></span>
</figure>
<figure id="toeplitzRA-plot"><figcaption>A graphical representation of the system matrix of a causal system; its values are zero above the main diagonal.</figcaption>
<span data-type="media" id="toeplitzRA" data-alt="Image">
<img src="/resources/a054a9f7f3b715ddeda2103e17999aa6ea896d55/toeplitzRA.png" data-media-type="image/png" alt="Image">
</span>
</figure>
</figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Af301e8c6-42ac-490d-bea5-ed6b75d06c30%401.html" data-type="page"><h1>Impulse Response and LTI System Stability</h1><div data-type="document-title">Impulse Response and LTI System Stability</div>
  <p id="delete_me">It is of practical significance in the design of discrete-time systems that they be "well behaved," meaning that for any "well behaved" input, the system gives a "well behaved" output. Colloquially speaking, we do not want an innocuous input to result in the system "blowing up." The technical term for "well behaved" systems is that they are <span data-type="term">stable</span>. It is essential for many systems that they be stable, for the sake of safety and proper operation within wider systems. For example, with steering or braking or aircraft control systems, it could be catastrophic if a small input led to a wildly divergent output.</p><p id="eip-700">Consider the recursive average system $y[n]=x[n]+\alpha y[n-1]$, with an eminently reasonable and contained input of the step function $u[n]$. For values of $\alpha$ less than $1$, the system is "well behaved," but the output "blows up" for $\alpha\gt 1$:
<figure id="wellbehaved" data-orient="vertical"><figcaption>Outputs of a recursive average system when the input is a step function.</figcaption><figure id="unitstep-plot"><figcaption>The input to the recursive average system is a unit step function.</figcaption>
<span data-type="media" id="NAME" data-alt="Image">
<img src="/resources/4c7d8dbc9a7432b9572712b2f7e08e157c910bd3/unitstep.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/858381eed532523baf9112ac9f03f273e086b97f/unitstep.eps" data-type="image"></span></span>
</figure>
<figure id="stableOut1-plot"><figcaption>For $\alpha\lt 1$, the system's output is contained.</figcaption>
<span data-type="media" id="stableOut1" data-alt="Image">
<img src="/resources/791baa93d770f63edfff95ff2d56249570533332/stableOut1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/07d0c63da1397b5656a976ae24fa68f90b994c23/stableOut1.eps" data-type="image"></span></span>
</figure>
<figure id="unstableOut1-plot"><figcaption>For $\alpha\gt 1$ the system's output increases with time exponentially.</figcaption>
<span data-type="media" id="unstableOut1" data-alt="Image">
<img src="/resources/6c2c68ed1572d19f961cec290f7cb54cc4941112/unstableOut1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/0e360a26cbf125857ec1441a8ef85e23675e5f91/unstableOut1.eps" data-type="image"></span></span>
</figure>
</figure>
Now as with any desirable characteristic or quality, there are many ways to define stability. We could say a system is stable if its output never exceeds a particular value, or perhaps that the output's energy per some time period is capped. One of the most common ways to define stability is <span data-type="term">bounded-input bounded-output (BIBO) stability.</span> A system is said to be BIBO stable if, for any bounded input (meaning that the magnitude of the signal never exceeds some finite value), the output is also bounded (but not necessarily by the same value as the input). Mathematically, we can put it like this:</p><p id="eip-373">Consider a discrete-time system $H$ and arbitrary input signal $x[n]$ with $|x[n]|\leq M_1\in R~,~\forall n$. Let $y[n]=H\{x[n]\}$. $H$ is BIBO stable if there exists some $M_2\in R$ such that $|y[n]|\leq M_2 \forall n$.</p><p id="eip-598">BIBO stability is a guarantee, a stamp of approval on a discrete-time system, certifying that the output will always be capped by some value, so long as the input also is.</p><p id="eip-673"><span data-type="title">The Impulse Response and BIBO Stability in LTI Systems</span>For LTI systems, there is again a special relationship between the system and its impulse response. The system's impulse response can tell us two things regarding the system: first, if it is BIBO stable, and if so, the particular value that bounds the output.</p><p id="eip-910">Consider a discrete-time LTI system $H$ with impulse response $h[n]$, and arbitrary input signal $x[n]$ with $|x[n]|\leq M_1~,~\forall M$. $H$ is BIBO stable if and only if there exists some $M_2$ such that $\|h[n]\|_1=M_2$. Furthermore, this $M_2$ (if it exists) bounds the output of the system: $|H\{x[n]\}|\leq M_1 M_2$.</p><p id="eip-205"><span data-type="title">Proof</span>The proof of our "if and only if" statement has two parts. First, we must show that the existence of $\|h[n]\_1$ implies BIBO stabilty. Second, we must show that BIBO stability implies the existence of $\|h[n]\_1$.</p><p id="eip-785">$\begin{align*}
|y[n]|&amp;=|H\{x[n]\}|\\
&amp;=|\sum_{m=-\infty}^\infty h[n-m]x[m]|\\
&amp;=|\sum_{m=-\infty}^\infty x[n-m]h[m]|\\
&amp;\leq \sum_{m=-\infty}^\infty|x[n-m]||h[m]|\\
&amp;\leq \sum_{m=-\infty}^\infty M_1|h[m]|\\
&amp;\leq M_1\sum_{m=-\infty}^\infty |h[m]|\\
&amp;\leq M_1M_2
\end{align*}$
So, if the impulse response $|\h[n]\|_1$ for an LTI system exists, then the system is BIBO stable.</p><p id="eip-698">The other side of the proof is to show that if a system is BIBO stable, the norm $\|h[n]\|_1$ of its impulse response must exist. We will demonstrate this by proving the contrapositive: if $\|h[n]\|_1$ is unbounded, the system is not BIBO stable. It will require the use of a the function $\textrm{sgn}\{x[n]\}$, which outputs 1 when $x[n]$ is positive, -1 when it is negative, and 0 when it is 0.</p><p id="eip-969">Consider an arbitrary impulse response $h[n]$ that is not absolutely summable, i.e., $\|h[n]\|_1$ is unbounded. For the system to be BIBO stable, any bounded input must result in a bounded output. So let the input $x[n]=\textrm{sgn}\{h[-n]\}$. Clearly, $x[n]$ is bounded: $\|x[n]\|_\infty$. But
$\begin{align*} y[0]&amp;=\sum_{m=-\infty}^\infty h[0-m]x[m]\\ &amp;=\sum_{m=-\infty}^\infty h[0-m]\textrm{sgn}\{h[m]\}\\ &amp;= \sum_{m=-\infty}^\infty|h[m]|\\
&amp;=\|h[n]\|_1 \end{align*}$
So in this case $y[0]$ is unbounded, so the system is not BIBO stable.
</p><p id="eip-512"><span data-type="title">Examples of BIBO and non-BIBO Systems</span>Suppose an LTI system (remember, impulse response absolute summability being equivalent to BIBO stability only applies to LTI systems!) has the impulse response $h[n]=\frac{1}{n^2}u[n-1]$:
<figure id="hn2"><figcaption>Example of an impulse response of a BIBO system.</figcaption><span data-type="media" id="hn2-plot" data-alt="Image">
<img src="/resources/ff0cc20fee846c2ca910fbd7b00981857b25d75d/hn2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/404cecfded4a78226d3017ea53280e288b7b4c59/hn2.eps" data-type="image"></span>
</span></figure>
As the norm $\|h[n]\|_1$ of this impulse response exists ($\sum_{n=1}^\infty \left|\frac{1}{n^2}\right| = \frac{\pi^2}{6}$, the system is BIBO stable. But suppose $h[n]=\frac{1}{n}u[n-1]$:
<figure id="hn1"><figcaption>Example of an impulse response of a system that is not BIBO stable.</figcaption><span data-type="media" id="hn1-plot" data-alt="Image">
<img src="/resources/19cfbb5cbb6b8eb58d5f244c0a41cf11dfae1221/hn1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b7a4db2938510f1f180cb724bbfc57c7868a0ee9/hn1.eps" data-type="image"></span>
</span></figure>
For this system, the norm $\|h[n]\|_1$ does not exist, as $\sum_{n=1}^\infty \left|\frac{1}{n}\right|\rightarrow\infty$. So this system is not BIBO stable.</p><p id="eip-929">Because of this impulse response property, it is evident that all FIR systems are BIBO stable (for a finite sums of finite values is finite).</p><p id="eip-472"><span data-type="title">Conclusion</span>Once again we have seen the significance of the impulse response for LTI systems. They can be used to compute a system's output, and they can be used to determine the causality of a system. Here we have seen they can be used to determine the BIBO stability of a system, a characteristic of great importance. But note two things. First, the connection between the impulse response and stability (as with those other connections) applies only to LTI systems. For example, the system $H\{x[n]\}=\frac{1}{x[n]+1}-1$ is not stable (the output explodes as the input tends to -1), yet its impulse response is $\frac{-1}{2}\delta[n]$, which is absolutely summable. And second, just because a system is not BIBO stable, this does not necessarily mean that it is not useful, but rather that care must be given to what is input into the system.</p></div></li></ul></li><li><div data-type="page"><h1>Orthogonal Bases and the Discrete Fourier Transform</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Af746e0a1-8da1-43a6-b1c6-3f1497c7245c%401.html" data-type="page"><h1>Orthogonal Bases</h1><div data-type="document-title">Orthogonal Bases</div>
  <p id="delete_me">Recall from before (LINK), how we can understand discrete-time signals to be vectors in a vector space. There are some very useful reasons why we might want to express some signal in a vector space in terms of other signals in that space. To better understand how all of this works, and to give us some mathematical foundations for it, we will consider the concept of bases.</p><p id="eip-593"><span data-type="title">The Basis of a Vector Space</span>Suppose we have some vector space $V$, such as $R^N$ or $C^N$, i.e., real or complex valued finite-length (of length $N$) discrete time signals. We define a <span data-type="term">basis</span> for $V$ is a set of vectors $\{b_k\},b_k\in V$ which span $V$ and are linearly independent. By spanning $V$, we mean that any vector in $V$ can be expressed as a linear combination of one or more vectors in $\{ b_k \}_{k=0}^{N-1}$:
$\sum_{k=0}^{N-1} \alpha_k \, b_k ~=~ \alpha_0\,b_0 + \alpha_1\,b_1 + \cdots + \alpha_{N-1}\,b_{N-1} \quad \forall \, x\in V, \alpha_k\in C$
By the vectors in the set $\{b_k\}$ being linearly independent, we mean that no vector in that set can be expressed as a linear combination of any of the others. The number of these spanning and linearly independent vectors in the basis is the <span data-type="term">dimension</span> of the basis. Our example bases $R^N$ or $C^N$ are of dimension--you guessed it--$N$.</p><p id="eip-919"><span data-type="title">The Basis Matrix</span>For the sake of cleaner and simpler mathematical expression--as well as the ability to connect the concept of a basis with other linear algebra tools--we can create a matrix with basis vectors as columns. If the dimension of the basis is $N$, then this collection of basis vectors will be an $N\times N$ matrix, which we'll call $B$:
$\textbf{B} ~=~ \begin{bmatrix} b_0 | b_1 | \cdots | b_{N-1} \end{bmatrix}$
Recall we can express any vector in a vector space as a linear combination of the basis vectors. We can put these weights $\{\alpha_k\}$ into an $N\times 1$ column vector:
$a = \begin{bmatrix}\alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{N-1} \end{bmatrix}$
With the basis matrix $\textbf{B}$ and the weights vector $a$, we can express the linear combination using a simple matrix multiplication:
$x=\textbf{B}a$
So we see that we can use the basis matrix and weights vector to refer to any vector in the vector space, through the linear combination of the basis vectors (which we can express with a matrix multiplication.

Now, it is a natural question to ask, given some vector $x$ in the vector space, how do we find the weights $a$ that will produce the expression $x=\textbf{B}a$? If we would like to express $x$ as a linear combination of basis vectors, it is important that we know how to find those weights!

Thankfully, it is very straightforward. Simply multiply each side of the matrix multiplication equation by the inverse of the basis matrix:
$\begin{align*}
x&amp;=\textbf{B}a\\
\textbf{B}^{-1}x&amp;=\textbf{B}^{-1}\textbf{B}a\\
\textbf{B}^{-1}x&amp;=\textbf{I}a\\
\textbf{B}^{-1}x&amp;=a\\
\end{align*}$
The weight vector is simply the inverse of the basis matrix, times the vector $x$ (the one we want to express in terms of the basis matrix).</p><p id="eip-66"><span data-type="title">Orthogonal and Orthonormal: Special Bases</span>We've already seen that a basis is a special collection of vectors from some vector space: it is a collection that spans the space, and is mutually linearly independent. If we add another requirement or two, we end up with two important sub-classes of bases. Suppose the vectors in some basis are not only spanning and linearly independent (which of course they must be, by definition, to form a basis), but that they are also mutually orthogonal, that the inner product of any basis vector with any other basis vector is 0:
$\forall b_k\in\{ b_k \}_{k=0}^{N-1},\langle b_k, b_l \rangle ~=~ 0, \quad k\neq l$ 
If such is the case, than this basis is said to be an <span data-type="term">orthogonal basis</span>.</p><p id="eip-59">So an orthogonal basis is a particular kind of basis, one whose vectors are mutually orthogonal. Among orthogonal bases, there are some whose vectors have unit 2-norms:
$\langle b_k, b_l \rangle ~=~ 0, \quad k\neq l\\
\| b_k \|_2 ~=~ 1$
Bases with this additional property are known as <span data-type="term">orthonormal</span> bases.</p><p id="eip-108"><span data-type="title">Basis Matrix of an Orthonormal Basis</span>Like any other basis, the vectors of an orthonormal basis can be put together to form a basis matrix. Recall how we find the weights $a$ to express some vector $x$ in terms of the basis matrix $\textbf{B}$:
$a=\textbf{B}^{-1}x$.
The reason that orthonormal bases are so special is that, in contrast to other bases, their matrix inverses are extremely easy to find. For orthonormal basis matrices,
$\textbf{B}^{-1}=\textbf{B}^H$
The inverse of an orthonormal basis matrix is simply its Hermitian (conjugate) transpose! So for these bases,
$a=\textbf{B}^{H}x$.
In linear algebra, a matrix that has the property that its inverse is simply its Hermitian transpose is called a <span data-type="term">unitary matrix</span>. If the matrix is real-valued, it is called an <span data-type="term">orthogonal matrix</span> (a bit of unfortunate nomenclature, considering its columns are actually mutually ORTHONORMAL).</p><p id="eip-785"><span data-type="title">Orthonormal Basis Signal Representation</span>Putting this all together, we can see the two key aspects of signal representation with orthonormal bases. There is the <span data-type="term">synthesis</span> side, that we can build up any vector $x$ in the vector space through a linear combination of basis vectors. And there is the <span data-type="term">analysis</span> side, that we can find the proper weights of this linear combination by multiplying $x$ by the Hermitian transpose of the basis matrix.
--Synthesis: $x ~=~ \textbf{B}a ~=~  \sum_{k=0}^{N-1} \alpha_k \, b_k$
--Analysis: $a=\textbf{B}^H x~,\textrm{or,}~ \alpha_k ~=~\langle x, b_k$</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aafae700e-90dd-4c27-8ffa-b9def44c3abb%401.html" data-type="page"><h1>Eigenanalysis of LTI Systems (Finite-Length Signals)</h1><div data-type="document-title">Eigenanalysis of LTI Systems (Finite-Length Signals)</div>
  <p id="delete_me">In the study of discrete-time signals and systems, concepts from linear algebra often provide additional insight. When it comes to LTI systems, a certain area of linear algebra is particularly helpful: eigenanalysis.  </p><p id="eip-847"><span data-type="title">Eigenvectors and Eigenvalues</span>Given a square matrix (one that has the same number of rows as columns) $A$, a vector $v$ is an eigenvector with corresponding scaler eigenvalue $\lambda$ if:
$Av=\lambda v$
There is a geometric interpretation to this eigenanalysis of the matrix $A$. Multiplying a matrix by one of its eigenvectors produces simply a scaled version of that same eigenvector (scaled by a factor of $\lambda$), so a matrix multiplication of an eigenvector does not change its orientation, only its strength.</p><p id="eip-913">Consider this example in two dimensions, a square matrix $A$:
$A=\begin{bmatrix}3 &amp; 1 \\ 1 &amp; 3\end{bmatrix}$
and the vector $v$:
$v=\begin{bmatrix}1 \\-1\end{bmatrix}$
Note what happens when we multiply the matrix $A$ by the vector $v$:
$\begin{align*}
Av&amp;=\begin{bmatrix}3 &amp; 1 \\ 1 &amp; 3 \end{bmatrix}\begin{bmatrix}1 \\ -1\end{bmatrix}\\
&amp;=\begin{bmatrix}(3-1) \\ (1-3)\end{bmatrix}\\
&amp;=\begin{bmatrix}2 \\ -2\end{bmatrix}\\
&amp;=2\begin{bmatrix}1 \\ -1\end{bmatrix}\\
&amp;=2v
\end{align*}$
So running the vector $v$ through the matrix $A$ simply scales the vector by 2.</p><p id="eip-899"><span data-type="title">Eigendecomposition: Handling Multiple Eigenvectors and Eigenvalues</span>So we have seen what an eigvenvectors and eigenvalues are for square matrices. Now, an $N\times N$ matrix will have $N$ eigenvectors (not necessarily distinct), each with its own eigenvalue. We can put all of these vectors and values into their own matrices. Suppose the eigenvectors of the matrix are $\{v_m\}_{m=0}^{N-1}$ and the values are $\{\lambda_m\}_{m=0}^{N-1}$. Then we can organize all of them like this:
$V= \begin{bmatrix}v_0 | v_1 | \cdots | v_{N-1} \end{bmatrix}$
$\Lambda = \begin{bmatrix}\lambda_0 \\ &amp; \lambda_1 \\ &amp;&amp; \ddots \\ &amp;&amp;&amp; \lambda_{N-1} \end{bmatrix}
$
With those vectors and values collected like that, we can express the eigenvector/value property $Av=\lambda v$ for all of the eigenvectors and eigvenvalues of the matrix $A$ at once:
$AV=V\Lambda$</p><p id="eip-334"><span data-type="title">Diagonalization</span>Now, if the square matrix $V$ is invertible (which will be the case if the columns of $A$ are linearly independent, which of course will happen if the vectors happen to form a basis), then we can do some special things with it.</p><p id="eip-254">Recall the eigendecomposition relationship:
$AV=V\Lambda$
If $V$ is invertible, then we can multiply each side of the equation by $V^{-1}$:
$V^{-1}AV=V^{-1}V\Lambda=I\Lambda=\Lambda$
So $V^{-1}AV=\Lambda$. Because multiplying $A$ by $V$ on one side and $V^{-1}$ on the other produces a diagonal matrix, we say that the matrix $V$ <span data-type="term">diagonalizes</span> $A$. If we multiply the eigendecomposition matrix the other way, we will have that $A=V\Lambda V^{-1}$. One of the reasons (more of which we'll see later) why we consider this diagonalization of $A$ is that it is easier to matrix multiply with diagonal matrices than full ones.</p><p id="eip-906"><span data-type="title">LTI Systems and Eigenanalysis</span>Perhaps you may be wondering how all of this linear algebra relates to discrete-time systems. For LTI systems operating on finite-length discrete-time systems, the input output relationship is:
$y=Hx,$
where $H$ is a circulant matrix (each row being a circularly shifted version of the system impulse response $h$).</p><p id="eip-67">Let's see what the eigenvectors of $H$ are. These will be the finite-length signals that, when input into the system, emerge as outputs simply as scaled versions of themselves. In that sense, they are somehow fundamentally related to all LTI systems.</p><p id="eip-497">It so happens that, remarkably, any and all LTI systems for finite-length (length $N$) signals have the exact same set of eigenvectors! The eigenvectors for any LTI length-$N$ system are complex harmonic sinusoids:
$s_k[n] ~=~ \frac{e^{j \frac{2\pi}{N}kn}}{\sqrt{N}}  ~=~ \frac{1}{\sqrt{N}}\left( \cos\!\left(\frac{2\pi}{N}kn\right) + j \sin\!\left(\frac{2\pi}{N}kn\right) \right),
\qquad 0\leq n,k \leq N-1$
So, if we have an LTI system--any LTI system--then giving an $s_k$ as an input will result in the output being $\lambda_k s_k$, with the particular values of $\lambda_k$ of course being dependent on the system:
<figure id="Hinout"><figcaption>When a complex harmonic sinusoid is input into an LTI system, the output is a scaled version of the input. Here the real (cosine) and imaginary (sine) parts of the sinusoid are plotted as the input. Note how the system merely scales the inputs.</figcaption><span data-type="media" id="Hinout-plot" data-alt="Image">
<img src="/resources/70b4829d336f2713c074638274705d84194ed3e2/Hinout-sk.png" data-media-type="image/png" alt="Image" width="600">
</span></figure>
To prove this special property of LTI systems, we simply compute the circular convolution sum for an LTI system with arbitrary impulse response $h[n]$ and input of the form $\frac{e^{j \frac{2\pi}{N}kn}}{\sqrt{N}}$:
$\begin{align*}
s_k[n] \circledast h[n] &amp;=
\sum_{m=0}^{N-1} s_k[(n-m)_N]\,h[m]\\
&amp;= \sum_{m=0}^{N-1} \frac{e^{j \frac{2\pi}{N}k(n-m)_N}}{\sqrt N} \,h[m] \\
&amp;=\sum_{m=0}^{N-1} \frac{e^{j \frac{2\pi}{N}k(n-m)}}{\sqrt N} \,h[m]\\
&amp;=\sum_{m=0}^{N-1} \frac{e^{j \frac{2\pi}{N}kn}}{\sqrt N} e^{-j \frac{2\pi}{N}km} \, h[m] \\
&amp;=\left( \sum_{m=0}^{N-1}  e^{-j \frac{2\pi}{N}km} \,h[m] \right)\frac{e^{j \frac{2\pi}{N}kn}}{\sqrt N}\\
&amp;=~ \lambda_k \, s_k[n]~,~\lambda_k=\left( \sum_{m=0}^{N-1}  e^{-j \frac{2\pi}{N}km} \,h[m] \right)
\end{align*}$
This proof reveals how we are to find the eigenvalues ($\lambda_k$) that correspond to each harmonic sinusoid eigenvector, they are simply the inner products of the eigenvectors with the system's impulse response. Each value $\lambda_k$ is called the system's <span data-type="term">frequency response</span> at frequency $k$, because it indicates how the system scales inputs of that particular frequency. It is a significant enough characteristic of the system to warrant its own notation: $H[k]$.</p><p id="eip-23"><span data-type="title">Eigendecomposition of LTI Systems</span>As with matrices in general, we can apply an eigendecomposition on an LTI system matrix. For LTI finite-length systems, these matrices are circulant:
<figure id="yHxlti"><figcaption>CAPTION.</figcaption><span data-type="media" id="yHxlti-plot" data-alt="Image">
<img src="/resources/99ab15509220820270acaf73d4b56fea054e438a/yHxlti.png" data-media-type="image/png" alt="Image" width="300">
</span></figure>
We have seen that the eigenvectors of LTI systems are harmonic complex sinusoids. We can stack these up into a single matrix $S$, the entries of which are $S_{n,k}$, which is plotted below:
<figure id="dftmtx" data-orient="horizontal"><figcaption>Graphical representation of the real and imaginary parts of the discrete-time finite length LTI system eigenvector matrix $S$.</figcaption><figure id="dftcosMatrix-plot"><figcaption>The real part of the eigenvector matrix $S$: $\cos(\frac{2\pi}{N}kn)/\sqrt{N}$.</figcaption>
<span data-type="media" id="dftcosMatrix" data-alt="Image">
<img src="/resources/93838d34da80d0a2a381afbfb9c481cc6c2ef3da/dftcosMatrix.png" data-media-type="image/png" alt="Image" width="200">
</span>
</figure>
<figure id="dftsinMatrix-plot"><figcaption>The real part of the eigenvector matrix $S$: $\sin(\frac{2\pi}{N}kn)/\sqrt{N}$.</figcaption>
<span data-type="media" id="dftsinMatrix" data-alt="Image">
<img src="/resources/82c3a082855b50bab8b2b288969a11f5183d82a2/dftsinMatrix.png" data-media-type="image/png" alt="Image" width="200">
</span>
</figure>
</figure>
Likewise, we can plot the respective eigenvalues of the eigenvectors, which above we defined to be the values of the frequency response of the system:
$\Lambda = \begin{bmatrix}  \lambda_0 \\ &amp; \lambda_1 \\ &amp;&amp; \ddots \\ &amp;&amp;&amp; \lambda_{N-1} \end{bmatrix}
~=~ \begin{bmatrix} H_u[0] \\ &amp; H_u[1] \\ &amp;&amp; \ddots \\ &amp;&amp;&amp; H_u[N-1] \end{bmatrix}$
Putting the equation $y=Hx$ together with the decomposition $H=S\Lambda S^H$, we have:
<figure id="ySLSxlti"><figcaption>Eigendecomposition of discrete-time finite length LTI systems.</figcaption><span data-type="media" id="ySLSxlti-plot" data-alt="Image">
<img src="/resources/7ada255e8b7c7675a8c99998fa047fba93ae35de/ySLSxlti.png" data-media-type="image/png" alt="Image" width="500">
</span></figure>
We already know one way of understanding how LTI systems operate on signal inputs: they convolve them with the system's impulse response (represented in linear algebra form by the equation $y=Hx$, where $H$ is circulant). The eigendecomposition gives us another understanding. The matrix $S^H$ takes the input and extracts what would be the coefficients of the input's representation as a linear combination of harmonic sinusoids (it turns out this is called the discrete Fourier transform). Then, multiplication by the diagonal matrix $\Lambda$ modifies these coefficients in a way that is particular to the system $H$ (for ALL LTI systems have the same $S$ and $S^H$). Finally, multiplication with the matrix $S$ takes the modified coefficients and expresses them as a linear combination of harmonic sinusoids to give the output $y$ (it turns out that operation is called in inverse discrete Fourier transform).</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Abebd669a-e439-4025-9551-074f09762d53%401.html" data-type="page"><h1>The Discrete Fourier Transform</h1><div data-type="document-title">The Discrete Fourier Transform</div>
  <p id="delete_me"><span data-type="title">The Significance of Complex Harmonic Sinusoids</span>Recall that complex harmonic sinusoids have the form:
$s_k[n]=\frac{e^{j \frac{2\pi}{N}kn}}{\sqrt{N}}$
In the context of finite-length discrete-time signals and systems, complex harmonic sinusoids are special, for several reasons. First, the collection of them $\{s_k\}_{k=0}^{N-1}$ form a basis for $C^N$, meaning that any signal in $C^N$ can be represented as a linear combination of that set of $N$ vectors. Not only this, but they are an orthonormal basis, which means that the weights of the linear combination can be found simply through the inner product of the signal with the harmonic sinusoid in question. And finally, these signals $\{s_k\}_{k=0}^{N-1}$ are important because they are eigenvectors of finite-length discrete-time LTI systems. This means that whenever a complex harmonic sinusoid is given as an input to any LTI system, the output is simply a scaled version of that input.</p><p id="eip-927"><span data-type="title">The Discrete Fourier Transform</span>In the early 19th century, Jean Baptiste Joseph Fourier showed that any function (later mathematics would more rigorously qualify this statement) could be composed as a (possibly infinite) sum of harmonic sinusoids. This work resulted in what would be an entire branch of mathematics, Fourier analysis. Fourier analysis extends to many different kinds of signals, including discrete-time finite-length signals. For those, the analysis produces the <span data-type="term">discrete Fourier transform (DFT)</span>. For signals $x[n]\in C^N$, the normalized DFT and inverse DFT are:
$\begin{align*}
X[k] &amp;= \sum_{n=0}^{N-1} x[n]  \frac{e^{-j \frac{2\pi}{N}kn}}{\sqrt N}\\
x[n]&amp;=\sum_{k=0}^{N-1} X[k] \, \frac{e^{j \frac{2\pi}{N}kn}}{\sqrt N}
\end{align*}$
We see that taking the inner product of $x[n]$ with harmonic sinusoids of different frequencies $k$ (which is what the first sum represents) produces a series of frequency coefficients $X{k]$. That is the DFT. We can also say that it is the <span data-type="term">analysis</span> aspect of the Fourier transform, for it gives us a frequency analysis/breakdown of the signal $x[n]$. The frequency coefficients can be used to reconstruct $x[n]$ using the second sum, which is the inverse DFT. It is known as <span data-type="term">synthesis</span>, for it shows how $x[n]$ can be built up as a linear combination of harmonic complex sinusoids, with the coefficients $X[k]$ telling us how much of each are needed.

<figure id="xtf" data-orient="vertical"><figcaption>A discrete-time finite length signal and its DFT.</figcaption><figure id="x101-plot"><figcaption>A discrete-time finite length signal $x[n]$.</figcaption>
<span data-type="media" id="x101" data-alt="Image">
<img src="/resources/59b01153ba8ef8407a8899be394fb081f678838d/x101.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f69d40e414a4c6c4609cadd2bbaecde5587c3048/x101.eps" data-type="image"></span></span>
</figure>
<figure id="x100-plot"><figcaption>$|X[k]|$, the magnitude of the DFT of $x[n]$.</figcaption>
<span data-type="media" id="NAME" data-alt="Image">
<img src="/resources/01b19df093a8da13557b6fa38d343a3d24e65556/x100.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2dfc6c3f2ca616b60b191abd4e9d2fbfbb7ff0cc/x100.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-679">We can express the above signal analysis and synthesis in matrix notation. First we stack up the complex harmonic sinusoids $\{s_k\}_{k=0}^{N-1}$ as columns in a matrix $S$:
${\bf S} = \begin{bmatrix} s_0 | s_1 | \cdots | s_{N-1} \end{bmatrix}$
With these vectors in a matrix, then the signal $x$ is composed of the linear combination of the sinusoids (the synthesis operation), with the weights in a vector $X$, through the matrix multiplication:
$x=SX$ (synthesis; inverse DFT)
Finding the weights $X$, given $x$, requires the inverse of the basis matrix $S$. However, since $S$ is unitary, the inverse is simply the Hermitian transpose:
$X=S^H x$ (analysis; DFT)</p><p id="eip-731"><span data-type="title">Normalized and Un-normalized DFT</span>To this point, we have been working with the normalized expression of the DFT and its inverse:
$DFT: X[k] = \sum_{n=0}^{N-1} x[n]\,  \frac{e^{-j \frac{2\pi}{N}kn}}{\sqrt N} \\
Inverse DFT: x[n] = \sum_{k=0}^{N-1} X[k] \, \frac{e^{j \frac{2\pi}{N}kn}}{\sqrt N}$
This form of the DFT has a certain symmetric elegance to it, the only difference in the DFT and its inverse being the negation of the exponent in the complex harmonic sinusoid.</p><p id="eip-260">However, it is far more common to use an un-normalized definition of the DFT, one which puts a $\frac{1}{N}$ scaling factor on the inverse, rather than the normalized version splitting this with a $\frac{1}{\sqrt{N}}$ in each formula. In practice, the DFT is virtually always understood in terms of the following un-normalized definition:
$DFT: X[k] = \sum_{n=0}^{N-1} x[n]\,  e^{-j \frac{2\pi}{N}kn} \\
Inverse DFT: x[n] = \frac{1}{N} \sum_{k=0}^{N-1} X_u[k] \, e^{j \frac{2\pi}{N}kn}$</p><p id="eip-604"><span data-type="title">Interpretations</span>So what exactly does the DFT MEAN? There are a variety of interpretations for what the DFT does. We have already explained things in terms of ANALYSIS and SYNTHESIS. With the inverse DFT, We can build up any signal $x[n]$ as a weighted sum (also known as a linear combination) of complex harmonic sinusoids; this is known as synthesis, which is in accordance with the Merriam-Webster dictionary definition of synthesis as being "the composition or combination of parts or elements so as to form a whole" (http://www.merriam-webster.com/dictionary/synthesis). But how are we to know how MUCH of each sinusoid goes in to the synthesis? The DFT tells us the coefficient values, one for each of the $N$ sinousids. This is the analysis aspect of the DFT, for it "analyzes" the signal in terms of the frequencies in it, how much there is of every possible frequency within the signal. To use a cooking metaphor, the analysis tells us the amount of each ingredient that goes into the dish that is our signal, and the cooking process is the synthesis which takes the ingredients and creates the finished product.</p><p id="eip-736">There is another way of looking at the DFT and inverse DFT. We note that there is a one-to-one correspondence between any signal $x[n]$ and its DFT $X[k]$; a signal $x[n]$ has only one DFT, and any $X[k]$ has only one inverse. So $x[n]$ and $X[k]$ are really referring to the same one thing--some signal--but only in different ways. $x[n]$ describes the signal in terms of its value at every given time location $n$, while $X[k]$ describes the signal in terms of how much of each frequency contributes to it. So there is a time-domain description of the signal, which is $x[n]$, and a frequency-domain description of it as well, which is $X[k]$. We could think of the time-domain and frequency-domain like two different languages, like English and Spanish. For some kind of language idea or expression, it can be represented in either one language or another, and given the expression in one language, it can be translated to the other. Of course, this is not a perfect metaphor; spoken languages are not always one-to-one (and some words in one language do not have an adequate word in another language). What this interpretation does convey is that time and frequency are two different, and yet also equivalent, ways of expressing a signal entity.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A457d1580-1d5b-40aa-bdb9-01fca8b5d67c%401.html" data-type="page"><h1>Discrete Fourier Transform Properties</h1><div data-type="document-title">Discrete Fourier Transform Properties</div>
  <p id="delete_me">Recall, for discrete-time finite length signals, the definition of the DFT and the inverse DFT, both in its normalized form:
$(DFT)~~X[k] = \sum_{n=0}^{N-1} x[n]\,  \frac{e^{-j \frac{2\pi}{N}kn}}{\sqrt N} \\[2mm]
(Inverse DFT)~~x[n] = \sum_{k=0}^{N-1} X[k] \, \frac{e^{j \frac{2\pi}{N}kn}}{\sqrt N}$
and in its much more commonly used un-normalized form:
$(DFT)~~X[k] = \sum_{n=0}^{N-1} x[n]\, e^{-j \frac{2\pi}{N}kn} \\[2mm]
(Inverse DFT)~~x[n] = \frac{1}{N}\sum_{k=0}^{N-1} X[k] \, e^{j \frac{2\pi}{N}kn}$
A signal $x[n]$ and its DFT $X[k]$ (recall there is a one-to-one correspondence) are referred to as a DFT PAIR. The DFT has a variety of properties, which we will now consider.</p><p id="eip-545"><span data-type="title">The DFT and Its Inverse Are Periodic</span>The DFT is defined for finite-length (length $N$) signals; so, for $x[n]$, $n$ runs from $0$ to $N-1$, as does $k$. But let is us see what happens when we consider a value of $k$ outside this range in the DFT formula, say $X[k+lN]$, where $l$ is some nonzero integer:
$\begin{align*}
X[k+l N] &amp;=~ \sum_{n=0}^{N-1} x[n]\,e^{-j \frac{2\pi}{N}(k+l N)n}\\
&amp;=\sum_{n=0}^{N-1} x[n]\, e^{-j \frac{2\pi}{N}kn} e^{-j \frac{2\pi}{N}l N n}\\
&amp;=\sum_{n=0}^{N-1} x[n]\, e^{-j \frac{2\pi}{N}kn}\cdot 1\\
&amp;=\sum_{n=0}^{N-1} x[n]\, e^{-j \frac{2\pi}{N}kn}\\
&amp;= X[k]
\end{align*}$
As $X[k+lN]=X[k]$, the DFT is periodic:
<figure id="Xperiodic1"><figcaption>CAPTION.</figcaption><span data-type="media" id="Xperiodic1-plot" data-alt="Image">
<img src="/resources/a9094b40828fd1d5596587442acec736fd92f22e/Xperiodic1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b44615d73d396517acb774825361af7f9803f870/Xperiodic1.eps" data-type="image"></span>
</span></figure>
By the same token, the inverse DFT is also periodic:
$\begin{align*}
x[n+l N] &amp;=~ \frac{1}{N}\sum_{k=0}^{N-1} X[k]\,e^{j \frac{2\pi}{N}k(n+lN)}\\
&amp;=\sum_{k=0}^{N-1} X[k]\, e^{j \frac{2\pi}{N}kn} e^{j \frac{2\pi}{N}klN}\\
&amp;=\sum_{k=0}^{N-1} X[k]\, e^{j \frac{2\pi}{N}kn}\cdot 1\\
&amp;=\sum_{k=0}^{N-1} X[k]\, e^{j \frac{2\pi}{N}kn}\\
&amp;= x[n]
\end{align*}$
Again this is to be expected because the complex harmonic sinusoids of the inverse DFT sum are periodic. This also further illustrates the fact that any finite-length signals can be understood as a single period of a periodic signal.</p><p id="eip-512"><span data-type="title">DFT Frequency Ranges</span>A complex harmonic sinuoids $e^{j(\frac{2\pi}{N}k)n}$ has, by definition, a frequency of $\frac{2\pi}{N}k$, which may label as $\omega_k$. In the DFT of a signal of length $N$, the variable $k$ rangers from $0$ to $N-1$, which corresponds to frequencies from $0$ to (just about) $2\pi$:
<figure id="Xrange1"><figcaption>For an $N=16$ length signal, one way to express the range of frequencies is for $k$ to run from $0$ to $N-1$, which corresponds to frequencies between $0$ and $2\pi$.</figcaption><span data-type="media" id="Xrange1-plot" data-alt="Image">
<img src="/resources/4656d9baafc408fadffc1416406313a564582845/Xrange1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/94b69dc70359a02c4fef88ae75e3f0a54f84521f/Xrange1.eps" data-type="image"></span>
</span></figure>
However, as we saw above, since the DFT is periodic, $0$ to $N-1$ is not the only range of $k$ we may use to express the DFT. Since $X[k]=X[k-N]$, we may let $k$ run from $-\frac{N}{2}$ to $\frac{N}{2}-1$ (for even $N$, that is; for odd $N$ it would be $-\frac{N-1}{2}$ to $\frac{N-1}{2}$):
<figure id="Xrange2"><figcaption>For an $N=16$ length signal, another way to express the range of frequencies is for $k$ to run from $-\frac{N}{2}$ to $\frac{N}{2}-1$, which corresponds to frequencies between $-\pi$ and $\pi$.</figcaption><span data-type="media" id="Xrange2-plot" data-alt="Image">
<img src="/resources/ce10b6354354e8258a7be31361d2c0fb8066ad82/Xrange2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5d36bc2bf70237147d530e46755835dc5f510e48/Xrange2.eps" data-type="image"></span>
</span></figure></p><p id="eip-354"><span data-type="title">Shifts in Time and Frequency</span>Let $x[n]$ and $X[k]$ be a DFT pair (i.e., $X[k]$ is the DFT of $x[n]$). A circular shift in time on $x[n]$ will produce a phase shift on $X[k]$:
$x[(n-m)_N] ~\stackrel{\textrm{DFT}}{\longleftrightarrow}~ e^{-j\frac{2\pi}{N}km} X[k]$
To prove this relationship, we first note that for the circular shift $(n-m)_N$, there is some integer $l$ such that $(n-m)_N=n-m+lN$. We will use that fact for a change of variables in our proof:
$\begin{align*}
\textrm{DFT}\{x[(n-m)_N]\}&amp;=\sum_{n=0}^{N-1} x[(n-m)_N]\,  e^{-j \frac{2\pi}{N}kn}\\
&amp;\textrm{Let }r=(n-m)_N=n-m+lN\\
&amp;=\sum_{r=0}^{N-1} x[r]\,e^{-j \frac{2\pi}{N}k (r+m-lN)}\\
&amp;=\sum_{r=0}^{N-1} x[r]\,e^{-j \frac{2\pi}{N}kr}e^{-j \frac{2\pi}{N}m}e^{-j \frac{2\pi}{N}lN}\\
&amp;=e^{-j \frac{2\pi}{N}m}\sum_{r=0}^{N-1} x[r]\,e^{-j \frac{2\pi}{N}kr}\cdot 1\\
&amp;=e^{-j\frac{2\pi}{N}km} X[k] 
\end{align*}$

As you might expect from the symmetrical similarities between the DFT and inverse DFT, a comparable relationship exists in the other direction as well. A circular shift in the frequency domain of a signal results in the modulation of the signal in the time domain:
$e^{j\frac{2\pi}{N}l n} \, x[n] ~\stackrel{\textrm{DFT}}{\longleftrightarrow}~  X[(k-l)_N]$
The proof of this property follows the same approach as that of the first.</p><p id="eip-803"><span data-type="title">The DFT is Linear</span>As the DFT of a signal amounts to being a weighted sum, it follows naturally that the DFT operation is linear. So if we have two arbitrary signals $x_1[n]$ and $x_2[n]$ (with DFTs $X_1[k]$ and $X_2[k]$) and arbitrary constants $\alpha_1$ and $\alpha_2$, then the DFT of $\alpha_1 x_1[n]+\alpha_2 x_2[n]$ is simply $\alpha_1 X_1[k]+\alpha_2 X_2[k]$:
$x_1[n] ~\stackrel{\textrm{DFT}}{\longleftrightarrow}~ X_1[k], ~~
x_2[n] ~\stackrel{\textrm{DFT}}{\longleftrightarrow}~ X_2[k]\\
\alpha_1 x_1[n] + \alpha_2 x[2] ~\stackrel{\textrm{DFT}}{\longleftrightarrow}~ \alpha_1 X_1[k] + \alpha_2 X_2[k]$
The proof is straightforward:
$\begin{align*}
\textrm{DFT}\{\alpha_1 x_1[n] + \alpha_2 x_2[n]\}&amp;=\sum_{n=0}^{N-1} (\alpha_1 x_1[n] + \alpha_2 x[2])  e^{-j \frac{2\pi}{N}kn}\\
&amp;=(\alpha_1\sum_{n=0}^{N-1} x_1[n]e^{-j \frac{2\pi}{N}kn}) + (\alpha_2\sum_{n=0}^{N-1}x_2[n] e^{-j \frac{2\pi}{N}kn})\\
&amp;=\alpha_1 X_1[k] + \alpha_2 X_2[k]
\end{align*}$</p><p id="eip-624"><span data-type="title">The Convolution/Multiplication Time/Frequency Relationship</span>We now reach perhaps the most significant DFT property, the relationship between convolution and multiplication in time and frequency. Suppose we have 2 discrete-time finite length signals $x_1[n]$ and $x_2[n]$, whose DFTs are $X_1[k]$ and $X_2[k]$. Let $y[n]$ be the circular convolution $y[n]=x_1[n]\circledast x_2[n]$. Then the DFT of $y[n]$ is equivalent to the product of the DFTs of $x_1[n]$ and $x_2[n]$: $Y[k]=X_1[k]X_2[k]$. Or, to express this relationship in DFT pair notation, we have:
$x_1[n]\circledast x_2[n] \stackrel{\textrm{DFT}}{\longleftrightarrow} X_1[k]X_2[k]$
The proof of this relationship will use the $r=(n-m)_N=n-m+lN$ change of variable we used earlier.
$\begin{align*}
\textrm{DFT}\{x_1[n]\circledast x_2[n]\}&amp;=~ \sum_{n=0}^{N-1} \left(  \sum_{m=0}^{N-1} \: x_1[(n-m)_N] \, x_2[m]  \right) e^{-j\frac{2\pi}{N}k n} \\
&amp;= \sum_{m=0}^{N-1} x_2[m] \left( \sum_{n=0}^{N-1} x_1[(n-m)_N] e^{-j\frac{2\pi}{N}k n} \right)\\
&amp;=\sum_{m=0}^{N-1} x_2[m] \left( \sum_{r=0}^{N-1} x_1[r] e^{-j\frac{2\pi}{N}k (r+m+lN)} \right) \\
&amp;=\sum_{m=0}^{N-1} x_2[m] \left( \sum_{r=0}^{N-1} x_1[r] e^{-j\frac{2\pi}{N}kr}e^{-j\frac{2\pi}{N}k m}e^{-j\frac{2\pi}{N}klN} \right) \\
&amp;=  \left( \sum_{m=0}^{N-1} x_2[m] e^{-j\frac{2\pi}{N}km} \right) \left( \sum_{r=0}^{N-1} x_1[r] e^{-j\frac{2\pi}{N}kr} \right)\\
&amp;=X_1[k]X_2[k]
\end{align*}$
So we see that convolution in two signals' time corresponds to their multiplication in frequency. It is certainly an interesting relationship, but it is also very practical: $N$ multiplications in the frequency domain is obviously much than the $N^2$ multiplications required to compute for an $N$-length signal. Of course, this requires first that the DFT of the two signals be computed, but we will see there are ways to compute that efficiently. What it all means is that a system's output (which is found via convolution) can be computed efficiently by way of multiplication in the frequency domain.</p><p id="eip-816"><span data-type="title">DFT Symmetry Properties</span>As the DFT is a weighted sum of complex signals (complex harmonic sinusoids), it follows that the DFT of signals is, in general, complex-valued.

<figure id="xcri" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="x105-plot"><figcaption>A real-valued finite-length signal $x[n]$.</figcaption>
<span data-type="media" id="x105" data-alt="Image">
<img src="/resources/c7a16cabfce7efb16af426ee73b02be4a0960002/x105.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a894413b0109cd5b171e42223e83582560bbade4/x105.eps" data-type="image"></span></span>
</figure>
<figure id="XR-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="XR" data-alt="Image">
<img src="/resources/afa2811e5985fb57bd4f4c9b76d79500a16f07f9/XR.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/03776a3379ba879b0bc4ca3e5b69ffaa6e28b10c/XR.eps" data-type="image"></span></span>
</figure>
<figure id="XI-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="XI" data-alt="Image">
<img src="/resources/5e0af7ff1015ecaeeabcb339f84d53bb11fd2b13/XI.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/e82681710c08fb6bdb5b63ca8ba6bb7f679050e4/XI.eps" data-type="image"></span></span>
</figure>
</figure>
Perhaps you have noticed, in the figures above, a certain symmetry for the signal's DFT. For the real part of the DFT, you see that $Re[X[k]]=Re[X[N-k]]$, and that  $Im[X[k]]=-Im[X[N-k]]$. Recalling that the DFT is periodic, if we consider frequencies from $-\frac{N}{2}$ to $\frac{N}{2}$, then we have $Re[X[k]]=Re[X[-k]]$, and that  $Im[X[k]]=-Im[X[-k]]$. Or, in words, the real part of the DFT is even, and the imaginary part is odd.
<figure id="xcris" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="x105s-plot"><figcaption>A real-valued finite-length signal $x[n]$.</figcaption>
<span data-type="media" id="x105s" data-alt="Image">
<img src="/resources/c7a16cabfce7efb16af426ee73b02be4a0960002/x105.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a894413b0109cd5b171e42223e83582560bbade4/x105.eps" data-type="image"></span></span>
</figure>
<figure id="XRS-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="XRS" data-alt="Image">
<img src="/resources/0aa88dc6dddc8c3ce431d72824c7dce0ed477b8d/XRS.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/96bcb710562f4196ee5d916a918666e88a3965d2/XRS.eps" data-type="image"></span></span>
</figure>
<figure id="XIS-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="XIS" data-alt="Image">
<img src="/resources/414bbbcfe03336e4a4da5060b8e636577d1c4e6b/XIS.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/57fb0dc831c1cdd0b34293d951a02342cbd1df45/XIS.eps" data-type="image"></span></span>
</figure>
</figure>
</p><p id="eip-597">This symmetry was not a coincidence for the DFT of that particular $x[n]$; because the complex harmonic sinusoid signals that make up the DFT sum formula are conjugate symmetric (the real part is even, the imaginary part is odd), the DFTs of all purely real signals will also be conjugate symmetric. The converse is true for purely imaginary signals; their DFTs will be symmetric such that the real part is odd and the imaginary part is even. If, in addition to being purely real or purely imaginary, the signal itself is also even or odd, that will further limit the characteristics of its DFT. All of these DFT symmetry properties are listed on the table below:
DFT SYMMETRY TABLE
</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A34418b41-f66e-425c-bec1-5d2289051e9f%401.html" data-type="page"><h1>The Fast Fourier Transform</h1><div data-type="document-title">The Fast Fourier Transform</div>
  <p id="delete_me"><span data-type="title">Tasks and Algorithms</span>The first thing to know about the <span data-type="term">fast Fourier transform</span>, or <span data-type="term">FFT</span>, is that it actually is not a unique type of mathematical transform at all. The FFT is simply a METHOD of computing a DFT. In that way, it is similar to the task of sorting a group of numbers from smallest to largest. No matter how you might go about the task of sorting the numbers, there is only one correctly sorted answer. Some approaches to sorting, however, may be quicker than others. For example, you could put the numbers in one list, and then progressively traverse through the list, comparing two adjacent items at a time and swapping them if necessary; each time you traverse the list, you will have put the next largest number in its place. This approach is usually called a "Bubble Sort." Another approach is to split the list of numbers into two sets, then divide each set in two, and divide those sets, and so on, until each little set has two (or fewer) numbers. You will then sort each set of two numbers (easy!), and then progressively combine these sorted sets together (also easy!) until a single sorted set remains. This is called a "Merge Sort," and is known as a "divide and conquer" algorithm.</p><p id="eip-437">Now, these two sorting methods achieve the same results, but the Merge Sort typically does so in far fewer steps (although the Bubble Sort will process an already or nearly sorted list quicker). If there are $N$ items in a list, Merge Sort requires about $N\log_2 N$ steps, while the Bubble Sort requires about $N^2$. Even with lists as small as $32$ items, that makes a big difference, the difference between $160$ and $1024$. So it is with the discrete Fourier transform. The DFT is a simple formula:
$X[k]=\sum_limits_{n=0}^{N-1}x[n]e^{-j\frac{2\pi}{N}kn}$
It would be possible to calculate this as a sum of $N$ multplications (going from $n=0$ to $n=N-1$), doing that $N$ times (for each $k$, again from $0$ to $N-1$). That's about $N^2$ multiplications and additions. But, there are many different ways one could calculate the DFT, just as there are many ways to sort a list. As with Merge Sort, there is a divide-and-conquer approach to finding the DFT which does so with far fewer computational steps, only about $N\log_2 N$.</p><p id="eip-118"><span data-type="title">The FFT: A Divide and Conquer Algorithm for Finding a DFT</span>The intuition behind the FFT algorithm is the same as that of the Merge Sort. The Merge Sort operates on the idea that it is easy to take two sorted lists and combine them into one large sorted list: you simply compare the beginning of each list, pick off the smallest of the two and place it in a new list, and then keep on doing that until the two lists are empty.</p><p id="eip-747">Something similar happens with the DFT. Suppose you have some signal $x[n]$, which you can split into two smaller signals terms of its even and odd indices, $x[2n]$ and $x[2n+1]$. Suppose now that you already know the DFTs of these two smaller signals, and call them $E[k]$ and $O[k]$. Then it is easy to find the DFT of the whole signal, $X[k]$, from these smaller ones:
$X[k]=E[k]+e^{j\frac{2\pi}{N}k}O[k]$
A proof of this follows. For simplification of notation, we let $W_N=e^{-j\frac{2\pi}{N}}$:
$\begin{align*}
X[k] &amp;= \sum_{n=0}^{N-1} x[n]\, W_N^{kn}\\
&amp;=\sum_{n=0}^{N/2-1} x[2n]\, W_N^{k (2n)} ~+~
\sum_{n=0}^{N/2-1} x[2n+1]\, W_N^{k(2n+1)}\\
&amp;=\sum_{n=0}^{N/2-1} x[2n]\, W_N^{2kn} ~+~ W_N^k \sum_{n=0}^{N/2-1} x[2n+1]\, W_N^{2kn} \\
&amp;=\sum_{n=0}^{N/2-1} x[2n]\, W_{N/2}^{kn} ~+~ W_N^k \sum_{n=0}^{N/2-1} x[2n+1]\, W_{N/2}^{kn} \\
&amp;=E[k] ~+~ W_N^k\, O[k]
\end{align*}$
You will note that the $k$ in $X[k]$ runs from $0$ to $N-1$, but that the even and odd DFTs are only of length $N/2$. It seems a little odd, then to consider the values of these DFTs for $k$ greater than $N/2$ (as is necessary in the formula). But there is no problem with this, since DFTs are periodic; as those DFTs are length $N/2$, we have $E[N/2+k]=E[k]$ for those values between $N/2$ and $N-1$.
So, we have that the DFT of an $N$-length signal is simply the weigthed sum of 2 $N/2$-length DFTs. And if we, somehow, are given the DFT of those even/odd sub-signals, there are only $N$ additions and multiplications involved in finding the DFT of the whole signal, which is of course much better than the $N^2$ needed to find the DFT by straightforward calculation of the DFT sum.</p><p id="eip-299">Of course, this requires that we somehow had the DFTs of the sub-divided signals! How do we find those? Well, again we will divide those sub-signals by even/odd indices, and then do that again, and again, until we have lots of length-2 sub-signals. The DFT of a length-2 signal is very easy to compute. If we call such a signal $t[n]$, then $T[0]=t[0]+t[1]$ and $T[1]=t[0]-t[1]$ (this is easy to verify for yourself).</p><p id="eip-116">With the DFTs of our many length-2 signals in hand, we use the even/odd sum formula to combine them find the DFT of the length-4 signals, and then length-8, and so on until we have the DFT of the whole signal. This is known as a decimation-in-time FFT, and while it assumes a signal length that is a power of 2, there are ways to apply the approach to signals of other lengths. The figure below illustrates the process of splitting up the DFT into smaller and smaller half-size DFTs:
<figure id="dfts" data-orient="vertical"><figcaption>The FFT is a "divide and conquer" algorithmic approach to finding a DFT. An N-length DFT can be found as the weighted sum of two N/2-length DFTs, and each of those are the sum of two N/4-length DFTs, and so on.</figcaption><figure id="fft8-plot"><figcaption>The DFT of a length-8 signal can be split into the the weighted sum of two length-4 DFTs, namely, the DFTs of the even and odd indices.</figcaption>
<span data-type="media" id="fft8" data-alt="Image">
<img src="/resources/76019fde329d271ac23e712cd7afb85824a59c6e/fft8.png" data-media-type="image/png" alt="Image" width="300">
</span>
</figure>
<figure id="fft4-plot"><figcaption>Each length-4 DFT is split into the weighted sum of two length-2 DFTs.</figcaption>
<span data-type="media" id="fft4" data-alt="Image">
<img src="/resources/46d06970d54c226f23bdb4920bce553bc26c05f8/fft4.png" data-media-type="image/png" alt="Image" width="300">
</span>
</figure><figure id="fft2-plot"><figcaption>The length-2 DFTs are found, and from there the rest of the overall DFT can be computed according to the half-size weighted sums.</figcaption>
<span data-type="media" id="fft2" data-alt="Image">
<img src="/resources/66cdb6917c2ceb7efb6fdd68b074c5ec84b339a2/fft2.png" data-media-type="image/png" alt="Image" width="300">
</span>
</figure>
<figure id="butterfly-plot"><figcaption>The 2-length DFTs are simple to find, just an addition and a subtraction. When those operations are represented graphically, they are said to have a "butterfly" structure.</figcaption>
<span data-type="media" id="butterfly" data-alt="Image">
<img src="/resources/a9554573ecb1ef9ce20f7bce7d1c6f817da6e9b2/butterfly.png" data-media-type="image/png" alt="Image" width="300">
</span>
</figure>
</figure></p><p id="eip-582"><span data-type="title">Computational Savings of the FFT</span>Recall that a straightforward sum and multiplication computation of the DFT requires about $N^2$ operations; to be precise, it is $N_2$ multiplications and $N^2-N$ additions. The FFT approach requires about $N\log N$ operations: $N\log_2 N -N$ multiplications and $N\log_2 N$ additions. For small signal sizes, the difference between $N^2$ and $N\log_2 N$ is not that significant. But as $N$ gets even moderately large (say, $N=32$ and up), the difference becomes larger and larger. By the time we consider typically signal sizes in signal processing, in which $N$ is in the millions, $N^2$ complex operations are prohibitive in terms of time and memory costs, while $N\log N$ is very manageable.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3c0a4ffb-e056-4637-94eb-6e1da70c808b%401.html" data-type="page"><h1>Fast Convolution with the FFT</h1><div data-type="document-title">Fast Convolution with the FFT</div>
  <p id="delete_me">We have seen, previously, these four important facts:
--the output of infinite-length LTI systems can be found via linear convolution
--linear convolution can be found through circular convolution, through zero-padding
--there is an equivalence between circular convolution in the signals' time domain and multiplication in their frequency domain
--the FFT is an algorithm that can quickly compute a DFT
Perhaps at the time it may have seemed that these findings were unrelated to each other. But as we now string them together, you can see the incredibly significant consequence: the output of LTI systems can be computed very efficiently. This truth has supported the incredible advances in signal processing over the past fifty years. </p><p id="eip-668"><span data-type="title">Putting It All Together...</span>Suppose we have an LTI system with an impulse response of $h[n]$ and an input of $x[n]$. We would like to find the output, $y[n]$. We could find this output via linear (infinite length) convolution. If the length of the impulse response is $N_h$ and that of the input signal is $N_x$, then about $N_h N_x$ operations would be required to compute this convolution. It would also be possible to compute the output through circular convolution, by zero-padding each signal to be of length $N_h+N_x-1$, and then circularly convolving the zero-padded signals. This would not save any computational steps, but it is a significant insight because the circular convolution could be performed by using DFTs: simply take the DFT of each zero-padded signal, multiply the two DFTs together, then take the inverse DFT of the result. Again, this does not at first seem to save any computational steps, and in fact seems to add even more, except that we have seen that the FFT is able to perform DFTs in about $N\log N$ operations. What that means is that if we zero-pad $x[n]$ and $h[n]$, then take the DFT of each (using the FFT algorithm), multiply these two together, then take the inverse DFT of the result, we can find the system output in about $2(N_h+N_x-1)\log_2 (N_h+N_x-1)$ operations. As signal lengths increase, this can end up being huge computational savings over the $N_h N_x$ operations it would take to find the output through a convolution sum. Below is a graphical depiction of the steps to quickly find the output, with the help of the FFT:
<figure id="fastConvo"><figcaption>Through zero-padding and forward and inverse DFTs, using the FFT algorithm, an LTI system's output can be found efficiently.</figcaption><span data-type="media" id="fastConvo-plot" data-alt="Image">
<img src="/resources/e3a3c4dd391de7ee16ef4934810400ffb38d5b2d/fastConvo.png" data-media-type="image/png" alt="Image" width="500">
</span></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Aa36f93aa-83fe-4fba-bed1-e18065409138%401.html" data-type="page"><h1>Other Orthogonal Bases</h1><div data-type="document-title">Other Orthogonal Bases</div>
  <p id="delete_me">We have seen that the discrete Fourier transform (DFT) is an example of an orthogonal basis for finite length discrete-time signals. As a basis, a linear combination of its basis elements $\{b_k\}$ can represent any signal $x$:
$x=\sum_{k=0}^{N-1}\alpha_k b_k=Ba$
Furthermore, as an orthogonal basis, its basis elements are mutually orthogonal, meaning that the weights needed to build up a particular signal $x$ can be found with a weighted inner product of $x$ and the basis element in question. If all of the basis elements are unit norm, then the basis is orthonormal, and a simple inner product will find the weights:
$\alpha_k=\langle x, b_k\rangle~,~a=B^H x$ 
The DFT is a particularly useful basis, for its basis vectors $e^{j\frac{2\pi}{N}kn}$ are the eigenfunctions of all LTI systems. As such, they are simply scaled when passing through a system. But as useful as the DFT is, there are other significant orthogonal bases to consider, as well.  </p><p id="eip-600"><span data-type="title">The Discrete Cosine Transform</span>Since the DFT uses complex harmonic sinusoids to analyze and synthesize signals, the DFT coefficients of real signals are almost always complex-valued (unless the signal is even, in which case the coefficients are purely real). This normally poses no problem, but there are some circumstances in which real coefficients are preferable. One example is when signal compression is desired. In such applications, a signal is transformed and only a few of the largest transform coefficients are kept. Having to store both the real and the imaginary parts of the coefficients would mean twice the storage.

It is for this reason that the discrete cosine transform (DCT) is used in signal compression applications (such as the JPEG image compression standard). The DCT is very similar to the DFT, as it also uses sinusoids as basis elements, except that its basis elements are purely real-valued. There are several varieties of the DCT, but the most popular is the DCT-II, which has the following basis elements:
$d_k[n] ~=~ \cos\!\left[ \frac{\pi}{N} \left(n+\frac{1}{2}\right) k \right], \quad 0\leq n,k \leq N-1$
<figure id="dct3"><figcaption>CAPTION.</figcaption><span data-type="media" id="dct3-plot" data-alt="Image">
<img src="/resources/59df54ded6fedb23bfcc6a85ae5b40fdc76a1e3a/dct3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a4fa8a59f74d2e6d96dc1834e39af90f2f2508b5/dct3.eps" data-type="image"></span>
</span></figure>
Unlike the DFT basis functions, the DCT functions are completely real-valued.
<figure id="basispics" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="cosMatrix-plot"><figcaption>The real part of the DFT functions.</figcaption>
<span data-type="media" id="cosMatrix" data-alt="Image">
<img src="/resources/69153040159707598839ff564d70f224a024f32b/cosMatrix.png" data-media-type="image/png" alt="Image" width="300">
</span>
</figure>
<figure id="sinMatrix-plot"><figcaption>The imaginary part of the DFT functions.</figcaption>
<span data-type="media" id="sinMatrix" data-alt="Image">
<img src="/resources/430954c3aa896313b0ae74297adfa90a3c8ee476/sinMatrix.png" data-media-type="image/png" alt="Image" width="300"></span>
</figure>
<figure id="dctMatrix-plot"><figcaption>The DCT functions are real-valued.</figcaption>
<span data-type="media" id="dctMatrix" data-alt="Image">
<img src="/resources/a5d4302ac9ecb237d22d1e05e5c43a2271af588c/dctMatrix.png" data-media-type="image/png" alt="Image" width="300"></span>
</figure>
</figure>
Since the DCT basis functions are real valued, the DCT transform coefficients for a real signal are also real valued. As a result, there are cases in which signals can be represented reasonably well when reconstructed with just a handful of DCT coefficients, compared to the same number of DFT coefficients.
<figure id="dctcomp" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="test1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="test1" data-alt="Image">
<img src="/resources/7d51577f67437884e32be90c79fe219cee869cfd/test1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9bdd78144a900ee39381f1236ea20c8a2623eeee/test1.eps" data-type="image"></span></span>
</figure>
<figure id="rffttest1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="rffttest1" data-alt="Image">
<img src="/resources/1f30dcbf034837f9b22b1fad227e3857ca66f7a8/rffttest1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7cf14105a998525482cb48ffc4b5c79a1fd9c932/rffttest1.eps" data-type="image"></span></span>
</figure>
<figure id="iffttest1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="iffttest1" data-alt="Image">
<img src="/resources/1fc6966e9fbf8d19fa83fcb00b7c7317e299dd80/iffttest1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/bee96bc735b6fdb9a873b0afa3ca4fe3769c6f11/iffttest1.eps" data-type="image"></span></span>
</figure>
<figure id="dcttest1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="dcttest1" data-alt="Image">
<img src="/resources/39064ac2dd156bbf5f48274e4e473ba4b0fc2f73/dcttest1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b2372b73f5eec21032fdc85bf8cf291336951626/dcttest1.eps" data-type="image"></span></span>
</figure>
</figure>
</p><p id="eip-866"><span data-type="title">Wavelet Transforms</span>We have seen that with the DFT (and DCT as well), signals can be represented in either the time domain (e.g., as $x[n]$), or in the frequency domain through their DFT (e.g., $X[k]$). Both representations are equivalent, of course, for one can always be derived from the other. But sometimes one way of expressing a class of signals is desired, such as when as signal can be expressed with just a few values. For example, the signal $x[n]=\delta[n]$ can be expressed with just a single nonzero value in the time domain, but it does not have any nonzero values in the frequency domain. Thus we say the signal is localized in time. Likewise, the signal $x[n]=e^{j\frac{2\pi}{16}n}$  has only a single nonzero DFT coefficient, while all of its values in the time domain are nonzero. That is a kind of signal we say is localized in frequency. If a signal is localized in time or in frequency, we can efficiently store and process its signal values, by storing/operating on the signal in the best of those two domains.</p><p id="eip-176">But some signals are not localized very well in time or in frequency. If a signal has frequency components that show up and then are absent at different times, then neither the time or the frequency domain does a good job of efficiently representing such as signal. An example of such a signal would be a musical melody; at any given time it may be expressed as being a single (predominant) frequency, but these frequencies (i.e., the notes) obviously change quite a bit over time (the work of Philip Glass notwithstanding).</p><p id="eip-526">There are many transforms that are designed specifically for analysis and concise representation of these signals that have localized frequency components, two of which we will consider. The first is the Haar Wavelet, which is one of a class of wavelet transforms. The basis functions of the Haar wavelet transform, like other wavelets, are localized in both time and frequency, as illustrated in the following figure:
<figure id="wavelets" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="wavelet1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="wavelet1" data-alt="Image">
<img src="/resources/15787d8fb8e9c3278ce0b9665cba74310d6db07d/wavelet1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d506f72f7ac3b287faee507221e150fd64c21605/wavelet1.eps" data-type="image"></span></span>
</figure>
<figure id="wavelet2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="wavelet2" data-alt="Image">
<img src="/resources/68c945f8d7f0ad618109efbfd8688f88b3b36796/wavelet2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/9de74f7c867e3c7bc51dc1bffb5585b34219438b/wavelet2.eps" data-type="image"></span></span>
</figure>
<figure id="wavelet3-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="wavelet3" data-alt="Image">
<img src="/resources/9a91ed4ee7e053416fa1cfa4e400b2928d3b1fc8/wavelet3.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/290f1cd0e2a5c22bf92d8a16dc9369f87886b317/wavelet3.eps" data-type="image"></span></span>
</figure>
</figure>
Unlike the DFT or DCT, wavelet basis functions are nonzero for most of their duration in time. But unlike delta functions, they have particular frequencies for their duration. So in a sense they have the benefits of both domains, combined.</p><p id="eip-579"><span data-type="title">The Short Time Fourier Transform</span>Another transform that considers signals which have time-localized frequency components is the short-time Fourier transform. The STFT is kind of like a DFT, except that it first splits a long signal into many smaller parts, and then takes a DFT of each of those parts. This "splitting" is done by multiplying the signal (of length $N$) by a moving window (of length $N_w$), progressively taking the DFT of the product:
$X[m,k]=\sum\limits_{n=0}^{N-1}x[n]w[n-m]e^{j\frac{2\pi}{N_w}k(n-m)$
The STFT of a signal is therefore usually represented as a two-dimensional image. Each column ($m$) of the image corresponds to a time window of a signal, while the rows ($k$) indicate how much of each frequency is present at a given time. It is therefore a bit like the sheet music score for a piano composition, in which the left-to-right axis of the music staff corresponds to time, while the markings up and down the staff correspond to the notes (i.e., frequencies) that are played at the given time.
<figure id="specs" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="alas-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="alas" data-alt="Image">
<img src="/resources/54660a19f07098b859744c14ddc1e34ca18c1910/alas.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ed1b413336e0dad28a2facbe0e140e5bcedeeea1/alas.eps" data-type="image"></span></span>
</figure>
<figure id="specgram-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="specgram" data-alt="Image">
<img src="/resources/a3b36f09e9774b559a791d8e6ea13d0fa3b4e589/specgram.png" data-media-type="image/png" alt="Image" width="435">
</span>
</figure>
</figure></p></div></li></ul></li><li><div data-type="page"><h1>The Discrete-Time Fourier Transform</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A580b4363-4532-4798-87b1-28491fa2bef7%401.html" data-type="page"><h1>Eigenanalysis of LTI Systems (Infinite-Length Signals)</h1><div data-type="document-title">Eigenanalysis of LTI Systems (Infinite-Length Signals)</div>
  <p id="delete_me"><span data-type="title">Eigenvectors of LTI Systems (infinite-length)</span>For a system $H$ operating on infinite-length discrete-time signals, an eigenvector is a signal which, when passing through the system, is modified only by a scalar factor. So suppose $s[n]$ is an eigenvector to system $H$, then $H\{s[n]\}=\lambda_s s[n]$, where $\lambda_s$ is a complex number.

It is a remarkable property that all LTI systems have a set of eigenvectors, and furthermore that these eigenvectors are of the form $e^{j\omega n}$. This means that, whatever $\omega$ may be, if $e^{j\omega n}$ is input into any LTI system, the output will simply be a scaled version of the input (the amount of scaling depending on the nature of the system, of course):
<figure id="hinout"><figcaption>Complex harmonic sinusoids are eigenvectors of discrete-time infinite length LTI systems. When given as inputs to any LTI system, the output is simply a scaled version of the input.</figcaption><span data-type="media" id="hinout-plot" data-alt="Image">
<img src="/resources/82e721bf77668098309015a9397292c794f913d5/hinout.png" data-media-type="image/png" alt="Image" width="600">
</span></figure></p><p id="eip-149">Proving this special property of LTI systems is straightforward; we simply express the output in terms of the input ($s_\omega[n]=e^{j\omega}n$) and system impulse response ($h[n]$) through the convolution sum, and simplify:
$\begin{align*}
s_\omega[n] \ast h[n] &amp;=\sum_{m=-\infty}^{\infty} s_\omega [n-m]\,h[m]\\
&amp;=\sum_{m=-\infty}^{\infty} e^{j \omega (n-m)} \,h[m] \\
&amp;=\sum_{m=-\infty}^{\infty} e^{j \omega n} \, e^{-j \omega m} \, h[m]\\
&amp;=\left( \sum_{m=-\infty}^{\infty}  h[m] \, e^{-j \omega m} \right) e^{j \omega n} \\
&amp;= \lambda_\omega \, s_\omega[n] ~~\checkmark
\end{align*}$
</p><p id="eip-964"><span data-type="title">The Frequency Response of an LTI System</span>Take a close look at the value $\lambda_\omega$ above. It is intimately related to the system, as we would expect, by the system's impulse response:
$\lambda_\omega=\sum_{m=-\infty}^{\infty}  h[m] \, e^{-j \omega m}$
This is simply the DTFT of the impulse response! It is so important, we give it a special notation $H(\omega)$, and a special name as well. $H(\omega)$ is called the <span data-type="term">frequency response</span>, for it explains how the system responds (i.e., in what way it scales) to particular input frequencies $\omega$. This is also apparent by seeing the DTFT as an inner product of $h[n]$ with the sinusoidal $e^{-j \omega m}$. The value of the inner product (which is the value $H(\omega)$ grows or shrink in ways corresponding to the similarity of $h[n]$ and $e^{-j \omega m}$.</p><p id="eip-562">Although we cannot display the system matrix involved (as it is infinite in length), the DTFT diagonalizes the system $H$ for infinite length signals, just as the DFT does with finite length ones (LINK). Expressing this diagonalization in terms of the DTFTs of the system, we have that if $y[n]=h[n]\ast x[n]$, then $Y(\omega)=H(\omega)X(\omega)$.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ac9fb3ca1-baaf-4bac-9130-e5d273c304c2%401.html" data-type="page"><h1>The Discrete-Time Fourier Transform</h1><div data-type="document-title">The Discrete-Time Fourier Transform</div>
  <p id="delete_me">It is a significant concept in signal processing that there is more than one way to express the information in a given signal. One way is by defining the signal in terms of the value it has at any given time $n$, thus the notation $x[n]$. But is also possible to express a signal in terms of how it is be a combination of signals from a particular signal set. We could express the information in the signal by noting how much of each signal in the signal set contributes in the combination.</p><p id="eip-412"><span data-type="title">Definition of the DTFT</span>The <span data-type="term">discrete-time Fourier transform</span>, or DTFT, is a common way to express infinite-length discrete-time signals in another way. As noted above, the signal $x[n]$ can be understood as an entity as having a particular value for every time $n$ in the range $-\infty\lt n\lt \infty$. But it also can be understood in this way:
$x[n]=\frac{1}{2\pi}\int\limits_{-\pi}^{\pi}X(\omega)e^{-j\omega n}d\omega$
$x[n]$ is expressed in terms of how much of each signal $e^{-j\omega n}$ goes in to the integral which composes it. The weighting of each signal is according to the function $X(\omega)$. The values of that function are found according to this formula:
$X(\omega)=\sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}$
This formula for $X(\omega)$ is known as the DTFT of $x[n]$, while that first integral formula is known as the inverse DTFT.</p><p id="eip-954"><span data-type="title">Interpretations of the DTFT</span>We have given above the formulas for the DTFT and the inverse DTFT:
--$X(\omega)=\sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}$
--$x[n]=\frac{1}{2\pi}\int\limits_{-\pi}^{\pi}X(\omega)e^{-j\omega n}d\omega$
But what do these formulas mean? There are a few ways of understanding them. One is to say that the two formulas are expressing the same entity, but in different domains: $x[n]$ is the time-domain representation of the signal, whereas $X(\omega)$ is the frequency-domain representation of it. In that sense, it is like two different language translations of the same book (this analogy is not perfect, for translations are not exactly one-to-one; there are many English-language translations of the Spanish book Don Quixote, but there is only one frequency-domain representation $X(\omega)$ for a time-domain signal $x[n]$).

A second way to understand the DTFT is in terms of the concepts of analysis and synthesis. The DTFT formula gives us a frequency analysis of the signal $x[n]$. The signal $X(\omega)$ analyzes $x[n]$ from a different point of view; it tells us the frequency content of $x[n]$, whether it is composed of only low frequencies, or just high ones, or whatever the case may be. This is because every value of $X(\omega)$ is simply the inner product of $x[n]$ with $e^{j\omega n}$; it tells us the "strength" of $e^{j\omega n}$ in the signal $x[n]$. The inverse DTFT then synthesizes, or composes, $x[n]$ based upon its weighting at each different frequency.</p><p id="eip-724"><span data-type="title">Relationship between the DTFT and the DFT</span>The DTFT is not the only Fourier transform for discrete-time signals. While the DTFT operates on infinite-length signals, the DFT (discrete Fourier transform) is a tool for frequency analysis of finite-length signals. A finite-length signal (say, of length $N$) $x[n]$ has an $N$ length frequency representation $X[k]$, according to these formulas:
$X[k]=\sum\limits_{n=0}^{N-1}x[n]e^{-j\frac{2\pi}{N}kn}$
$x[n]=\frac{1}{N}\sum\limits_{k=0}^{N-1}X[k]e^{j\frac{2\pi}{N}kn}$

As $x[n]$ and $X[k]$ are N-periodic, we can also represent them in this way, shifted about $n$ and $k$ at the origin:
$X[k]=\sum\limits_{n=-\frac{N}{2}}^{\frac{N}{2}-1}x[n]e^{-j\frac{2\pi}{N}kn}$
$x[n]=\frac{1}{N}\sum\limits_{k=-\frac{N}{2}}^{\frac{N}{2}-1}X[k]e^{j\frac{2\pi}{N}kn}$

We can develop the DTFT by seeing what happens to the DFT as $N$ gets larger and larger. As $N$ approaches $\infty$, so also do $n$ and $k$ run from $\infty$ to $\infty$. Consider also what happens to $e^{-j\frac{2\pi}{N}kn}$. As $N$ gets infinitely large, the exponent term $\frac{2\pi}{N}k$ becomes a continuous value, running from $-\pi$ to $\pi$, and we can call that value $\omega$. We have then that the DFT sum becomes:
$X(\omega)=\sum\limits_{n=-\infty}^{\infty}x[n]e^{-j\omega n}$
which is the definition of the DTFT.

Similarly, on the synthesis side of the transform, the DFT "sum" will not run over a discrete number of values $k$, but rather become an integral over the continuous variable $\omega$:
$x[n]=\int\limits_{-\pi}^{\pi}X(\omega)e^{j\omega n}\frac{d\omega}{2\pi}$
which is our inverse DTFT.

Graphically, we can see how increasing the length of a signal by adding more and more zeros on either side of it results in the signal's DFT (which is expressed in terms of $k$) appearing to be more and more like a continuous function (expressed in terms of $\omega$):
<figure id="DFTtoDTFT"><span data-type="media" id="DFTtoDTFT-plot" data-alt="Image">
<img src="/resources/8ee1029134894cd43081ab8b336c7fad78339ebb/DFTtoDTFT.png" data-media-type="image/png" alt="Image" width="700">
</span></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8829b7cb-62ab-43a2-9910-a9b40452c825%401.html" data-type="page"><h1>Discrete Time Fourier Transform Examples</h1><div data-type="document-title">Discrete Time Fourier Transform Examples</div>
  <p id="delete_me">The DTFT and inverse DTFT are defined as follows:
$X(\omega) ~=~ \sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}, ~~~~~~ -\pi \leq \omega \lt \pi$
$x[n] ~=~ \int_{-\pi}^\pi X(\omega)\, e^{j\omega n} \, \frac{d\omega}{2\pi} , ~~~~ \infty\lt n\lt\infty  $

Let's work out some examples of DTFT and inverse DTFT calculations</p><p id="eip-651"><span data-type="title">Impulse Response of an Ideal Lowpass Filter</span>We'll start with an ideal lowpass filter. From its frequency response we can see that it blocks all incoming frequencies having a magnitude greater than $|\omega_c|$:
$
H(\omega) = \begin{cases}
	1 &amp; -\omega_c \leq \omega \leq \omega_c \\
	0 &amp; {\sf otherwise} \\
\end{cases}
$
<figure id="lowpass"><span data-type="media" id="lowpass-plot" data-alt="Image">
<img src="/resources/d18227a52ce143c17cf9104f0d9257ab23e0edc8/lowpass.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f496f2f957f0fcd0727b42914815312a7df8fa40/lowpass.eps" data-type="image"></span>
</span></figure><figure id="sinc1"><figcaption>CAPTION.</figcaption><span data-type="media" id="sinc1-plot" data-alt="Image">
<img src="/resources/8943e8b9947ec28b65cc44367f82f98f3d9bcee0/sinc1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4c6d6dfc40a7a6e41025ffdbda418df09e355ad6/sinc1.eps" data-type="image"></span>
</span></figure></p><p id="eip-371"><span data-type="title">DTFT of a Moving Average System</span>Consider now a moving average system, where the system output at a time $n$ is the average of $M$ input values, symmetric about $n$. The impulse response of such a system is a pulse function:
$p[n] = \begin{cases}
	1 &amp; -M\leq n \leq M \\
	0 &amp; {\sf otherwise} \\
\end{cases}
$
<figure id="pulse2"><figcaption>A symmetrical pulse, with $M=3$.</figcaption><span data-type="media" id="pulse2-plot" data-alt="Image">
<img src="/resources/608cdcf9539e1d106072ddd7e5cd8ebc138ce76c/pulse2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ee872f97f76fe304d6008c14efbe40e213920705/pulse2.eps" data-type="image"></span>
</span></figure>

Suppose we would like to find the frequency response of this system. We merely need to take the DTFT of the impulse response:
$P(\omega) ~=~ \sum_{n=-\infty}^{\infty} p[n]\,  e^{-j \omega n}\\
~=~ \sum_{n=-M}^M  e^{-j \omega n}\\
~=~ \sum_{n=-M}^M  \left( e^{-j \omega} \right)^n\\
~=~ \frac{ e^{j \omega M} - e^{-j \omega (M+1)}}{1-e^{-j \omega}}\\
~=~ frac{ e^{j \omega M} - e^{-j \omega (M+1)}}{1-e^{-j \omega}}\\
~=~  \frac{ e^{-j\omega/2} \left( e^{j \omega \frac{2M+1}{2}} - e^{-j \omega \frac{2M+1}{2}} \right)}
{ e^{-j\omega/2}\left( e^{j\omega/2}-e^{-j \omega/2} \right)} \\
~=~ \frac{ 2j \sin\left(\omega \frac{2M+1}{2}\right) }{ 2j \sin\left(\frac{\omega}{2}\right) }\\
~=~\frac{ \sin\left(\frac{2M+1}{2}\,\omega \right) }{ \sin\left(\frac{\omega}{2}\right) }
$
This frequency correspondence to the time-domain pulse is known as a digital sinc:
<figure id="Fpulse1"><figcaption>A digital sinc function.</figcaption><span data-type="media" id="Fpulse1-plot" data-alt="Image">
<img src="/resources/fb22a48d6f35b5c43b5de34496cad635a7a6b20f/Fpulse1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/68ae41b01139fa8550e2636075f46ea6efbf2e3c/Fpulse1.eps" data-type="image"></span>
</span></figure>
The digital sinc function is similar to a regular sinc function, except that rather than slowly decaying over time, it is $2\pi$ periodic.

When we compare this to the first example, the impulse response of an ideal lowpass filter, we see an interesting correspondence. Pulses in one domain (either time or frequency) have sinc-like representations in the alternate domain.</p><p id="eip-685"><span data-type="title">DTFT of a One-Sided Exponential</span>Having found the frequency response of a moving average system, let's now do the same for a recursive average system. The input/output relationship of such a system can be expressed as $y[n]=x[n]+\alpha y[n-1],$ where $|\alpha|\lt 1$. The impulse response for such a system is $h[n]=\alpha^n u[n]$:
<figure id="hRA"><figcaption>The impulse response of a recursive average system.</figcaption><span data-type="media" id="hRA-plot" data-alt="Image">
<img src="/resources/1c3d318b007a38fc4f2caa2924e1a3aa52860192/hRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7c8c5679755aea30d1e7c26e26b3e9dfd874f2e2/hRA.eps" data-type="image"></span>
</span></figure>
We'll now take the DTFT of this impulse response:
$H(\omega) ~=~ \sum_{n=-\infty}^{\infty} h[n]\,  e^{-j \omega n} ~=~ \sum_{n=0}^\infty \alpha^n\, e^{-j \omega n}
~=~ \sum_{n=0}^\infty ( \alpha\, e^{-j \omega})^n ~=~ \frac{1}{1 - \alpha\, e^{-j \omega} }$
<figure id="FhRA"><figcaption>The magnitude of the frequency response of a one-sided exponential.</figcaption><span data-type="media" id="FhRA-plot" data-alt="Image">
<img src="/resources/f1ccb984af7441b29b93d17da4ede25528f0145f/FhRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/110b9398820fdbc1c8c74c215397fe47bc41282e/FhRA.eps" data-type="image"></span>
</span></figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A7989578f-0dd9-4eed-9311-ff866f9888f7%401.html" data-type="page"><h1>The Discrete-Time Fourier Transform of a Sinusoid</h1><div data-type="document-title">The Discrete-Time Fourier Transform of a Sinusoid</div>
  <p id="delete_me">Recall the definition of the DTFT of a signal $x[n]$:
$
X(\omega) ~=~ \sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}
$

Let's see what the DTFT of a complex exponential signal $e^{-j \omega_0 n}$:
$
X(\omega_0) ~=~ \sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n} ~=~  \sum_{n=-\infty}^{\infty} e^{j \omega_0 n}\,  e^{-j \omega n} ~=~ \sum_{n=-\infty}^{\infty} e^{-j (\omega-\omega_0) n}
$

The nature of that sum is not immediately clear. When $\omega=\omega_0$, then the sum becomes unbounded: $\sum_{n=-\infty}^{\infty} 1=\infty$. But when $\omega\neq\omega_0$, what does an infinite sum for a sinusoid mean?</p><p id="eip-734"><span data-type="title">The Dirac Delta Function</span>To find the DTFT of a sinusoid, it will to be necessary to consider another function, the dirac delta function. Suppose we have a function $d_\epsilon(\omega)$:
<figure id="diraclimit1"><span data-type="media" id="diraclimit1-plot" data-alt="Image">
<img src="/resources/6b26452eccc80b49dc8a1046104e9fd5e29c9aa6/diraclimit1.png" data-media-type="image/png" alt="Image" width="200">
</span></figure>
Note how the non-zero portion of this function has a width of $\epsilon$ and a height of $\frac{1}{\epsilon}$. The area underneath the function is therefore always $1$, no matter what $\epsilon$ may be.

We will let the value of $\epsilon$ get smaller and smaller. As it approaches zero, the function will become taller and narrower, until at the limiting case it is "infinitely" tall and "infinitesimally" narrow. We will denote the function in this limiting case $\delta(\omega)$. But it will still have an area of $1$, thus
$\int \delta(\omega)d\omega=1$
Suppose we scale the $\delta(\omega)$ by another function $X(\omega)$. Since $\delta(\omega)$ is non-zero at all values other than at $\omega=0$, scaling $\delta(\omega)$ by $X(\omega)$ within the integral will yield the following:
$\int X(\omega)\delta(\omega)d\omega=X(0)$

What if the integrand has $\delta(\omega-\omega_0)$ and is scaled by $X(\omega)$. In this case the integrand is only nonzero at $\omega=\omega_0$, thus:
$\int X(\omega)\delta(\omega-\omega_0)d\omega=X(\omega_0)$

The $\delta(\omega)$ "function" we have derived is known as the <span data-type="term">Dirac delta function</span>. It is not technically a function (it is a generalized function, or distribution), but for our purposes we can treat it as a function with infinite height, infinitesimal narrowness, and an area of 1 at $\omega=0$.
</p><p id="eip-718"><span data-type="title">The DTFT of a Sinusoid</span>Having introduced the Dirac delta function, let's see what happens if we attempt to find its inverse DTFT:
$
\int_{-\pi}^\pi 2\pi \delta(\omega-\omega_0)\, e^{j\omega n} \, \frac{d\omega}{2\pi}
~=~ e^{j\omega_0 n}
$

The inverse DTFT of a dirac delta function is a complex sinusoid, which means that the DTFT of a complex sinusoid is a dirac delta:
$
e^{j\omega_0 n} ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ 2\pi\,\delta(\omega-\omega_0)
$

Via Euler's formula, we use the above to also find the DTFT of discrete-time cosines and sines:
$\cos(\omega_0 n) ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~
\pi\,\delta(\omega-\omega_0) + \pi\,\delta(\omega+\omega_0)
$

$
\sin(\omega_0 n) ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~
\frac{\pi}{j}\,\delta(\omega-\omega_0) +  \frac{\pi}{j}\,\delta(\omega+\omega_0)
$</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Adc2b62cc-439b-4895-a130-68f945d55269%401.html" data-type="page"><h1>Discrete Time Fourier Transform Properties</h1><div data-type="document-title">Discrete Time Fourier Transform Properties</div>
  <p id="delete_me">Recall the definition of the DTFT and the inverse DTFT:
$
X(\omega) ~=~ \sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}, ~~~~~~ -\pi \leq \omega \lt \pi
$

$
x[n] ~=~ \int_{-\pi}^\pi X(\omega)\, e^{j\omega n} \, \frac{d\omega}{2\pi} , ~~~~~~ \infty\lt n\lt\infty
$

We refer to a signal and its corresponding DTFT as a DTFT signal pair:
$
x[n] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ X(\omega)
$</p><p id="eip-740"><span data-type="title">DTFT Periodicity</span>Although the DTFT of a signal is typically defined only on a $2\pi$ interval, it is nevertheless straightforward to show that the DTFT is actually $2\pi$ periodic:
$X(\omega+2\pi k) ~=~ \sum_{n=-\infty}^{\infty} x[n]\, e^{-j (\omega+2\pi k) n}
~=~ \sum_{n=-\infty}^{\infty} x[n]\,  e^{-j \omega n}  \, e^{-j 2\pi k n}
~=~ X(\omega)~~\checkmark$
<figure id="Pdsinc"><figcaption>The $2\pi$ periodicity of the DTFT.</figcaption><span data-type="media" id="Pdsinc-plot" data-alt="Image">
<img src="/resources/8c9118f87fc885e30277dfbaae34714be3fbe23d/Pdsinc.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/153fa015ae648b47df217bc3e95c5ca936fdf01c/Pdsinc.eps" data-type="image"></span>
</span></figure></p><p id="eip-319"><span data-type="title">DTFT Frequencies</span>Because $X(\omega)$ is essentially the inner product of a signal $x[n]$ with the signal $e^{j\omega n}$, we can say that $X(\omega)$ tells us how strongly the signal $e^{j\omega n}$ appears in $x[n]$. $X(\omega)$, then, is a measure of the "frequency content" of the signal $x[n]$. Consider the plot below of the DTFT of some signal $x[n]$:
<figure id="dsinc"><span data-type="media" id="dsinc-plot" data-alt="Image">
<img src="/resources/21d0cf716461b7ac7f4ad632e6ef4a9afef5979c/dsinc.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8ef067874d5220ad5f251b65e0f49b930d1fef70/dsinc.eps" data-type="image"></span>
</span></figure>
This plot shows us that the signal $x[n]$ has a significant amount of low frequency content (frequencies around $\omega=0$), and less higher frequency content (frequencies around $\omega=\pm\pi, remember that the DTFT is $2\pi$ periodic).
</p><p id="eip-582">As the DTFT is $2\pi$ periodic, all the information in a DTFT is contained in any $2\pi$ length section. However, there are only two intervals that are typically used. One is to refer to the DTFT from the interval of $0$ to $2\pi$. The low frequencies are at the extremities of the plot, with the highest frequency in the center:
<figure id="dsinc1"><span data-type="media" id="dsinc1-plot" data-alt="Image">
<img src="/resources/55821973b8f894df2750ccb859e711ee98899700/dsinc1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5313b5f94128b298dda6367ef6b63a8e2f854228/dsinc1.eps" data-type="image"></span>
</span></figure>
The other commonly used interval chosen to express the DTFT is between $\pi$ and $\pi$. In that interval, the low frequencies are at the center, with the high frequencies at the edges:
<figure id="dsincagain"><span data-type="media" id="dsincagain-plot" data-alt="Image">
<img src="/resources/21d0cf716461b7ac7f4ad632e6ef4a9afef5979c/dsinc.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8ef067874d5220ad5f251b65e0f49b930d1fef70/dsinc.eps" data-type="image"></span>
</span></figure></p><p id="eip-245"><span data-type="title">The DTFT and Time Shifts</span>If a signal is shifted in time, what effect might this have on its DTFT? Supposing $x[n]$ and $X(\omega)$ are a DTFT pair, we have that
$
x[n-m] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ e^{-j\omega m} X(\omega)
$
So shifting a signal in time corresponds to a modulation (multiplication by a complex sinusoid) in frequency. We can use the DTFT formula to prove this relationship, by way of a change of variables $r=n-m$:
$\begin{align*}
\sum_{n=-\infty}^{\infty} x[n-m]\,  e^{-j \omega n}
&amp;=
\sum_{r=-\infty}^{\infty} x[r]\,  e^{-j \omega (r+m)}\\
&amp;=
\sum_{r=-\infty}^{\infty} x[r]\,  e^{-j \omega r} \, e^{-j \omega m} \\[2mm]
&amp;=
e^{-j \omega m} \sum_{r=-\infty}^{\infty} x[r]\,  e^{-j \omega r}\\
amp;=
e^{-j \omega m} \, X(\omega) ~~\checkmark
\end{align*}$</p><p id="eip-167"><span data-type="title">The DTFT and Time Modulation</span>We saw above how a shift in time corresponds to modulation in frequency. What do you suppose happens when a signal is modulated in time? If you guessed that it is shifted in frequency, you're right! If a signal $x[n]$ has a DTFT of $X(\omega)$, then we have this DTFT pair:
$
e^{j\omega_0 n} \, x[n] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~  X(\omega-\omega_0)
$
Below is the proof:
$
\sum_{n=-\infty}^{\infty} e^{j\omega_0 n} \, x[n]\,  e^{-j \omega n}
~=~
\sum_{n=-\infty}^{\infty} x[n]\,  e^{-j (\omega-\omega_0) n}
~=~
X(\omega-\omega_0)~~\checkmark
$</p><p id="eip-676"><span data-type="title">The DTFT and Convolution</span>Suppose that the impulse response of an LTI system is $h[n]$, the input to the system is $x[n]$, and the output is $y[n]$. Because the system is LTI, these three signals have a special relationship:
$
y[n] ~=~ x[n] * h[n] ~=~ \sum_{m=-\infty}^{\infty} \: h[n-m] \, x[m]
$
The output $y[n]$ is the convolution of $x[n]$ with $h[n]$. Just as with the other DTFT properties, it turns out there is also a relationship in the frequency domain. Consider the DTFT of each of those signals; call them $H(\omega)$, $X(\omega)$, and $Y(\omega)$. The convolution of the signals $x$ and $h$ in time corresponds to the multiplication of their DTFTs in frequency:
$y[n]=x[n]*h[n] \stackrel{\mathrm{DTFT}}{\longleftrightarrow} Y(\omega)=X(\omega)H(\omega)$
This relationship is very important. It gives insight, showing us how LTI systems modify the frequencies of input signals. It is also useful, because it gives us an alternative way of finding the output of a system. We could take the DTFTs of the input and impulse response, multiply them together, and then take the inverse DTFT of the result to find the output. There are some cases where this process might be easier than finding the convolution sum.</p><p id="eip-540"><span data-type="title">The DTFT and Linearity</span>Since the DTFT is an infinite sum, it should come as no surprise that it is a linear operator. Nevertheless, it is a helpful property to know. Suppose we have the following DTFT pairs:
$x_1[n] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ X_1(\omega) ~~~~~~~~
x_2[n] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ X_2(\omega)$
Then by the linearity of the DTFT we have that, for any constants $\alpha_1$ and $\alpha_2$:
$
\alpha_1 x_1[n] + \alpha_2 x[2] ~\stackrel{\mathrm{DTFT}}{\longleftrightarrow}~ \alpha_1 X_1(\omega)+ \alpha_2 X_2(\omega)
$

</p><p id="eip-421"><span data-type="title">DTFT Symmetry</span>Looking at the plots of the DTFTs above, or perhaps of those you have computed on your own, you may notice that they are often symmetrical. It so happens that the DTFTs of real-valued signals always have a kind of symmetry, but there are other symmetry relationships as well. For example, signals that are purely imaginary and odd have DTFTs that are purely real and odd. These types of symmetry are a result of a property of the complex exponentials which build up the DTFTs. Any signal of the form $e^{j\omega n}$ is conjugate symmetric, meaning that its real part is even and its imaginary part is odd. Additionally, for conjugate symmetric signals, their magnitude is even and their phase is odd.</p><p id="eip-249">As a result of the symmetry of the signals which compose a DTFT, the DTFTs of certain signals also have symmetries, according to the table below:
***DTFT SYMMETRY TABLE HERE***</p></div></li></ul></li><li><div data-type="page"><h1>The z-Transform</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3c5fd8f2-8393-4409-b8d1-2f748f731cbf%401.html" data-type="page"><h1>The z-Transform</h1><div data-type="document-title">The z-Transform</div>
  <p id="delete_me"><span data-type="title">Generalizing the DTFT</span>We have seen that the DTFT is a useful tool for representing and analyzing discrete-time infinite length signals and LTI systems. With the DTFT, signals can be represented in terms of their frequency composition. Because complex harmonic sinusoids are eigenvectors of LTI systems, the DTFT of a system's impulse response gives us the system's frequency response (it tells us how the system modifies input signals according to their composite frequencies).</p><p id="eip-688">The backbone of the DTFT is the complex harmonic sinusoid: $e^{j\omega n}$. This type of signal is actually a specific example of a more broader class of signals $z^n$, where $z$ is a complex number. Signals in this class are called complex exponentials. As $z^n=|z|^n e^{j(\angle z)n}$, we see that complex exponentials are simply complex harmonic sinusoids, with exponential function envelopes. When $|z|\lt 1$, they decay in time:
<figure id="cexp10s" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="cexp10I-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="cexp10I" data-alt="Image">
<img src="/resources/4cf6e19d7f0eecb48cd170934c999c8b8d5a403f/cexp10I.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/41cf99191c9f2af4638880b24dd0c91edb79e733/cexp10I.eps" data-type="image"></span></span>
</figure>
<figure id="cexp10R-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="cexp10R" data-alt="Image">
<img src="/resources/801397602fa05fb339435101f553e920572bee85/cexp10R.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/e9092cc0a5188002c66744cfcc4e768486111313/cexp10R.eps" data-type="image"></span></span>
</figure>
</figure>
When $|z|\gt 1$, they grow in time:
<figure id="cexp11s" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="cexp11I-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="cexp11I" data-alt="Image">
<img src="/resources/18dbc467052e4784e80bbba6332e2f4bf42ac114/cexp11I.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/20581641824cf7950ed5b3924f225a551ae925c7/cexp11I.eps" data-type="image"></span></span>
</figure>
<figure id="cexp11R-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="cexp11R" data-alt="Image">
<img src="/resources/5fb0166f970b736d17f4907c1f00bf789989dcea/cexp11R.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7f97f7127d3d818459154485c67968ac1a3f6d99/cexp11R.eps" data-type="image"></span></span>
</figure>
</figure>
And when $|z|=1$, they are of course simply complex harmonic sinusoids of the form $e^{j\omega n}$.</p><p id="eip-228">Since complex harmonic sinusoids have the special property that they are eigenvectors of LTI systems, we may ask if the more general signal class of complex exponentials does, as well. To test, we will find the an LTI system's output (i.e., the result of the convolution sum) to some generic complex exponential $z^n$:
$\begin{align*}
y[n]&amp;=z^n\ast h[n]\\
&amp;=\sum\limits_{m=-\infty}^\infty x[n-m]h[n]\\
&amp;=\sum\limits_{m=-\infty}^\infty z^{n-m}h[n]\\
&amp;=\sum\limits_{m=-\infty}^\infty z^n z^{-m} h[n]\\
&amp;=z^n \sum\limits_{m=-\infty}^\infty  z^{-m} h[n]\\
&amp;=\left(\sum\limits_{m=-\infty}^\infty  z^{-m} h[n]\right)z^n\\
&amp;=H(z)z^n~,~H(z)=\sum\limits_{m=-\infty}^\infty z^{-m} h[n]
\end{align*}$
If we input a complex exponential into some LTI system, then indeed the output is simply a scaled version of that same exponential. So complex exponentials are also eigenvectors of LTI systems. Now, the amount a given LTI system scales a complex exponential $z^n$ is very important, so much so that it has a special name--the system's transfer function (it tells us how the system "transfers" the input into the output)--and a special label, $H(z)$.</p><p id="eip-520">This transfer function of LTI systems, $H(z)$, is clearly a generalization of the frequency response $H(\omega)$. The frequency response indicated how an LTI system $H$ scales input complex sinusoids $e^{j\oemga}$, and likewise the transfer function $H(z)$ tells us how the system scales input complex exponentials $z^n$. As $e^{j\omega n}=z^n|_{z=e^{j\omega}}$ it follows that $H(\omega)=H(z)|_{z=e^{j\omega}}$. So it would seem that the DTFT of a function (for $H(\omega)$ is the DTFT of $h[n]$) can be generalized to be a broader kind of transform, which is represented by $H(z)$. This broader class of transform is called the z-transform.</p><p id="eip-184"><span data-type="title">The z-Transform</span>Let $x[n]$ be a discrete-time infinite length signal. Then
$X(z)=\sum\limits_{n=-\infty}^{\infty}x[n]z^{-n}$
is defined to be the bilateral <span data-type="term">z-transform</span> of $x[n]$ (the unilateral z-transform sums only from $n=0$ to $\infty$; unless otherwise stated, we assume the transform is bilateral).</p><p id="eip-230">There are a few things we can note immediately about the z-transform. First, unlike either transform we have considered (the DFT and DTFT), it is a complex function of a COMPLEX variable. Second, and like the DTFT (but not the DFT), the z-transform may not necessarily exist for all $z$ and/or all $x[n]$. And third, like both other transforms, the z-transform diagonalizes LTI systems. By this, we mean that if a signal $x[n]$ is expressed in terms of its z-transform, then the z-transform of the output at various $z$ is simply the pointwise multiplication of the input's transform with the transform of the impulse response (i.e., the transfer function:
$Y(z)=H(z)X(z)$</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Ac6c8ebe6-a3bc-4c56-b2a0-86da3d6e0cb2%401.html" data-type="page"><h1>The z-Transform Region of Convergence</h1><div data-type="document-title">The z-Transform Region of Convergence</div>
  <p id="delete_me"><span data-type="title">The Existence of the z-Transform</span>Recall the definition of the z-transform for some discrete-time infinite length signal $x[n]$:
$X(z)=\sum\limits_{n=-\infty}^{\infty}x[n]z^{-n}$
Given that this is an infinite sum (unlike, say, the sum of $N$ terms in a DFT), it will not necessarily converge for all $x[n]$ and/or all $z$. For some $x[n]$, it will converge for all values of $z$. For example, if $x[n]$=\delta[n]$, then:
$X(z)=\sum\limits_{n=-\infty}^{\infty}\delta[n]z^{-n}=1$
For some, $x[n]$ and $z$, the sum may not converge. For example, if $x[n]=1$ and $z=1$, the sum is unbounded. As it happens, if indeed $x[n]=1$ then there is no nonzero $z$ such that the sum converges.</p><p id="eip-721"><span data-type="title">The Region of Convergence</span>So for some $x[n]$ (like $x[n]=\delta[n]$) the z-transform sum will converge for all $z$, and for some $x[n]$ (like $x[n]=1$) the z-transform will not converge for ANY $z$. As you might suspect, there are some $x[n]$ for which the sum converges for some $z$. These values of $z$, those for which the z-transform exists, are known as the z-transform's <span data-type="term">region of convergence</span>. Consider, for example the z-transform for the signal $x_1[n]=\alpha^n u[n]~,~\alpha=.8$:
<figure id="xRA"><span data-type="media" id="xRA-plot" data-alt="Image">
<img src="/resources/5b011d78ce24d555916fd09826defe8a36ddf3be/xRA.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5052ef0729b9bc77836df3c8e851c48ee27327de/xRA.eps" data-type="image"></span>
</span></figure>
$\begin{align*}
X_1(z) &amp;=\sum_{n=-\infty}^{\infty}  x_1[n] \, z^{-n}\\
&amp;= \sum_{n=0}^{\infty}  \alpha^n \, z^{-n}\\
&amp;= \sum_{n=0}^{\infty}  (\alpha \, z^{-1})^n\\
&amp;=\begin{cases}
\frac{1}{1-\alpha \, z^{-1}}&amp;|\alpha z^{-1}|\lt 1\\
\textrm{undefined}&amp;\textrm{else}
\end{cases}
\end{align*}$
So, the z-transform $X_1(z)$ only exists for $|\alpha z^{-1}|\lt 1$, or put another way, for $|z|\gt \alpha$; it is not defined for other values of $z$. This should not be too terribly shocking, for we are familiar with other functions that only exist for certain values, e.g. $\log{x}$ only for $x\gt 0$, or $\frac{1}{x}$ only for $x\neq 0$.</p><p id="eip-797">However, there is a difference between the z-transform and its region of convergence and functions like $\log{x}$ or $\frac{1}{x}$. Where the defined regions of existence of the values of $\log{x}$ or $\frac{1}{x}$ are quite clear, there is nothing intrinsic about the function $\frac{z}{z-\alpha}$ to suggest it only exists for $|z|\gt\alpha$ (of course it is apparent that $z$ cannot equal $\alpha). Why in the world would it be a problem if $z=\alpha/2$, for then we would simply have $\frac{\alpha/2}{\alpha/2-\alpha}$, right? The reason is that the function was a simplification of the sum, a simplification that existed only for certain $z$.</p><p id="eip-852">To see why that is the case, consider another signal related to the one we just considered. Let $x_2[n]=-\alpha^{n} u[-n-1]~,~\alpha=.8$:
<figure id="xRA1"><span data-type="media" id="xRA1-plot" data-alt="Image">
<img src="/resources/dd3f689c09bb5063c8750f46dce12cd54bf80996/xRA1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/5516705a4229810ff2c13ea652efc37d9e611b65/xRA1.eps" data-type="image"></span>
</span></figure>
We will now consider its z-transform:
$\begin{align*}
X_2(z) &amp;= \sum_{n=-\infty}^{\infty}  x_2[n] \, z^{-n}\\
&amp;= \sum_{n=-\infty}^{-1}  -\alpha^{n} \, z^{-n}\\
&amp;= -\sum_{n=1}^{\infty}\alpha^{-n} \, z^{n}\\
&amp;= -\sum_{n=0}^{\infty} (\alpha^{-1}\, z)^{n} + 1 \\[2mm]
&amp;=\begin{cases} \frac{-1}{1-\alpha^{-1} \, z} + 1 &amp;|\alpha^{-1}z|\lt1\\
\textrm{undefined}&amp;\textrm{else}\end{cases} \\
&amp;= \begin{cases} \frac{-1}{1-\alpha^{-1} \, z}  + \frac{1 - \alpha^{-1} \, z}{1-\alpha^{-1} \, z} &amp;|\alpha^{-1}z|\lt1\\
\textrm{undefined}&amp;\textrm{else}\end{cases} \\
&amp;= \begin{cases}  \frac{1}{1-\alpha \, z^{-1}}&amp;|\alpha^{-1}z|\lt1\\
\textrm{undefined}&amp;\textrm{else}\end{cases} \\
\end{align*}$</p><p id="eip-570">So for $x_2[n]$, its z-transform is defined only for $|\alpha^{-1}z|\lt 1$, or equivalently, $|z|\lt\alpha$, and for those values it is $\frac{1}{1-\alpha z^{-1}}$. Now we see another reason why the region of convergence is so important; it is not merely added information about the z-transform, it is intrinsic to its very definition. The z-transforms for $x_1[n]$ and $x_2[n]$ are completely different, which is to be expected as those are completely different signals. However, the only difference between the transforms is the region of convergence:
$\begin{align*}
x_1[n]=\alpha^{n} u[n]&amp;\leftrightarrow X_1(z)=\begin{cases}  \frac{1}{1-\alpha \, z^{-1}}&amp;|z|\gt\alpha\\
\textrm{undefined}&amp;\textrm{else}\end{cases}\\
x_2[n]=-\alpha^{n} u[-n-1]&amp;\leftrightarrow X_2(z)=\begin{cases}  \frac{1}{1-\alpha \, z^{-1}}&amp;|z|\lt\alpha\\
\textrm{undefined}&amp;\textrm{else}\end{cases}
\end{align*}$</p><p id="eip-990"><span data-type="title">Types of ROCs</span>We have seen a few different types of ROCs for the z-transforms of various signals. If the z-transform for a discrete-time signal exists, then the region of convergence will fall into one of these three categories:
--if the signal has a finite non-zero duration, then the region of convergence will be the entire z-plane, with the possible exception of $z=\infty$ (if the signal has nonzero values at time indices less than zero) and $z=0$ (if the signal has nonzero values at time indices greater than zero).
--if the signal is of infinite duration as $n$ approaches $\infty (but not $-\infty$), then the region of convergence will extend outside some disk (e.g., $|z|$\gt 1)
--if the signal is of infinite duration as $n$ approaches $-\infty (but not $\infty$), then the region of convergence will be inside some disk (e.g., $|z|$\lt 1)
--if the signal is of infinite duration as $n$ approaches both $\infty$ and $-\infty, then the region of convergence--if it exists--will be an annulus (e.g., $1\lt |z|\lt 2$)
</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A1390e34e-6790-4c09-bf0c-bb99998e86ad%401.html" data-type="page"><h1>The Transfer Function of Discrete-Time LTI Systems</h1><div data-type="document-title">The Transfer Function of Discrete-Time LTI Systems</div>
  <p id="delete_me"><span data-type="title">LTI Systems in the Time Domain</span>One of the most general ways of defining an LTI system is by expressing its output at some time $n$ as a weighted sum of values of the input and output at a finite number various other times. If all those times are at or prior to $n$, such systems a system is causal. In terms of a difference equation, the system definition would look something like this:
$y[n] ~=~ b_0 x[n] + b_1 x[n-1] + b_2 x[n-2] + \cdots + b_M x[n-M]
-a_1 y[n-1] - a_2 y[n-2] + \cdots - a_N y[n-N]$
This kind of system can also be displayed as a block diagram:
<figure id="filter"><figcaption>CAPTION.</figcaption><span data-type="media" id="filter-plot" data-alt="Image">
<img src="/resources/dafc15b36fa6b557803956ecdbd302ab54e2022f/filter.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
In the block diagram, the $z^{-1}$ blocks represent time delays, the triangles represent multiplications, and the $\Sigma$ blocks represent summations of the signals being input into them. The output at time $n$ is a weighted sum of the previous $M$ input values (along with the current value) and previous $N$ output values.</p><p id="eip-480"><span data-type="title">LTI Systems in the z-Domain</span>Consider again the input/output relationship of a causal LTI system:
$y[n] ~=~ b_0 x[n] + b_1 x[n-1] + b_2 x[n-2] + \cdots + b_M x[n-M]
-a_1 y[n-1] - a_2 y[n-2] + \cdots - a_N y[n-N]$
We can take the z-transform of this whole equation, bearing in mind that the transform is a linear operation and that delays of $n_0$ in time correspond to multiplications by $z^{-n_0}$ in the z-domain. That would leave us with this:
$Y(z)=~ b_0 X(z) + b_1 z^{-1} X(z) + b_2 z^{-2} X(z) + \cdots b_M z^{-M} X(z)- a_1 z^{-1} Y(z) - a_2 z^{-2} Y(z) - \cdots - a_N z^{-N} Y(z)$
From there we can do a little rearranging:
$\begin{align*}
Y(z)&amp;=b_0 X(z) + b_1 z^{-1} X(z) + b_2 z^{-2} X(z) + \cdots b_M z^{-M} X(z)- a_1 z^{-1} Y(z) - a_2 z^{-2} Y(z) - \cdots - a_N z^{-N} Y(z)\\
Y(z)+ a_1 z^{-1} Y(z) a_2 z^{-2} Y(z) + \cdots + a_N z^{-N} Y(z)&amp;=b_0 X(z) + b_1 z^{-1} X(z) + b_2 z^{-2} X(z) + \cdots b_M z^{-M}X(z)\\
Y(z)(1+a_1 z^{-1} + a_2 z^{-2}  + \cdots + a_Nz^{-N})&amp;=X(z)(b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M})\\
H(z)=\frac{Y(z)}{X(z)}&amp;=\frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}}{1+a_1 z^{-1} + a_2 z^{-2}  + \cdots + a_Nz^{-N}}
\end{align*}$
This $H(z)$, which is a rational function in terms of $z$, is called the system's <span data-type="term">transfer function</span>.</p><p id="eip-236"><span data-type="title">The Transfer Function: Poles and Zeroes, ROC, and Stability</span>So we see that the transfer function for one of the most common classes of LTI systems--in which the output is a weighted sum of a finite number of past inputs and outputs--is a rational function in terms of $z$:
$H(z)=\frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}}{1+a_1 z^{-1} + a_2 z^{-2}  + \cdots + a_Nz^{-N}}$
We can express this function in terms of positive powers of $z$ by simple factoring, and then according to the fundamental theorem of algebra, we can represent the numerator and denominator polynomials with respect to their roots:
$\begin{align*}
H(z)&amp;=\frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}}{1+a_1 z^{-1} + a_2 z^{-2}  + \cdots + a_Nz^{-N}}\\
&amp;=\frac{z^{-M}(b_0z^M+b_1z^{M-1}+b_2z^{M-2}+\cdots+b_M)}{z^{-N}(z^N+a_1z^{N-1}+a_2z^{N-2}+\cdots+a_N)}\\
&amp;=\frac{z^{-M}b_0(z-\zeta_1)(z-\zeta_2)\cdots(z-\zeta_M)}{z^{-N}(z-p_1)(z-p_2)\cdots(z-p_N)}
\end{align*}$
The values of $z$ for which the numerator is zero are called <span data-type="term">zeros</span>, and the values for which the denominator is zero are called <span data-type="term">poles</span>.</p><p id="eip-601">Now, there is a special reason the transfer function is given the notation $H(z)$: it happens to be the z-transform of $h[n]$, the impulse response of the system. Like all z-transforms, it has a region of convergence. We have seen previously that the ROC for a z-transform will be one of the following:
--no z (the z-transform does not converge for any z)
--the entire z plane (with the possible exception of z=0 or z=\infty)
--all z outside a certain disc (e.g., $|z|\gt 1$)
--all z within a certain disc (e.g., $|z|\lt 1$)
--z within an annulus (e.g., $1\lt|z|\lt 2$)

Poles play a very significant role in the ROC for a transfer function; in fact, they completely define the ROC. The ROC of a z-transform cannot contain a pole (for the function would not exist at that point), which means that once we know the poles of a system, we know the potential ROCs. We say POTENTIAL because there are a variety of possible ROCs for a transfer function with one ore more poles; to be precice, if there are $M$ disctinct poles, then there will be $M$+1 possible ROCs: the disc within the pole of smallest magnitude, the region outside the disc or radius equal to the largest pole, and then the $M$-1 annuli of regions between adjacent poles. If, suppose, the system in question has 2 poles, then there will be 3 possible regions of convergence:
<figure id="ROCs" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="ROC0-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="ROC0" data-alt="Image">
<img src="/resources/694bf2e55ea62ae1c9328cd5ed577c93442dfd32/ROC0.png" data-media-type="image/png" alt="Image">
</span>
</figure>
<figure id="ROC1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="ROC1" data-alt="Image">
<img src="/resources/46cc2b6987bec80781927c794c9d990975a9cd98/ROC1.png" data-media-type="image/png" alt="Image">
</span>
</figure><figure id="ROC2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="ROC2" data-alt="Image">
<img src="/resources/1dd64e1c518de44e5345c9f80a3a6a8835ac81d2/ROC2.png" data-media-type="image/png" alt="Image">
</span>
</figure><figure id="ROC3-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="ROC3" data-alt="Image">
<img src="/resources/e11f138381343881d36e6137152b683ac5ca227f/ROC3.png" data-media-type="image/png" alt="Image">
</span>
</figure>
</figure>
As we have also seen previously, each type of ROC corresponds to a particular kind of discrete-time signal. ROCs which are a disc correspond to signals with a non-zero duration extending to $n=-\infty$, those that are all $z$ outside of a certain disc correspond to signals with a non-zero duration extending to $n=-\infty$, and those that are an annulus correspond to signals whose non-zero duration extends both to $n=-\infty$ and $n=\infty$. For the class of discrete-time LTI system that we have been just considering, the impulse response is causal, and thus if it is infinite, will have a duration that extends towards $n=\infty$. Thus the transfer function of such a system has a ROC that extends outward from the outermost pole.
</p><div data-type="example" class="example" id="eip-26"><div data-type="title" class="title">Finding the Transfer Funcion, Zeros and Poles, and ROC</div><p id="eip-973">Consider a discrete-time system represented by this block diagram:
<figure id="filterEx"><figcaption>CAPTION.</figcaption><span data-type="media" id="filterEx-plot" data-alt="Image">
<img src="/resources/97ef6ff2460810ead2a93d5d014ac085d5c0aabf/filterEx.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
We would like to find its transfer function, along with its poles and zeros. The first step will be to express the input/output relationship in the time domain. From there we can take the z-transform of each side and find $\frac{Y(z)}{X(z)}$, which is by definition the transfer function:
$\begin{align*}
y[n]&amp;=x[n]+3x[n-1]+\frac{11}{4}x[n-2]+\frac{3}{4}x[n-3]+y[n-1]-\frac{1}{2}y[n-2]\\
y[n]-y[n-1]+\frac{1}{2}y[n-2]&amp;=x[n]+3x[n-1]+\frac{11}{4}x[n-2]+\frac{3}{4}x[n-3]\\
Y(z)-z^{-1}Y(z)+\frac{1}{2}z^{-2}Y(z)&amp;=X(z)+3z^{-1}X(z)+\frac{11}{4}z^{-2}X(z)+\frac{3}{4}z^{-3}X(z)\\
Y(z)(1-z^{-1}+\frac{1}{2}z^{-2})&amp;=X(z)(1+3z^{-1}+\frac{11}{4}z^{-2}+\frac{3}{4}z^{-3})\\
H(z)=\frac{Y(z)}{X(z)}&amp;=\frac{1+3z^{-1}+\frac{11}{4}z^{-2}+\frac{3}{4}z^{-3}}{1-z^{-1}+\frac{1}{2}z^{-2}}\\
&amp;=\frac{z^{-3}(z^3+3z^{2}+\frac{11}{4}z+\frac{3}{4})}{z^{-2}(z^2-z+\frac{1}{2})}\\
&amp;=\frac{(z+1)(z+\frac{1}{2})(z+\frac{3}{2})}{z(z-\frac{1}{2}-\frac{j}{2})(z-\frac{1}{2}+\frac{j}{2})}
\end{align*}$
So, having found the transfer function and factored the numerator and denominator, we see readily that the zeros of the system are $z=-1/2$, $-1$, $-3/2$, and the poles for the system are $z=0$, $1/2+j/2$, $1/2-j/2$. As the system is causal (from the block diagram we see the output is dependent only on present and/or past values of the input and output), the ROC will extend outward from the outermost pole. The two nonzero poles are equidistant from the origin, at a distance of $\frac{\sqrt{2}}{2}$, so the ROC is $|z|\gt \frac{\sqrt{2}}{2}$. A pole/zero plot, with the ROC shaded, is below:
<figure id="filterExpz"><figcaption>CAPTION.</figcaption><span data-type="media" id="filterExzp-plot" data-alt="Image">
<img src="/resources/3166eade6ac016b470b6dbb0ccd3ccc674f981ab/filterExzp.png" data-media-type="image/png" alt="Image" width="300">
</span></figure></p></div><p id="eip-742"><span data-type="title">From the Transfer Function to the Frequency Response</span>We have seen that for LTI systems whose output is a weighted sum of a finite number of past inputs and outputs, the transfer function amounts to being a rational function of the form
$H(z)=\frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}}{1+a_1 z^{-1} + a_2 z^{-2}  + \cdots + a_Nz^{-N}}$
or equivalently,
$\frac{z^{-M}b_0(z-\zeta_1)(z-\zeta_2)\cdots(z-\zeta_M)}{z^{-N}(z-p_1)(z-p_2)\cdots(z-p_N)}$
One of the benefits in expressing the transfer function as the products of those single order polynomials is the elegant way it leads to understanding the magnitude of the transfer function:
$|H(z)|=\frac{|z^{-M}||b_0||(z-\zeta_1)||(z-\zeta_2)|\cdots|(z-\zeta_M)|}{|z^{-N}||(z-p_1)||(z-p_2)|\cdots|(z-p_N)|}$
Note what terms like $|z-p|$ represent. The magnitude is $\sqrt{(Re(z)-Re(p))^2+(Im(z)-Im(p))^2}$ which is simply the euclidean distance between $z$ and $p$ on the complex plane. So the magnitude of the transfer function at some point $z$ essentially amounts to being the products of the distances between $z$ and the system's zeros, divided by the products of the distances between $z$ and the system's poles. The closer $z$ is to a zero, the smaller the magnitude of the transfer function will be; the closer $z$ is to a pole, the larger the magnitude of the transfer function will be.
</p><p id="eip-42">Now recall that a system's frequency response, i.e. the amount the system scales the input at particular frequencies, is simply the z-transform evaluated on the unit circle, for $H(e^{j\omega})=H(z)|_{z=e^{j\omega}}$. If we have a pole-zero plot, we can quickly visualize the magnitude of the system's frequency response by traversing around the unit circle. As we go, the closer we are to a zero, the smaller it will be, while the closer we are to a pole, the larger it will be. As a result of this phenomenon (and the fact that the transfer function is determined by the pole and zero locations), we can see that the locations of a system's poles and zeros in the z-plane totally characterize a system (within a scaling factor). Thus, knowing the poles and zeros of a system is essentially equivalent to knowing exactly what kind of system it is.</p><div data-type="example" class="example" id="eip-65"><div data-type="title" class="title">Visualizing the Magnitude of the Frequency Response</div><p id="eip-686">Consider the pole-zero plot from the example above, with a unit circle overlaid:
<figure id="filterExzpfr"><figcaption>CAPTION.</figcaption><span data-type="media" id="filterExzpfr-plot" data-alt="Image">
<img src="/resources/d876e1d7a2a432352451e438dd37771e216127cd/filterExzpfr.png" data-media-type="image/png" alt="Image">
</span></figure>
The frequency response of this system is simply value of the transfer function, evaluated along this unit circle. The magnitude of the transfer function is the product of the distances to the zeros, divided by the product of the distances to the poles. By traversing along the unit circle and noting these distances, we can visualize what the magnitude of the frequency response will be.

We will start at $\omega=0$. Here we can easily find the value of the magnitude, since $H(e^{j0})=H(1)$. From the transfer function we found in the first example, we have $H(1)=\frac{(1+3+\frac{11}{4}+\frac{3}{4})}{(1-1+\frac{1}{2})}=15$. So $|H(e^{j\omega})$ at $\omega=0$ is $15$. Now, as we move counter-clockwise around the circle (i.e., from $\omega=0$ to $\omega=\pi/2$ to $\omega=\pi$), note what happens. As $\omega$ approaches $\pi/4$ on the unit circle, it is drawing nearer and nearer to a pole; therefore, the magnitude of the frequency response is going to increase as $\omega$ progresses from $0$ to $\pi/4$. At that point, we will be travelling away from that pole, and nearer to the three zeros; accordingly, the magnitude of the frequency response will decrease. Eventually we will actually land squarely on a zero at $\omega=\pi$; this is obviously as close as we could possibly get to a zero. As the distance to the zero at that point is zero, the magnitude of the frequency response there is zero. We can repeat this process moving clockwise from $\omega=0$ to $\omega=-\pi$ and the magnitude will change in the precisely same way, due to the symmetry. Having moved along the entire unit circle, we can see how our intuition matches what is indeed the magnitude of the frequency response, found via plotting software:
<figure id="Hfilter"><figcaption>CAPTION.</figcaption><span data-type="media" id="Hfilter-plot" data-alt="Image">
<img src="/resources/6a57d3a96af9c9a4d814035494afa648b99124fb/Hfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/40b95efd7d1c6c9c151ef3c0c967e1277ffb7a6a/Hfilter.eps" data-type="image"></span>
</span></figure>
Note that the magnitude is $15$ at $\omega=0$, then increases to a peak at about $\pm\pi/4$, and then decreases to $0$ as $\omega$ traverses from $\pm\pi/4$ to $\pm\pi$.</p></div><p id="eip-44"><span data-type="title">The Transfer Function ROC, and Stability</span>It has been explained previously that the region of convergence of a z-transform are the $z$ for which the sum $\sum x[n] z^{-n}$ converges. There is an alternative, though related definition that is also often used in signal processing, that the region of convergence are $z$ for which the sum $\sum x[n] z^{-n}$ converges ABSOLUTELY, i.e. that $\sum |x[n] z^{-n}|$. For virtually all of the signals that we will be considering, those two definitions are equivalent. The reason the second definition is often used is because of a relationship it establishes with the stability of the system.

Suppose that a system's transfer function $H(z)$ has a particular ROC (defined in the absolutely summable sense). If the unit circle ($|z|=1$) is contained by this ROC, then the system is BIBO stable. To prove this fact, we see simply that the $|z|=1$ being in the ROC means that $\sum |h[n]e^{j\omega}|$ converges for all $\omega$, and then note that for $\omega=0$, the convergent sum is $\sum |h[n]|$. Previously we proved that for LTI systems, $\sum |h[n]|$ converging is equivalent to the system being BIBO stable.</p><p id="eip-964">So if the unit circle is in a transfer function's ROC, then that system is BIBO stable. This is a helpful way to determine stability. We do not need to prove that an arbitrary bounded input will produce a bounded output, nor find the impulse response and determine its absolute summability; we simply need to know the transfer function's ROC.

If the system in question is causal (which is the case for most all practical systems of interest), then the ROC will extend outwards from the outermost pole. This means that, for causal systems, the location of the poles also determine BIBO stability. If all of a causal system's poles lie within the unit circle, then the ROC will extend to include the unit circle, and hence the system will be BIBO stable.</p><div data-type="example" class="example" id="eip-944"><div data-type="title" class="title">Stability and the Pole/Zero Plot</div><p id="eip-860">Consider again the pole/zero plot from the system in the previous examples:
<figure id="filterExzpfr2"><figcaption>CAPTION.</figcaption><span data-type="media" id="filterExzpfr2-plot" data-alt="Image">
<img src="/resources/d876e1d7a2a432352451e438dd37771e216127cd/filterExzpfr.png" data-media-type="image/png" alt="Image">
</span></figure>
Since the ROC contains the $|z|=1$ unit circle, this system is BIBO stable.</p></div><p id="eip-353"><span data-type="title">Poles/Zeros and FIR/IIR Systems</span>We have seen that the poles and zeros of a system characterize it, which of course includes the nature of its frequency response and whether or not the system is BIBO stable. Given how significant the poles and zeros of a system are, it should come as no surprise that knowing about the poles and zeros also allows us to determine whether the system is FIR or IIR.

Put simply, if a system's transfer function has any poles (other than at $z=0$), then the system is IIR. If it has only zeros, then it is FIR. This follows from what we saw with regard to the nature of different kinds of ROCs. If the ROC is the entire $z$ plane (with the possible exception of $z=0$), then the impulse response is of finite duration; hence the system is FIR. But if there are any non-zero poles on the plane, than the ROC will either be a disk, an annulus, or will extend outside of a disk; in each of those cases, the impulse response is of infinite duration.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3Abe2b609c-e531-4fd5-8d1b-0d952acf6956%401.html" data-type="page"><h1>z-Transform Properties</h1><div data-type="document-title">z-Transform Properties</div>
  <p id="delete_me">As with other signal transforms, the z-transform has a number of significant properties, including ways in which changes to a signal in one domain impacts its representation in another domain. To examine these properties, we will again use the notation of a z-transform "pair." If we have
$X(z)=\sum\limits_{n=-\infty}^{\infty}x[n]z^{-n}$,
then we express the relationship between $x[n]$ and its z-transform $X(z)$ as
$x[n]\leftrightarrow X(z)$</p><p id="eip-109"><span data-type="title">The z-Transform and the DTFT</span>The DTFT is a special case of the z-transform. If we have $x[n]\leftrightarrow X(z)$ and the ROC of $X(z)$ contains the unit circle, then the DTFT of $x[n]$, $X(e^{j\omega})$, is simply the z-transform $X(z)$ evaluated on the unit circle $z=e^{j\omega}$:
$X(e^{j\omega})=X(z)|_{z=e^{j\omega}}$</p><p id="eip-274"><span data-type="title">The z-Transform is Linear</span>Given that the z-transform is simply an infinite sum, if follows then that it will be a linear operator. If we have $x_1[n]\leftrightarrow X_1(z)$ and $x_2[n]\leftrightarrow X_2(z)$, and $\alpha_1,\alpha_2\in C$, then:
$\alpha_1 x_1[n]+\alpha_2 x_2[n]\leftrightarrow \alpha_1 X_1(z)+\alpha_2 X_2(z)$
However, the ROC of the new transform is the intersection of those of $X_1(x)$ and $X_2(z)$: $ROC_{\alpha_1 X_1+\alpha_2 X_2}=ROC_{X_1}\bigcap ROC_{X_2}$</p><p id="eip-755"><span data-type="title">The z-Transform and Shifts in Time</span>If $x[n]$ and $X(z)$ are a z-transform pair, then:
$x[n-m]\leftrightarrow z^{-m}X(z)$
and the ROC remains the same as for $X(z)$ (with the possible addition/removal of a pole at zero and/or zero at infinity):
$\begin{align*}
\sum_{n=-\infty}^{\infty} x[n-m]\,  z^{-n}&amp;= \sum_{r=-\infty}^{\infty} x[r]\,  z^{-(r+m)}\\
&amp;=\sum_{r=-\infty}^{\infty} x[r]\,  z^{-r} \, z^{-m} \\
&amp;=z^{-m} \sum_{r=-\infty}^{\infty} x[r]\,  z^{-r}\\
&amp;=z^{-m} X(z)
\end{align*}$</p><p id="eip-170"><span data-type="title">The z-Transform and Modulation</span>If $x[n]$ and $X(z)$ are a z-transform pair, then:
$z^n_0 x[n]\leftrightarrow X(\frac{z}{z_0})$
To determine the ROC of the new z-transform, substitute $\frac{z}{z_0}$ for $z$ in the original ROC definition, and simplify. For example, if the ROC of $X(z)$ is $|z|\gt 1$, then the new ROC will be $|\frac{z}{z_0}|\gt 1 \rightarrow |z|\gt |z_0|$
Proof:
$\begin{align*}
\sum_{n=-\infty}^{\infty} (z_0^n x[n])  z^{-n} &amp;=\sum_{n=-\infty}^{\infty} x[n](z/z_0)^{-n}\\
&amp;=X\left(\frac{z}{z_0}\right)\end{align*}$</p><p id="eip-528"><span data-type="title">The z-Transform and Complex Conjugation</span>If $x[n]$ and $X(z)$ are a z-transform pair, then:
$x^*[n] \leftrightarrow} X^*(z^*)$
Proof:
$\begin{align*}
\sum_{n=-\infty}^{\infty} x^*[n] z^{-n}&amp;=\left(\sum_{n=-\infty}^{\infty} x[n] (z^*)^{-n}\right)^*\\
&amp;=X^*(z^*)
\end{align*}$</p><p id="eip-419"><span data-type="title">The z-Transform and Time Reversal</span>If $x[n]$ and $X(z)$ are a z-transform pair, then:
$x[-n]\leftrightarrow}X(z^{-1})$
With the ROC inverted: to find the ROC, substitute $\frac{1}{z}$ for $z$ in the original ROC expression and simplify.
Proof:
$\begin{align*}
\sum_{n=-\infty}^{\infty} x[-n] z^{-n}&amp;=\sum_{m=-\infty}^{\infty} x[m]z^{m}\\
&amp;=\sum_{m=-\infty}^{\infty} x[m] (z^{-1})^{-m}\\
&amp;=X(z^{-1})
\end{align*}$</p><p id="eip-925"><span data-type="title">The z-Transform and Convolution</span>If $x[n]\leftrightarrow X(z)$, $h[n]\leftrightarrow H(z)$, and $y[n]\leftrightarrow Y(z)$ all all z-transform pairs, and $y[n]=x[n]\ast h[n]$, then:
$Y(z)=X(z)H(z)$,
with the ROC of $Y(z)$ being the intersection of the ROCs of $X(z)$ and $H(z)$.</p><p id="eip-59"><span data-type="title">Common z-Transform Pairs</span>$\begin{array}{c|c|c}
x[n] &amp; X(z) &amp; ROC \\
\hline \\
\delta[n] &amp; 1 &amp; all z\\
u[n] &amp; \frac{1}{1-z^{-1}} &amp; |z|\gt 1 \\
\alpha^n u[n] &amp; \frac{1}{1-\alpha z^{-1}} &amp; |z|\gt|\alpha| \\
-\alpha^n u[-n-1] &amp; \frac{1}{1-\alpha z^{-1}} &amp; |z|\lt|\alpha|
\end{array}$</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A439e30e4-02d6-4e39-98c2-da8295ce4ed8%401.html" data-type="page"><h1>The Inverse z-Transform</h1><div data-type="document-title">The Inverse z-Transform</div>
  <p id="delete_me">The z-transform for a signal $x[n]$ has been defined as:
$X(z)=\sum\limits_{n=-\infty}^{\infty}x[n]z^{-n}$
As with other transforms such as the DFT and DTFT, we would like to recover $x[n]$ from its transform. The technical definition for this inverse transform is:
$x[n] = \oint_{\mathcal{C}} X(z) z^n \frac{dz}{j 2\pi z}$
Evaluation of this contour integral requires knowledge of complex analysis, but there is another way to find the inverse transform, which we will now consider.</p><p id="eip-126"><span data-type="title">Inverse Transform by Inspection</span>In order to find the inverse z-transform, we do not necessarily need to calculate anything; rather, we will seek to represent the z-transform in terms of entities for which we know the z-transform pair. For example, we know that the z-transform of $\alpha^n u[n]$ is $\frac{1}{1-\alpha z^{-1}}~,~|z|\gt\alpha$. So if we can represent some signal's z-transform into something that looks like $\frac{1}{1-\alpha z^{-1}}$, we will be on the right track to finding the inverse.</p><p id="eip-906">Key to this process is the fact that if a z-transform is rational (which is the case for virtually all of our cases of interest), then its numerator and denominator can be factored according to its poles and zeros. This means that an $X(z)$ of the form:
$X(z)=\frac{b_0+b_1z^{-1}+b_2z^{-2}+\cdots+b_Mz^{-M}}{1+a_1z^{-1}+a_2z^{-2}+\cdots+a_Nz^{-N}}$
can equivalently be expressed as:
$X(z)=z^{N-M} \frac{(1-\zeta_1 z^{-1})(1-\zeta_2 z^{-1}) \cdots (1-\zeta_M z^{-1})}
{(1-p_1 z^{-1})(1-p_2 z^{-1}) \cdots (1-p_N z^{-1})}$
Note how this expression is starting to look similar to $\frac{1}{1-\alpha z^{-1}}$. We can do a little more work on $X(z)$ to help with things even more. A rational function can be expressed in terms of products of $\frac{1}{1-\alpha z^{-1}}$ terms, but even better, it can also be represented as a sum of such terms. Therefore, any $X(z)$ of the form
$X(z)=z^{N-M} \frac{(1-\zeta_1 z^{-1})(1-\zeta_2 z^{-1}) \cdots (1-\zeta_M z^{-1})}
{(1-p_1 z^{-1})(1-p_2 z^{-1}) \cdots (1-p_N z^{-1})}$
can be re-arranged to be expressed as
$X(z)=C_0+\frac{C_1}{1-p_1 z^{-1}}+\frac{C_2}{1-p_2 z^{-1}}+\cdots+\frac{C_N}{1-p_N z^{-1}}$
Now, this is something we can work with. Using the linearity property of the z-transform, we can take the inverse transform of that whole expression to yield (assuming the ROC of $X(z)$ extends outwards from the outermost pole)
$x[n]=C_0\delta[n]+C_1 p_1^n u[n]+C_2 p_2^n u[n]+\cdots+C_N p_N^n u[n]$</p><p id="eip-998"><span data-type="title">Partial Fractions</span>Of course, we need to somehow find those $C$ constants to be able to know the inverse transform. The process of finding 
$X(z)=C_0+\frac{C_1}{1-p_1 z^{-1}}+\frac{C_2}{1-p_2 z^{-1}}+\cdots+\frac{C_N}{1-p_N z^{-1}}$
from
$X(z)=\frac{b_0+b_1z^{-1}+b_2z^{-2}+\cdots+b_Mz^{-M}}{1+a_1z^{-1}+a_2z^{-2}+\cdots+a_Nz^{-N}}$
is known as a partial fraction decomposition.</p><p id="eip-83">The first step of the partial fraction decomposition is to make sure the order of the denominator polynomial is greater than that of the numerator. If not, then we first need to carry out a polynomial long division and then continue with the remainder. For example, suppose
$X(z)=\frac{5 +2z^{-1} -z^{-2}}{1 -\frac{1}{6} z^{-1} -\frac{1}{6}z^{-2}}~~,~|z|\gt\frac{1}{2}$
The long division is a single step:
\begin{align*}
&amp;~~~~~~~~~~~~~~~~~6\\
1 -\frac{1}{6} z^{-1}-\frac{1}{6}z^{-2}~~&amp;\overline{\smash{)}5 +2z^{-1} -z^{-2}}\\
&amp; \underline{~6-1z^{-1}-z^{-2}}\\
&amp;-1+3z^{-1}
\end{align*}$
So $X(z)=\frac{5 +2z^{-1} -z^{-2}}{1 -\frac{1}{6} z^{-1} -\frac{1}{6}z^{-2}}=6+\frac{-1+3z^{-1}}{1 -\frac{1}{6} z^{-1} -\frac{1}{6}z^{-2}}$
We can now work on that second term, as the denominator's order is greater than the numerator. The next step, then, will be to factor the denominator:
$\frac{-1+3z^{-1}}{1 -\frac{1}{6} z^{-1} -\frac{1}{6}z^{-2}}=\frac{-1+3z^{-1}}{(1-\frac{1}{2}z^{-1})(1+\frac{1}{3}z^{-1})}$
From here, the next step is to express that in terms of a sum:
$\frac{-1+3z^{-1}}{(1-\frac{1}{2}z^{-1})(1+\frac{1}{3}z^{-1})}=\frac{C_1}{1-\frac{1}{2}z^{-1}}+\frac{C_2}{1+\frac{1}{3}z^{-1}}$
The above form assumes that each of the poles is unique; if a pole is repeated, e.g. $\frac{1}{(1-az^{-1})^2}$, then that single term in the product would correspond to two in the sum, $\frac{C_1}{1-az^{-1}}+\frac{C_{1'}}{(1-az^{-1})^2}.

At this point, we simply solve for the $C$ coefficients:
$\begin{align*}
\frac{-1+3z^{-1}}{(1-\frac{1}{2}z^{-1})(1+\frac{1}{3}z^{-1})}&amp;=\frac{C_1}{1-\frac{1}{2}z^{-1}}+\frac{C_2}{1+\frac{1}{3}z^{-1}}\\
-1+3z^{-1}&amp;=C_1(1+\frac{1}{3}z^{-1})+C_2(1-\frac{1}{2}z^{-1})\\
-1+3z^{-1}&amp;=(C_1+C_2)+z^{-1}(\frac{1}{3}C_1-\frac{1}{2}C_2)\\
C_1+C_2=-1, &amp;~\frac{1}{3}C_1-\frac{1}{2}C_2=3\rightarrow C_1=3~,~C_2=-4
\end{align*}$

Given these coefficients, we now have that
$X(z)=6+\frac{3}{1-\frac{1}{2}z^{-1}}-\frac{4}{1+\frac{1}{3}z^{-1}}~~,~|z|\gt\frac{1}{2}$</p><p id="eip-55">Having split the transform into a sum through partial fractions, the inverse transform is now straightforward because (due to the linearity of the z-transform) we can take the inverse of each piece of the sum. So for:
$X(z)=6+\frac{3}{1-\frac{1}{2}z^{-1}}-\frac{4}{1+\frac{1}{3}z^{-1}}~~,~|z|\gt\frac{1}{2}$
The inverse transform of $1$ is $\delta[n]$, the inverse of $\frac{1}{1-\frac{1}{2}z^{-1}}$ is $(\frac{1}{2})^n u[n]$, and the inverse of $\frac{1}{1+\frac{1}{3}z^{-1}}$ is $(\frac{-1}{3})^n u[n]$. For the latter two terms the ROC of the transform is must-know information; if the ROC extended inwards instead of outwards, the inverses would have been left-sided instead of right-sided. Putting all of this together through linearity, we have that the inverse transform is:
$x[n]=6\delta[n]+3(\frac{1}{2})^n u[n]-4(\frac{-1}{3})^n u[n]$.
<figure id="xex"><figcaption>CAPTION.</figcaption><span data-type="media" id="xex-plot" data-alt="Image">
<img src="/resources/738315641957e6ee0b03ad077d96fdbca6b035c6/xex.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/260a5d20f152b85ff18219d304f95dceb46dfe9f/xex.eps" data-type="image"></span>
</span></figure></p></div></li></ul></li><li><div data-type="page"><h1>Discrete-Time Filters</h1></div><ul><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A941c71be-ae4a-4f23-8cee-462506b4d564%401.html" data-type="page"><h1>Discrete-Time Filtering and Filter Design</h1><div data-type="document-title">Discrete-Time Filtering and Filter Design</div>
  <p id="delete_me">Having considered discrete-time signal types, systems and their properties, and various signal transformations that can assist with analysis, we are ready to put this knowledge to use. After all, signals and systems are not just interesting mathematical entities-- they are actually useful! Signals represent real-world information, and systems can perform a variety of important modifications on them.</p><p id="eip-255"><span data-type="title">Filtering: LTI Systems in the Frequency Domain</span>Before we can design a filter to meet some kind of desired specification, we first need to remind ourselves of what LTI systems can do. We will explain filtering from the standpoint of infinite-length signals, but the principles apply similarly to finite-length signals as well. Recall these two important facts: first, that any discrete-time signal can be represented as an integral of complex sinusoids, and second, that complex sinusoids are eigenvectors of LTI systems. Putting those facts together, we see that LTI can be understood as taking in discrete-time signals as inputs, and then modifying them by scaling their frequency components. The amount the system scales each frequency component $\omega$ is the system's frequency response $H(\omega)$. The frequency response is itself simply the DTFT of the system's impulse response $h[n]$.</p><p id="eip-821">So, LTI systems work by taking in input signals and scaling each input frequency according to the system's frequency response in order to produce the output. In this way, they can be thought of as being (infinitesimally) finely scaled versions of a graphic equalizer, which modifies input music signals according to frequency bands.
<figure id="graphiceq"><figcaption>The filtering operation of discrete-time LTI systems is similar to the frequency tuning of musical signals with a graphic equalizer. [https://www.flickr.com/photos/12577725@N05/6103827986] Yamaha EQ-500 by Touho_T, used under [http://creativecommons.org/licenses/by/2.0/] CC BY / cropped from original.</figcaption><span data-type="media" id="graphiceq-plot" data-alt="Image">
<img src="/resources/a1eb8d22812f0a8f65f7b3e947742a5ee28c61e4/graphiceq.jpg" data-media-type="image/jpg" alt="Image" width="400">
</span></figure></p><p id="eip-914">Since there is an infinite number of discrete-time frequencies $\omega$ (as it is a real number between $-\pi$ and $\pi$), and each one can be scaled in an infinite number of ways (since the scaling factor is any complex number), there are obviously an infinite number of potential filters/systems. That said, filters are usually categorized as being one of four different classes.</p><p id="eip-469"><span data-type="title">Filter Types</span>The most common filter types are named according to the signal frequency components they either stop (scale by zero) or allow to pass. The magnitudes of their frequency response and their corresponding impulse responses are plotted below.
<figure id="LPfigs" data-orient="vertical"><figcaption>The frequency response magnitude and impulse response of a <span data-type="term">low-pass filter</span>. Note how the lowest frequencies (those near to $\omega=0$) are scaled by a (relatively) constant value, while the highest frequencies (those near $\omega=\pm\pi$) are attenuated.</figcaption><figure id="LPfilter-plot">
<span data-type="media" id="LPfilter" data-alt="Image">
<img src="/resources/f8efc6f22e8e0715c3e77fb9fb55252aa96fccbe/LPfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/e0499effd7499770dc129011dc4b8ceee032e5b1/LPfilter.eps" data-type="image"></span></span>
</figure>
<figure id="hLP-plot">
<span data-type="media" id="hLP" data-alt="Image">
<img src="/resources/c9685e890f583eef1345b61895409548afe66e61/hLP.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d0caa27d27d4e21033dbdb521c73cf55ec3684cd/hLP.eps" data-type="image"></span></span>
</figure>
</figure>
<figure id="HPfigs" data-orient="vertical"><figcaption>A <span data-type="term">high-pass filter</span> does the opposite of a low-pass filter; it attenuates lower frequencies and allows higher frequencies to pass.</figcaption><figure id="HPfilter-plot">
<span data-type="media" id="HPfilter" data-alt="Image">
<img src="/resources/12e85f8ac58e099ded37e91d3cc01f85a85c739e/HPfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f8d41e6d5bd23396619395949124459a1d8795be/HPfilter.eps" data-type="image"></span></span>
</figure>
<figure id="hHP-plot">
<span data-type="media" id="hHP" data-alt="Image">
<img src="/resources/16c370322da5814d488c73092157fb7c0f0ccc5d/hHP.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1c193fa4dcef489f39508046559aba1b91d19805/hHP.eps" data-type="image"></span></span>
</figure>
</figure>
<figure id="BPfigs" data-orient="vertical"><figcaption>Plotted above are the frequency response magnitude and impulse response of a <span data-type="term">band-pass filter</span>. These filters allow a narrow (contiguous) band of frequencies to pass (here, those near $\omega=\pm\frac{\pi}{2}$) while attenuating the rest.</figcaption><figure id="BPfilter-plot">
<span data-type="media" id="BPfilter" data-alt="Image">
<img src="/resources/1de55b59037338ed15dfe5bffc36de9bae6040f9/BPfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2e201fd28e9fe8744032c2d52e7c888a1a81de40/BPfilter.eps" data-type="image"></span></span>
</figure>
<figure id="hBP-plot">
<span data-type="media" id="hBP" data-alt="Image">
<img src="/resources/ffddf84a05724012a13695aa9ce520a9d196ca3f/hBP.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/14832e4bc2bc941eb088f31b97374b8469a446b5/hBP.eps" data-type="image"></span></span>
</figure>
</figure>
<figure id="BSfigs" data-orient="vertical"><figcaption>A <span data-type="term">band-stop filter</span>, like the band-pass filter, also focuses on a narrow band of frequencies. But in this case, it attenuates that band, while allowing other frequencies to pass.</figcaption><figure id="BSfilter-plot">
<span data-type="media" id="BSfilter" data-alt="Image">
<img src="/resources/9679c852ed16544071fbfbfa0787d0babfefabd6/BSfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d909f8a8060d50b7b18941b229e615a6b8761003/BSfilter.eps" data-type="image"></span></span>
</figure>
<figure id="hBS-plot">
<span data-type="media" id="hBS" data-alt="Image">
<img src="/resources/ad1043ef6c628b82b785f74708e3712595348282/hBS.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/89691ce3b235f3d245d0c80c796a2b7bd9961f9d/hBS.eps" data-type="image"></span></span>
</figure>
</figure>
As can be seen in the frequency responses above, the names of the typical filter classes give a straightforward description of the filters' function. Note, though, that there may be some level of ambiguity between filter names. For example, a low-pass filter is also a type of band-pass filter (with low frequencies being the "band"), and for that matter is also a band-stop filter (the high frequencies being the band that is stopped). We could say the same thing about high-pass filters, as well. In practice, then, band-pass and band-stop filters typically are understood to pass or stop bands that are not about $0$ or $\pi$, but some other frequency. Additionally, the pass and stop bands of these two filters are typically much narrower than those for high-pass or low-pass filters.</p><p id="eip-789"><span data-type="title">Filter Design</span>Note the simplicity of the four filters plotted above; their impulse responses have only 2 or 4 non-zero values, and even those are either 1 or -1. In practice, discrete-time filters can have impulse response lengths that are much longer and more complex. The advantages of such filters is that their frequency pass- and stop- points are much sharper, with more level pass-bands. We can see this trade-off when comparing the frequency and impulse response of the low-pass filter above with the following "ideal" low-pass filter:
<figure id="idealLP" data-orient="vertical"><figcaption>The frequency response magnitude and impulse response of an <span data-type="term">ideal low-pass filter</span>.</figcaption><figure id="idealLPfilter-plot">
<span data-type="media" id="idealLPfilter" data-alt="Image">
<img src="/resources/f2714c68b4b5890658f8ca9af543f6a01346eef7/idealLPfilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1bf4258b187323a1e9fead3c43c0a5560bc344c2/idealLPfilter.eps" data-type="image"></span></span>
</figure>
<figure id="sinc1-plot">
<span data-type="media" id="sinc1" data-alt="Image">
<img src="/resources/8943e8b9947ec28b65cc44367f82f98f3d9bcee0/sinc1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/240a77101fb46776c0ca8575e1134a18dd2099d6/sinc1.eps" data-type="image"></span></span>
</figure>
</figure>
We can see why this filter is called "ideal." For all of the frequencies below a certain point $\omega_c$, the filter scales them by 1, meaning it does not change their magnitude at all; then, for all frequencies higher than $\omega_c$, it perfectly zeroes them out. Of course, this comes at a cost: the impulse response of this filter ($h[n]=2 \omega_c \frac{\sin(\omega_c n)}{\omega_c n}$) is infinitely long and is not causal (meaning the system needs to know future values of the input to calculate the current value of the output). Furthermore, because its transfer function is not a rational function of finite order, it has infinite complexity, and thus cannot be perfectly implemented in practice.</p><p id="eip-375">The task of filter design is to work with these two constraints, having a desirable frequency response without having an overly complex impulse response (which affects the complexity and cost of the filter's real-world implementation). The "desirability" of a filter's frequency response is typically expressed by giving certain constraints on its performance:
<figure id="filterspec"><figcaption>The performance constraints for filters are typically expressed as allowable deviations from $|H(\omega)|=1$ in the filter's pass-band (which is indicated by $\omega_p$) and from $|H(\omega)|=0$ in the stop-band (the "cutoff" of which is indicated by $\omega_c$).</figcaption><span data-type="media" id="filterspec-plot" data-alt="Image">
<img src="/resources/e3502062666e85abf9bbdb95a13d72e42c1db83d/filterspec.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
Within given filter design parameters, the simplest possible implementation is sought. There are two main approaches towards implementation, IIR and FIR; we will subsequently consider the characteristics, advantages, and disadvantages of each.</p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A8e1e5826-5004-4d75-8231-9e36db5682a6%401.html" data-type="page"><h1>IIR Filter Design</h1><div data-type="document-title">IIR Filter Design</div>
  <p id="delete_me"><span data-type="title">Review of IIR Systems</span>As discrete-time LTI filters are simply discrete-time LTI systems, they can be broadly classified--like systems--as being either FIR or IIR.
<figure id="filter"><figcaption>Implementation of a discrete-time LTI system.</figcaption><span data-type="media" id="filter-plot" data-alt="Image">
<img src="/resources/dafc15b36fa6b557803956ecdbd302ab54e2022f/filter.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
The figure above shows the implementation of any discrete-time LTI system; the way all such systems vary are simply in their order (the size of $N$ and $M$) and in their coefficients. Systems are IIR if they have any non-zero $a$ coefficients. If we see how such systems are represented in z-domain,
$\begin{align*}
H(z)= \frac{Y(z)}{X(z)} &amp;= \frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}}
{1 + a_1 z^{-1} + a_2 z^{-2} + \cdots + a_N z^{-N}}\\
&amp;= z^{N-M}\, \frac{(z-\zeta_1)(z-\zeta_2) \cdots (z-\zeta_M)}{(z-p_1)(z-p_2) \cdots (z-p_N)},
\end{align*}$
then we can also say that IIR systems are those which have (non-zero and non-infinite) poles.</p><p id="eip-760"><span data-type="title">IIR Filters</span>In discrete-time, the design and implementation of IIR filters is largely a matter of appropriating tried and tested design elements from continuous-time filters. Signal processing obviously predates the advent of discrete-time signals and systems, and before this time efforts were made to improve the design of analog filters. Theory eventually approached a point where high-performing filters could be created through a straightforward mathematical formulation. For discrete-time purposes, these filters can be simply translated from the Laplace (continuous-time) to the z domain through the use of some kind of transformation mapping (such as the bilinear transform, where $s=c\frac{z-1}{z+1}$).  Among the various analog filter types that were invented and then eventually employed for discrete-time use, we will consider low-pass filter implementations of three of the most significant types: Butterworth, Chebyshev, and Elliptical.</p><p id="eip-763"><span data-type="title">Butterworth Filter</span>The Butterworth filter's defining characteristic is its flatness in both its pass- and stop-bands.
<figure id="butters" data-orient="vertical"><figcaption>The impulse response, frequency response magnitude, and pole-zero plot of an order $N=6$ low-pass Butterworth filter.</figcaption><figure id="Butterh-plot">
<span data-type="media" id="Butterh" data-alt="Image">
<img src="/resources/934c590b3dab67e391beb7d00a898730f7b48c2b/Butterh.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/6d7587fd53f718f64cdf473883adc5d296229f43/Butterh.eps" data-type="image"></span></span>
</figure>
<figure id="ButterFilter-plot">
<span data-type="media" id="NAME" data-alt="Image">
<img src="/resources/44e14b766cfd7275dcc9145c9941af6e1f5a1946/ButterFilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/df3b4cfa3c1da6327b9904b3795a7d394b43dd41/ButterFilter.eps" data-type="image"></span></span>
</figure>
<figure id="ButterPZ-plot">
<span data-type="media" id="ButterPZ" data-alt="Image">
<img src="/resources/9d6c307bf55110b25193de26026ba82068c234a0/ButterPZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/241067f2cf43ccffc1df1c1c14b478c08a6b9253/ButterPZ.eps" data-type="image"></span></span>
</figure>
</figure>
As with the other IIR filter implementations that will be considered here, the filter above has an order of just $N=6$: it takes merely 6 poles and 6 zeros to create this particular filter, which is 10-100 times smaller an order than that of similar FIR filters. More than other IIR filters, the Butterworth also achieves a near-uniform flatness in its pass- and stop-bands. However, this does come at a cost; compared to other IIR filters of the same order, the Butterworth's transition region (centered about $\omega=\pm\frac{\pi}{2}$ above) is more gradual, meaning there is a larger range of frequencies that are not either completely passed or attenuated out.</p><p id="eip-888"><span data-type="title">Elliptic Filter</span>The Elliptic filter has characteristics that are essentially the opposite of that of the Butterworth filter.
<figure id="ellips" data-orient="vertical"><figcaption>The impulse response, frequency response magnitude, and pole-zero plot of an order $N=6$ low-pass Elliptic filter.</figcaption><figure id="Elliph-plot">
<span data-type="media" id="Elliph" data-alt="Image">
<img src="/resources/fa749dbbd9ce7dc0d302396c8f09e63e98adb929/Elliph.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ebfc8cd711d94e3fd8f6cdb4796ab2009e066cd9/Elliph.eps" data-type="image"></span></span>
</figure>
<figure id="EllipFilter-plot">
<span data-type="media" id="EllipFilter" data-alt="Image">
<img src="/resources/307c519926a523bc5aea22ce064c8b0f621965c7/EllipFilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/3d0845d42e6a03c42189de3f9b15b262edebd0e6/EllipFilter.eps" data-type="image"></span></span>
</figure>
<figure id="EllipPZ-plot">
<span data-type="media" id="EllipPZ" data-alt="Image">
<img src="/resources/c864355efc574b1b6f6e77d751554efecdef9bea/EllipPZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/4d7876855d4cfd6deaee9268d4f92a73b2d1fe59/EllipPZ.eps" data-type="image"></span></span>
</figure>
</figure>
Unlike the Butterworth filter, the magnitude of the Elliptic filter's frequency response has considerable variation about its targets of $|H(\omega)|=1$ in the pass-band and $|H(\omega)|=0$ in the stop-band. However, in return for having these ripples across the response, note how it makes up for the Butterworth's main disadvantage. In contrast to the Butterworth filter's somewhat lazy slide from the pass-through region to the attenuation region, the Elliptic filter's frequency response has a very steep transition at its cutoff point; virtually all frequencies reside in either the filter's pass- or stop-band.</p><p id="eip-585"><span data-type="title">Chebyshev Filters</span>The Butterworth filter has a smooth frequency response magnitude, but a wide transition region between its pass-band and stop-band. The Elliptic filter has a very narrow transition region, but must trade-off for this with ripples across its frequency response. We might wonder if there is a filter that could be a kind of compromise between the two, not too ripply, and with a decently steep transition at the cutoff point. It just so happens that there is.
<figure id="Cheby1s" data-orient="vertical"><figcaption>The impulse response, frequency response magnitude, and pole-zero plot of an order $N=6$ low-pass Chebyshev Type-1 filter.</figcaption><figure id="Cheby1h-plot">
<span data-type="media" id="Cheby1h" data-alt="Image">
<img src="/resources/269e2be6b4bd3eec6fe3d9547e88ee8509ccb32b/Cheby1h.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d49a262550072f4ecf3855f5b70190e6c59afa43/Cheby1h.eps" data-type="image"></span></span>
</figure>
<figure id="Cheby1Filter-plot">
<span data-type="media" id="Cheby1Filter" data-alt="Image">
<img src="/resources/242c997ee0590051689c8d8fc801f7d98bfc4faf/Cheby1Filter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/013dd3627daee9e94df2c23dca92cbd1d0a57896/Cheby1Filter.eps" data-type="image"></span></span>
</figure>
<figure id="Cheby1PZ-plot">
<span data-type="media" id="Cheby1PZ" data-alt="Image">
<img src="/resources/1c9331a9cee1ddea57a506d5f09aef90ebd90f7e/Cheby1PZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/f89351e8dfbf385e76e2ca2e9b22c55a8d6cc4fb/Cheby1PZ.eps" data-type="image"></span></span>
</figure>
</figure>
The Chebyshev Type-1 filter has some ripple, but note that is only in the pass-band, whereas it has a very smooth response at about $|H(\omega)|=0$ in the stop-band. Additionally, it has a fairly steep transition between the pass- and stop-bands. There is a second type of Chebyshev filter that has the same steep transition region, while swapping which region has ripples and which one is flat:
<figure id="Cheby2s" data-orient="vertical"><figcaption>The impulse response, frequency response magnitude, and pole-zero plot of an order $N=6$ low-pass Chebyshev Type-2 filter.</figcaption><figure id="Cheby2h-plot">
<span data-type="media" id="Cheby2h" data-alt="Image">
<img src="/resources/ba3b0d99c4186f50de7fc16940a0967fb7069186/Cheby2h.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/3c6a78c9dc3f5e88758f1b4c7cb1ce489fff26bd/Cheby2h.eps" data-type="image"></span></span>
</figure>
<figure id="Cheby2Filter-plot">
<span data-type="media" id="Cheby2Filter" data-alt="Image">
<img src="/resources/67e67295b3e1e252f9f1c961ff70f60f520a17ea/Cheby2Filter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/611ae1acbb4631e3b53bef017a583306247f03c9/Cheby2Filter.eps" data-type="image"></span></span>
</figure>
<figure id="Cheby2PZ-plot">
<span data-type="media" id="Cheby2PZ" data-alt="Image">
<img src="/resources/8ffed65dbd0994ad893eb2454ef40f02d354e30a/Cheby2PZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1a07f0b24c0f13afc6172d75c7888308d768ffa3/Cheby2PZ.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-260"><span data-type="title">IIR Filters: An Evaluation</span>Among IIR filters, we have seen that they all have their own advantages and disadvantages. For a given filter-length, the different implementations trade-off on having either a smooth frequency response magnitude or sharp transition region (compromise between the two).
<figure id="CompFilter"><figcaption>A superposition of frequency response magnitude of the four IIR filters considered: Butterworth (black), Elliptic (green), Chebyshev Type-1 (blue), and Chebyshev Type-2 (red).</figcaption><span data-type="media" id="CompFilter-plot" data-alt="Image">
<img src="/resources/d62ad8d713f7eb9a651d77fcee0ae7f33a1789da/CompFilter.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2669fe7778f7f43f45a479b9e5ed49a26ddaa180/CompFilter.eps" data-type="image"></span>
</span></figure>
As mentioned above, all of these filters also have an advantage when compared to FIR filters, for they achieve desired frequency response characteristics while being relatively small in length (thus less hardware/processing is required for their implementation). But as you might expect, they do have a couple downsides. First: unlike FIR filters, IIR filters have poles. The sensitivity of such poles is what allows the filters to have such strong effects with small filter orders, but that also means that deviations in pole locations (which might happen in a real-world setting) introduce more distortion than a similar deviation of a zero. And second, IIR filters have non-linear phase responses.
<figure id="CompFilterAng"><figcaption>The phase of the four different IIR filter frequency responses: Butterworth (black), Elliptic (green), Chebyshev Type-1 (blue), and Chebyshev Type-2 (red).</figcaption><span data-type="media" id="CompFilterAng-plot" data-alt="Image">
<img src="/resources/7a1e603c400f2c86813357f4fa589e1c9df1b7e4/CompFilterAng.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/52b65ac1c2d5fa875f2077d5a753afe88b49ce51/CompFilterAng.eps" data-type="image"></span>
</span></figure>
Consider especially the pass-band of the filters above. While the Butterworth and Chebyshev Type-1 filters come closer than the others to being linear, all are definitely "curvy" (i.e., nonlinear) across the pass-band. A linear phase response is desirable because it means that all input frequencies will have the same time delay through the filter; a non-linear response means that some frequencies will be output before others.</p><p id="eip-220">Suppose we have a signal composed of two sinusoids, $x[n]=\sin(\frac{2\pi 50}{1000}n)+.25\sin(\frac{2\pi 150}{1000}n)$, and we run this signal through the low-pass Elliptic filter shown above (in green). Both of these sinusoids are within the pass-band of the filter (as their frequencies are $\pm\frac{1}{10}\pi$ and $\pm\frac{3}{10}\pi$), so we would expect the output of the filter, $y[n]$, to be essentially unchanged, perhaps only a bit attenuated because of the ripple in the frequency response. However, this expectation does not take into account the phase response. Because of the phase non-linearity, the sinusoids are shifted different amounts of time, resulting in them being out of sync when compared to the original plot:
<figure id="sinesInOut" data-orient="vertical"><figcaption>The non-linear phase response of Elliptic filters is evident when comparing the above input and output.</figcaption><figure id="sinesIn-plot"><figcaption>Signal input into a low-pass Ellipic filter, with (both frequencies) of the signal within the filter's pass-band.</figcaption>
<span data-type="media" id="sinesIn" data-alt="Image">
<img src="/resources/75107e4cae59a38a8078a0327b420cc488a08cb5/sinesIn.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8ba0996d4dcbb2fdce9cbd56da917169c30a5884/sinesIn.eps" data-type="image"></span>
</span>
</figure>
<figure id="sinesOut-plot"><figcaption>The output of the filter.</figcaption>
<span data-type="media" id="sinesOut" data-alt="Image">
<img src="/resources/ef510746b3604f44d485e09d2009f4da6b947144/sinesOut.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/954ef298e107b80370b9270e2d6a16617d0d5b08/sinesOut.eps" data-type="image"></span>
</span>
</figure>
</figure></p></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A7be7bd64-267c-491e-bb67-ff9e515f7dfd%401.html" data-type="page"><h1>FIR Filter Design</h1><div data-type="document-title">FIR Filter Design</div>
  <p id="delete_me"><span data-type="title">FIR Systems</span>Recall the system representation of discrete-time LTI systems:
<figure id="filter"><figcaption>Implementation of a discrete-time LTI system.</figcaption><span data-type="media" id="filter-plot" data-alt="Image">
<img src="/resources/dafc15b36fa6b557803956ecdbd302ab54e2022f/filter.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
FIR systems are those that do not have poles/feedback, so the representation can be simplified:
<figure id="FIR"><figcaption>Implementation of an FIR discrete-time LTI system.</figcaption><span data-type="media" id="FIR-plot" data-alt="Image">
<img src="/resources/ff92f39a46794db08621386ebbf78ab05ca38dc2/FIR.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
From the straightforward time domain expression of this kind of system ($y[n] = b_0 x[n] + b_1 x[n-1]+ b_2 x[n-2] + \cdots b_M x[n-M]$) we have the following transfer function:
$\begin{align*} H(z)= \frac{Y(z)}{X(z)} &amp;= b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots b_M z^{-M}\\ &amp;= z^{-M}(z-\zeta_1)(z-\zeta_2) \cdots (z-\zeta_M) \end{align*}$ 
In the z-domain, such systems have only zeros (not counting poles at $z=0$, or in the case of acausal systems, poles at $z=\infty$).</p><p id="eip-403"><span data-type="title">FIR Filters</span>As FIR filters are FIR discrete-time LTI systems, their design is simply a matter of adjusting the $b$ coefficients of the time-domain representation (or equivalently, the location of the zeros in the z- domain). As with IIR systems, these systems are designed to perform to particular specifications in the frequency domain:
<figure id="filterspec"><figcaption>CAPTION.</figcaption><span data-type="media" id="filterspec-plot" data-alt="Image">
<img src="/resources/e3502062666e85abf9bbdb95a13d72e42c1db83d/filterspec.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
FIR filters must be of much higher order (i.e., have more non-zero coefficients in their time-domain representations) than IIR filters to achieve similar performance, but in exhange for this cost have several benefits over IIR filters. First, an FIR filter is GUARANTEED to be BIBO stable. Recalling that an LTI system is BIBO stable if and only if its impulse response is absolutely summable, the finite length of FIR impulse responses guarantees this condition. And second, unlike the non-linearity with IIR filters, FIR filters can be easily designed to have a <span data-type="term">generalized linear phase</span> response (a phase response that is linear, with one caveat; see below).</p><p id="eip-465">In order to have a generalized linear phase response, FIR filters of length $N$ must be designed so that their impulse responses $h[n]$ are either even or odd symmetric about the center time value(s) ($(N-1)/2$ for odd $n$, $n=N/2$ and $n=N/2-1$ for even $N$). This design can be attained for $N$ that are either even or odd. We will consider when $N$ is odd, i.e. when $N=M+1$ where $M$ is an even number. For such a filter length, evenness implies that $h[n]=h[M-n]$. Suppose the impulse response is designed so that such is the case. Let's take a look at the system's frequency response, using that symmetry to simplify things:
$\begin{align*}
H(\omega) &amp;= \sum_{n=0}^{M} h[n]e^{-j\omega n}\\
&amp;= \sum_{n=0}^{M/2-1} h[n]e^{-j\omega n} + h[M/2]e^{-j\omega M/2} +
\sum_{n=M/2+1}^{M} h[n] e^{-j\omega n}
\\
&amp;= \sum_{n=0}^{M/2-1} h[n]e^{-j\omega n} + h[M/2]e^{-j\omega M/2} +
\sum_{n=M/2+1}^{M} h[M-n] e^{-j\omega n}
\\
&amp;= \sum_{n=0}^{M/2-1} h[n]e^{-j\omega n} + h[M/2]e^{-j\omega M/2} +
\sum_{r=0}^{M/2-1} h[r] e^{-j\omega (M-r)}
\\
&amp;= h[M/2]e^{-j\omega M/2} + \sum_{n=0}^{M/2-1} h[n] \left(e^{-j\omega n} + e^{j\omega (n-M)} \right)\\
&amp;=  \left( h[M/2] + \sum_{n=0}^{M/2-1} 2  h[n] \cos(\omega (n-M/2)) \right) e^{-j\omega M/2}\\
&amp;= A(\omega) e^{-j\omega M/2}
\end{align*}$
Take a close look at the $A(\omega)$ term. It is called the filter's amplitude, because its amplitude is the same as the amplitude of the frequency response: $|A(\omega)|=|H(\omega)|$. It is also purely real-valued. Thus the frequency response of the filter is a real number multiplied by an exponential function $e^{-j\omega M/2}$ that has linear phase. This means that the frequency response has a perfectly linear phase, except when the sign of $A(\omega)$ changes. However, since $A(\omega)$ will hover around the value of $1$ in the filter's pass-band, the filter will have linear phase in the pass-band.</p><p id="eip-199"><span data-type="title">Creating FIR Filters</span>For an FIR filter of odd-length ($N=M+1$, where $M$ is even) and even symmetry about $n=\frac{M}{2}$ ($h[n]=h[M-n]$) we have that the filter's frequency response is:
$H(\omega)=A(\omega) e^{-j\omega M/2}$
where
$A(\omega)=h[M/2] ~+~ \sum_{n=0}^{M/2-1} 2 \, h[n] \cos(\omega (n-M/2))$
Because (generalized) linear phase is desirable, we can focus on meeting filter specifications in terms of modifying $A(\omega)$:
<figure id="specFIR"><figcaption>FIR filter design amounts to creating an $A(\omega)$ to fit particular specifications.</figcaption><span data-type="media" id="specFIR-plot" data-alt="Image">
<img src="/resources/26bf27876936deaed0c4218cbe80e9a012c6b98c/specFIR.png" data-media-type="image/png" alt="Image" width="400">
</span></figure>
The goal will be to meet the requirements with as small of a filter size as possible. To that end, there are two pieces of information that will be very useful for us. Both involve the "ripples" of $A(\omega)$ centered around $A(\omega)=1$ in the pass-band and $A(\omega)=0$ in the stop-band. It turns out that among all filters that meet some defined specifications on $A(\omega)$ (the error bounds in the pass- and stop-bands and the pass and stop frequencies), the filter of shortest length $M+1$ will:
--be an <span data-type="term">equiripple filter</span>, meaning that all of the ripple oscillations will be of the same deviation about $A(\omega)=1$ in the pass-band and $A(\omega)=0$ in the stop-band (in this sense, the "spread" equally the error out over the entire frequency range)
--will touch the error bounding box $\frac{M}{2}+2$ times
Putting those properties together, we can see that there are two ways of going about FIR equiripple filter design. One way might be to specify the desired filter length $N=M+1$ (or, equivalently, <span data-type="term">filter order</span> $M$), and then use the properties to create an equiripple filter based upon pass-band and stop-band frequencies; because of the first property, this filter will minimize the maximum deviation from $1$ in the pass-band and from $0$ in the stop-band. To design such a filter, one could use the command firpm in MATLAB. Another approach could be to specify these desired deviations, and then go about finding the filter of minimum length that will satisfy these deviations. To do that, one could use the firpmord command in MATLAB.</p><div data-type="example" class="example" id="eip-66"><div data-type="title" class="title">Equiripple Filter of Length $N=21$</div><p id="eip-394">Suppose we wanted to create a low-pass filter of length $N=21$ (order $M=20$), with a pass-band from $\omega=0$ to $\omega=.3\pi$ and a stop-band from $\omega=.35\pi$ to $\omega=\pi$. Among all possible FIR filters of length $N=21$, an FIR equiripple filter will have the smallest maximum deviation from the optimal frequency response magnitudes (of $|H(\omega)|=1$ in the pass-band and $|H(\omega)|=0$ in the stop-band). According to the equiripple filter properties, its frequency response will touch these error bounding boxes $\frac{M}{2}+2=12$ times.
<figure id="hhopts" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="HHoptfir1"><figcaption>CAPTION.</figcaption><span data-type="media" id="HHoptfir1-plot" data-alt="Image">
<img src="/resources/efff90aba32a974b9c84d644cd86122da7d2f67f/HHoptfir1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/c5c625e996015c0ca6107a1a97186e570a9cac44/HHoptfir1.eps" data-type="image"></span>
</span>
</figure>
<figure id="HHoptfir1BB-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="HHoptfir1BB" data-alt="Image">
<img src="/resources/6371bffea63efcef39c66937e95ed4b665c0d5ef/HHoptfir1BB.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2673707b8dfeb17d6639854679e7f936df399e7b/HHoptfir1BB.eps" data-type="image"></span></span>
</figure>
</figure>
The equiripple filter is also designed, as described above, to have an impulse response that has even symmetry about its $n=10$ center time value. As a result, it will have a generalized linear phase:
<figure id="hoptfir1lp" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="hoptfir1-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="hoptfir1" data-alt="Image">
<img src="/resources/451457a6780f83055e45875b6b21054fc6863247/hoptfir1.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/ce79dc171182e1262ae065f617144f7fbc6fc61f/hoptfir1.eps" data-type="image"></span></span>
</figure>
<figure id="HHoptfir1angle-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="HHoptfir1angle" data-alt="Image">
<img src="/resources/b3c4527d63331f659bd11779192187080c823b0a/HHoptfir1angle.svg" data-media-type="image/svg+xml" alt="Image" width="500">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/8151e777a1a72e34098b78626e02764d554424f1/HHoptfir1angle.eps" data-type="image"></span></span>
</figure>
</figure>
Finally, as this filter is an FIR filter, it will not have any poles (except for those at $z=0$ and, if it were acausal, at $z=\infty$). Since its order is $M=20$, it does have $20$ zeros:
<figure id="HHoptfir1PZ"><figcaption>CAPTION.</figcaption><span data-type="media" id="HHoptfir1PZ-plot" data-alt="Image">
<img src="/resources/68f148dbfce3c053f3025059c42008f3302b8deb/HHoptfir1PZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/aa8de28bfce870c09b686d646d5cc84cfffcd900/HHoptfir1PZ.eps" data-type="image"></span>
</span></figure></p><div data-type="example" class="example" id="eip-259"><div data-type="title" class="title">Equiripple Filter of Length $N=101$</div><p id="eip-709">The filter in the first example, while optimal for its length, still has considerable deviation from the ideal frequency response of $1$ in the pass-band and $0$ in the stop-band. Having a more ideal response is straightforward, simply increase the length of the filter; as described above, the MATLAB command firpmord could be used to tell us the minimum length required to achieve a certain margin of tolerance from the ideal response. Suppose we had some requirement in mind, and that command told us a filter length of $101$ was required. The resulting equiripple filter of that length is below.
<figure id="opt2plots" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="HHoptfir2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="HHoptfir2" data-alt="Image">
<img src="/resources/fab5479359353d4add0968dfbdabdf5283bb2ac4/HHoptfir2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7c4e587e05e90af47dbc7d80b811017039d1e688/HHoptfir2.eps" data-type="image"></span></span>
</figure>
<figure id="hoptfir2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="hoptfir2" data-alt="Image">
<img src="/resources/9cd2384180f7076e76fea989bfd91075ff1de25a/hoptfir2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/42f4200e9f363b265319170be5d44d1f71c5ad7f/hoptfir2.eps" data-type="image"></span></span>
</figure>
<figure id="HHoptfir2PZ-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="HHoptfir2PZ" data-alt="Image">
<img src="/resources/1ca0bb9a5f9222454b2ed72589217611af9c24b6/HHoptfir2PZ.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="Hoptfir2PZ.eps" data-type="image"></span></span>
</figure>
</figure></p></div></div></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A02966e04-2314-4b5f-9852-58f310e29b78%401.html" data-type="page"><h1>Inverse Filters and Deconvolution</h1><div data-type="document-title">Inverse Filters and Deconvolution</div>
  <p id="delete_me">We have already seen how discrete-time LTI systems can be represented mathematically in both the time domain (through the impulse response $h[n]$) and frequency domain (through either the frequency response $H(\omega)$ or the transfer function $H(z)$). But systems can be thought of not only as entities that we create to modify signals, but as real-world environments which might perform unwanted (or only temporarily wanted) changes to the signal. For example, a television or cell phone signal traveling in the air through a downtown area might reach its destination with added echoes. Or a camera lens might introduce unintentional blur on a photograph. Or a signal might be modified to help with its transmission. In each of these cases, we can represent the modifications as a system that takes an input and produces an output, and we ultimately would like to undo the changes the system made; we would like to recover the input, given the output. A system that seeks to recover this original input is called an <span data-type="term">inverse filter</span>, and the process of inverse filtering is also known as <span data-type="term">deconvolution</span>.</p><p id="eip-409"><span data-type="title">Inverse Filtering in the z-Domain</span>Suppose there is an LTI system $G$ that takes an input signal $x[n]$ and produces output $y[n]$. What we would like to do is create another system $H$ that takes this $y[n]$ and then, ideally, produce $x[n]$ as its output; we will call the actual output $\hat{x}[n]$.</p><p id="eip-643">It might not be clear at first how to create a system that recovers $x[n]$ from $y[n]$, especially if we consider the problem in the time domain: if $y[n]=x[n]\ast g[n]$, then what must $h[n]$ be so that $h[n]\ast y[n]=h[n]\ast (x[n]\ast g[n])=x[n]$? By the associative and commutative properties of convolution, you could say we would want $h[n]\ast g[n]=\delta[n]$ because $\delta[n]\ast x[n]=x[n]$, but even then, how do you create such an impulse response?</p><p id="eip-1000">The problem is more straightforward when we consider it from the z-domain:
$\begin{align*}
\hat{X}(z)&amp;=H(z)Y(z)\\
&amp;=H(z)G(z)X(z)\\
&amp;=X(z)~,~H(z)=\frac{1}{G(z)}~\forall ~z
\end{align*}$
So if we can create $H(z)$ such that it equals $\frac{1}{G(z)}$, then we can recover the original input $x[n]$ by filtering the output $y[n]$ through a system whose transfer function is $H(z)$. If the original filter $G(z)$ is a rational function with certain poles and zeros, then the inverse filter will also be a rational function; and, since $H(z)=\frac{1}{G(z)}$, it follows naturally that $H(z)$ will have poles at the locations of $G(z)$'s zeros and zeros at the location of $G(z)$'s poles:
$\begin{align*}
G(z) &amp;= z^{N-M}\, \frac{(z-\zeta_1)(z-\zeta_2) \cdots (z-\zeta_M)}{(z-p_1)(z-p_2) \cdots (z-p_N)},\\
~\\
H(z)=\frac{1}{G(z)}&amp;=z^{M-N}\, \frac{(z-p_1)(z-p_2) \cdots (z-p_N)}{(z-\zeta_1)(z-\zeta_2) \cdots (z-\zeta_M)}
\end{align*}$</p><div data-type="example" class="example" id="eip-958"><div data-type="title" class="title">An Inverse Filter</div><p id="eip-934">Suppose a discrete-time signal $x[n]$ is input through a filter $G(z)$ with frequency response and pole/zero plot as displayed below:
<figure id="gfilters" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="Gfiltercnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="NAME" data-alt="Image">
<img src="/resources/c7a8aec3010bd68a5f527150f84d6f9c7daf6237/Gfiltercnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/b7d771832920ddc1eb612c80a83067bdfdd5bf90/Gfiltercnx.eps" data-type="image"></span></span>
</figure>
<figure id="GfilterPZcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="GfilterPZcnx" data-alt="Image">
<img src="/resources/820eb623f19acdfc39a836dcebc4e72ed1c6b362/GfilterPZcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d4ab8c52a6615b03bf38f97995f030466a83f236/GfilterPZcnx.eps" data-type="image"></span></span>
</figure>
</figure>
When the signal $x[n]$ is passed through this filter, the signal $y[n]$ is the output:
<figure id="gsigs" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="signalx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="signalxcnx" data-alt="Image">
<img src="/resources/02e68302b6abb3791202d0f295c95c95ddf89a23/signalxcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a397c05b1372e1d59b0eb5baf68ce5dcbf16c046/signalxcnx.eps" data-type="image"></span></span>
</figure>
<figure id="signalycnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="signalycnx" data-alt="Image">
<img src="/resources/c82fcd20cb896cda0380a905ea213599086dd38a/signalycnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/fde8f9a6e534e30150cb6ff8014720f22a425336/signalycnx.eps" data-type="image"></span></span>
</figure>
</figure>
In order to recover the input $x[n]$ from the output $y[n]$, we can run $y[n]$ through an inverse filter $H(z)$ where $H(z)=\frac{1}{G(z)}$:
<figure id="hfiltplots" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="Hfilterinvcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="Hfilterinvcnx" data-alt="Image">
<img src="/resources/8ee5c6abdb616529d63eee2fe6c061c46c764dc1/Hfilterinvcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2bf7bf1ec0d360292ada547846c3df1a6cfe7d5f/Hfilterinvcnx.eps" data-type="image"></span></span>
</figure>
<figure id="HfilterPZcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="HfilterPZcnx" data-alt="Image">
<img src="/resources/364ff45fbc44929aadf6c74e1b12f960f61566a0/HfilterPZcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/cab839ec21d90a5a986bb6cc6e4b1746f542221d/HfilterPZcnx.eps" data-type="image"></span></span>
</figure>
</figure>
The result is a recovered signal that is identical to the original input:
<figure id="signalxicnx"><figcaption>CAPTION.</figcaption><span data-type="media" id="signalxicnx-plot" data-alt="Image">
<img src="/resources/a8098d29e037a70d9f929ccadcbafe67bb214cb6/signalxicnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/0f5d70535a27be278719942893ca5d714de66d3c/signalxicnx.eps" data-type="image"></span>
</span></figure></p></div><p id="eip-112"><span data-type="title">Approximate Inverse Filters</span>As remarkable as this simple inverse filtering solution is, there is a huge proviso to it: the $H(z)$ must be a stable system. If it is also to be causal (which is of practical importance), that means that all of its poles must lie within the unit circle. As its pole location are simply the zero locations of $G(z)$, this means that $G(z)$ is invertible if all of its ZEROS are within the unit circle. If, on the other hand, $G(z)$ has zeros on or outside the unit circle, then the inverse filter $H(z)=\frac{1}{G(z)}$ cannot be counted on to recover the original input.</p><p id="eip-956">There is an intuitive reason why a $G(z)$ with a pole on or outside the unit circle could not be inverted. Suppose the system $G$ has this simple input/output relationship:
$y[n]=x[n]-x[n-1]$
Such a system is a primitive high-pass filter, with a single zero on the unit circle at $z=1$. What this zero does is completely eliminate the $\omega=0$ (i.e., DC) frequency component of the input signal. If all we have, then, is $y[n]$, we have absolutely no way of knowing what that value was, therefore there is no $H(z)$ that could possibly recover an arbitrary $x[n]$ from $y[n]$ (of course, if it so happened that the original signal $x[n]$ did not have any $\omega=0$ component, then it could be recovered). Looking at it from a z-domain point of view, there would have to be a pole for $H(z)$ at $z=1$, which is BIBO unstable (any input with a non-zero $\omega=0$ component will "blow up").</p><p id="eip-231">Even if $G(z)$ is not invertible, we can still try to recover $x[n]$ as best as we can. As we do so, we will usually want to make sure our inverse filter $H(z)$ is BIBO stable. If it happens that $\frac{1}{G(z)}$ has poles on or outside the unit circle, we will simply move them inside the unit circle. A straightforward way of doing this is by <span data-type="term">regularization</span>. We choose the smallest constant value of $r$ that will make the following $H_a(z)$ BIBO stable:
$H_a(z)=\frac{1}{G(z)+r}$.</p><div data-type="example" class="example" id="eip-135"><div data-type="title" class="title">An Approximate Inverse Filter</div><p id="eip-415">Suppose that a discrete-time signal $x[n]$ passes through a filter $G(z)$ that is very similar to the one in the first example, except that this $G(z)$ has a zero outside the unit circle. In this case, the $H(z)$ inverse filter cannot merely be $\frac{1}{G(z)}$, as it would therefore have a pole outside the unit circle and would not be BIBO stable:
<figure id="agfilts" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="aGfiltercnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="aGfiltercnx" data-alt="Image">
<img src="/resources/195c1eea9ee6922318fc70621cded379ddcf580d/aGfiltercnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/82570ca692dc41903e89f0cbf4ef569c196b0ac8/aGfiltercnx.eps" data-type="image"></span></span>
</figure>
<figure id="aGfilterPZcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="aGfilterPZcnx" data-alt="Image">
<img src="/resources/bfcf8c75cd413cfcb379b5dad8ad16c383ec320c/aGfilterPZcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1b74e20efecd91305fd59795e1c3183673542126/aGfilterPZcnx.eps" data-type="image"></span></span>
</figure>
</figure>
However, if we let $H_a(z)=\frac{1}{G(z)+\frac{1}{16}}$, then this will move what would have been a pole outside the unit circle into one inside the circle:
<figure id="ahfilts" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="aHfiltercnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="aHfiltercnx" data-alt="Image">
<img src="/resources/92dae3235d51e8115c3376afd5ba6cf79fd5657d/aHfiltercnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/2aeec5ccd8bf9a76e2b86d3fa823eed57b17533f/aHfiltercnx.eps" data-type="image"></span></span>
</figure>
<figure id="aHfilterPZcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="aHfilterPZcnx" data-alt="Image">
<img src="/resources/dd205585c664dd2a644e88da2ed5383fdb7910a0/aHfilterPZcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/54b03d283292d68f7176122c4f583257b5a17998/aHfilterPZcnx.eps" data-type="image"></span></span>
</figure>
</figure>
As a result, this $H_a(z)$ is BIBO stable. Since it is not exactly $\frac{1}{G(z)}$, then it will not exactly recover $x[n]$, but it is a stable filter that can at least come close to doing so, as we can see in the following plots:
<figure id="afiltout" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="signalxcnx2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="signalxcnx2" data-alt="Image">
<img src="/resources/02e68302b6abb3791202d0f295c95c95ddf89a23/signalxcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/a397c05b1372e1d59b0eb5baf68ce5dcbf16c046/signalxcnx.eps" data-type="image"></span></span>
</figure>
<figure id="asignalxicnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="asignalxicnx" data-alt="Image">
<img src="/resources/12a640ba878da907d6e96cd3e2e377f76f550922/asignalxicnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/7a5fd2aa7591ce19a518576f710bb61fbf730523/asignalxicnx.eps" data-type="image"></span></span>
</figure>
</figure></p></div></div></li><li><div href="/contents/8e3c8b79-7dcc-4ec0-ba6e-0d3443a34180%402.45%3A3177f913-457f-4e88-9e72-4709c2379d0e%401.html" data-type="page"><h1>Matched Filters</h1><div data-type="document-title">Matched Filters</div>
  <p id="delete_me">Most discrete-time filters aim to modify an input signal according to desired performance in the frequency domain; for example, a low-pass filter attenuates the high frequency components of an input signal. However, there may be instances in which the goals of the filter are best understood in the time domain. One example of such a filter is the <span data-type="term">matched filter</span>.</p><p id="eip-818">The objective of a matched filter is very straightforward: find the location a particular signal occurs in another (larger) signal. While this is a task often employed in children's games (I Spy) or puzzles (Where's Waldo), it of course has more "grown up" uses, as well: for example, sonar/radar work by transmitting a signal and then listening (searching) for an echoed version of it.</p><p id="eip-276">We have already considered a signal processing technique that works very well at finding one signal in another. Recall what the inner product operation does. The inner product of two signals $x$ and $y$, $/lt x,y/rt=\sum_n x[n]y*[n]$, measures the their correspondence: the more alike the two signals are, the greater the inner product will be. The more unlike they are, the smaller it will be, with a minimum absolute value of zero (which means the signals are orthogonal).</p><p id="eip-28">So it makes sense, then, that the process of matched filtering will use inner products. But it will take more than a single inner product, for we are looking for where a small signal occurs in a larger one. What we would like to do is perform an inner product between the small signal at every possible time location in the larger signal, and then examine were in the large signal this inner product is at its greatest. If we would like to find where the signal $x[n]$ occurs in signal $y[n]$, then we would simply calculate $\lt y[n],x[n-m]\gt$ at every time instance $m$ and note where the maximum value occurs.
<figure id="sigs" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="xsignalcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xsignalcnx" data-alt="Image">
<img src="/resources/6db1129ef8547e6557aaa99bb2273502543d321d/xsignalcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="NAME.eps" data-type="image"></span></span>
</figure>
<figure id="ysignalScnx-2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="ysignalScnx-2" data-alt="Image">
<img src="/resources/3da684b3ce12e8a06a0463c93beb325f1c0158e3/ysignalScnx-2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1963c58475d836580a5de663abc06fbd110578b1/ysignalScnx-2.eps" data-type="image"></span></span>
</figure>
<figure id="xysignalScnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="xysignalScnx" data-alt="Image">
<img src="/resources/6b973bacd2bc605f84af6ded86575f3f32e96695/xysignalScnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/d668562fddff9ad51d367e295e6520a164f98278/xysignalScnx.eps" data-type="image"></span></span>
</figure>
</figure></p><p id="eip-776">If the process of taking an inner product at every time location in a signal sounds familiar, it is because that is essentially what discrete-time convolution is! Noting that the usual definition switches the $m$ and $n$ variables, recall the convolution sum:
$y\ast x=\sum_n y[n]x[m-n]$
At every time location in the signal, the convolution sum is performing an inner-product like operation between $y[n]$ and $x[-n]$. The only thing that is missing for it to be an inner product is complex conjugation, so technically the convolution sum is performing an inner product between $y[n]$ and $x*[m-n]$; it is determining how similar $y[n]$ and $x*[-n]$ are at time $m$. So if we would like to find where $x[n]$ occurs in $y[n]$, then we would simply need to carry out the convolution $y[n]\ast x*[-n]$ and find its maximum value.
<figure id="convs" data-orient="vertical"><figcaption>CAPTION.</figcaption><figure id="Cxsignalcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="Cxsignalcnx" data-alt="Image">
<img src="/resources/6db1129ef8547e6557aaa99bb2273502543d321d/xsignalcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="NAME.eps" data-type="image"></span></span>
</figure>
<figure id="CysignalScnx-2-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="CysignalScnx-2" data-alt="Image">
<img src="/resources/3da684b3ce12e8a06a0463c93beb325f1c0158e3/ysignalScnx-2.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/1963c58475d836580a5de663abc06fbd110578b1/ysignalScnx-2.eps" data-type="image"></span></span>
</figure>
<figure id="dsignalcnx-plot"><figcaption>CAPTION.</figcaption>
<span data-type="media" id="dsignalcnx" data-alt="Image">
<img src="/resources/2c5b069d26395b78e6ef030dc6ca90b470005ffc/dsignalcnx.svg" data-media-type="image/svg+xml" alt="Image">
<span data-media-type="application/postscript" data-print="true" data-src="/resources/c8f4125557cb0d1ba4f0f3c40f3be82e476500f6/dsignalcnx.eps" data-type="image"></span></span>
</figure>
</figure>
Since we are looking for the maximum value, or (if we believe the signal may occur more than once in the larger signal) for some threshold to be met, matched filtering can be used in the presence of noise. In the example above, $x[n]$ appears in a noisy signal $y[n]$, but the convolution sum does not mind--it simply finds for us the location in $y[n]$ with the strongest correlation to $x[n]$. Because matched filtering works in this way, it is also sometimes referred to (especially in statistical contexts) as cross correlation.</p></div></li></ul></li></ul></li></ul></body>
</html>
